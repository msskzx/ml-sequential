{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hJAumQyI45Cg"
   },
   "source": [
    "# Social Computing/Social Gaming - Summer 2021\n",
    "# Exercise Sheet 5 - Hate Speech\n",
    "Online hate speech is a large scale phenomenon that gained more and more traction in modern society in recent years. Violence attributed to online hate speech has increased worldwide. The same technology that allows social media to galvanize activist movements and NGOs can be used by hate/crime groups seeking to organize and recruit. It also allows conspiration theorists to reach audiences far broader than their core community. It is time – now more than ever –  to put systems in place that make sure social media is not used as a tool to conduct criminal activities. Fortunately, modern technology allows us to do just that.\n",
    "\n",
    "In this exercise sheet, we will attempt to accurately and automatically detect two instances of hate speech in Twitter: sexism and racism. The first step in this process will be to prepare the data before it is fed to the model. We do this with the help of the Universal Sentence Encoder, which is explained in more detail later. Additionally, we also need to encode the labels and split the data.\n",
    "\n",
    "We then take two different approaches in classifying the data. In other words, we will create, train, evaluate and compare two models. One of them is purely based on text (the Base Model) and the other also takes the social context of the users into account (the Social Model)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3oiBPS0r45Co"
   },
   "source": [
    "## Why Twitter?\n",
    "Hate crimes are communicative acts, often provoked by events that incite retribution in a targeted group. The continued growth of online social networks and micro-blogging Web services, such as Twitter, enable an extensive and near real-time data source through which the analysis of hateful and antagonistic responses to “trigger” events can be undertaken. Such data affords researchers with the possibility to measure the online social mood and emotion following large-scale, disruptive, and emotive events. Twitter is a defensible and logical source of data for such analysis given that users of social media are more likely to express emotional content due to deindividuation (anonymity, lack of self-awareness in groups, disinhibition) [1]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UhxfaKOZfEBN"
   },
   "source": [
    "## Task 5.0: The Data\n",
    "For this, we have picked the dataset of Waseem and Hovy [2], in a slightly modified version. The collection originally contained 16,914 labeled tweets, however some of them are not accessible via Twitter API anymore. As a consequence, the dataset now contains 16,849 tweets divided in the following categories: 3,378 *sexism*, 1,970 *racism* and 11,501 *neither*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bnz1j9RsBjiu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6LnDiJ1UnYs9"
   },
   "outputs": [],
   "source": [
    "# Reads the data set from a .csv file\n",
    "data = pd.read_csv('tweets.csv', low_memory=False)\n",
    "data = data.astype(str)\n",
    "\n",
    "# This drop operation is necessary because of an inconsistency in the dataset\n",
    "data = data.drop([3343, 3344])\n",
    "data = data[['text', 'label']]\n",
    "\n",
    "# We need to do a unique and precise reordering to match with graph information later on\n",
    "unique_tweets, indices = np.unique(data['text'].to_numpy(), return_index=True)\n",
    "ordered_labels = data['label'].to_numpy()[indices]\n",
    "data = pd.DataFrame(np.stack((unique_tweets, ordered_labels), axis=1), columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_ztw1QebAK0",
    "outputId": "ce3a10af-c304-4366-d070-1f1c4d40865c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TWEETS: 16849, RACIST: 1970, SEXIST: 3378, NEITHER: 11501\n"
     ]
    }
   ],
   "source": [
    "# See the summary of the dataset's content\n",
    "\n",
    "print(\"TOTAL TWEETS: {}, RACIST: {}, SEXIST: {}, NEITHER: {}\".\\\n",
    "      format(len(data), len(data[data[\"label\"] == 'racism']), len(data[data[\"label\"] == 'sexism']), len(data[data['label'] == 'none'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PgCD_Y9o45Cu"
   },
   "source": [
    "## Task 5.1: Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xta8UAMEe0E"
   },
   "source": [
    "### a) Encode the labels\n",
    "In order for [PyTorch](https://pytorch.org) to work with the labels, they need to have a specific format. Strings need to be replaced by numbers with an according mapping.\n",
    "\n",
    "Map the labels from the `label_mapping = ['sexism' 'none' 'racism']` to a numeric vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4VAbuqeEXok",
    "outputId": "9cc4eb3b-7bd3-42d5-b2e0-31d3a98278c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16849,)\n",
      "['none' 'racism' 'sexism']\n",
      "(16849,)\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "original_labels = np.array(data[\"label\"].tolist())\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(original_labels)\n",
    "\n",
    "print(original_labels.shape)\n",
    "print(np.unique(original_labels))\n",
    "print(labels.shape)\n",
    "print(np.unique(labels))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TYQoBAAPbAK1"
   },
   "source": [
    "### b) Universal Sentence Encoder\n",
    "Google's Universal Sentence Encoder ([USE](https://tfhub.dev/google/universal-sentence-encoder/4) [4]) is a convenient way to map any type of sentence to a 512-dimensional vector. In these 512-dimensional vectors semantic meaning is encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oYeA9OwTbAK1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 15:04:01.829025: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Run this code block only once as the download will take some time and embedding is very memory expensive!\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WymD6U7fbAK1"
   },
   "source": [
    "In this task you are suppossed to get a feeling for this type of embedding. Find a pair of sentences that are similar in their meaning but not syntactically. After that, think of two semantically very different sentences.\n",
    "Obtain the values for them and compare them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-dg6flX4bAK1"
   },
   "source": [
    "**TODO: Write your observations here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IsVjvYLobAK2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try out USE here\n",
    "exs = [\n",
    "    \"samurai honorable\",\n",
    "    \"samurai powerful\",\n",
    "    \"ninja stealthy\",\n",
    "    \"ninja fast\",\n",
    "    \"huntress strong\",\n",
    "    \"huntress precise\",\n",
    "    \"nurse caring\",\n",
    "    \"nurse helpful\",\n",
    "]\n",
    "\n",
    "embeddings = embed(exs)\n",
    "\n",
    "print(embeddings.shape)\n",
    "\n",
    "# plot the embeddings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1])\n",
    "for i, txt in enumerate(exs):\n",
    "    plt.annotate(txt, (embeddings[i, 0], embeddings[i, 1]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9d_j-DwjbAK2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16849,)\n",
      "(16849, 512)\n"
     ]
    }
   ],
   "source": [
    "# DONE: Now encode our dataset's tweets\n",
    "print(data['text'].shape)\n",
    "encoded_tweets = np.array(embed(data['text']))\n",
    "print(encoded_tweets.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "k-RWP1FAbAK2"
   },
   "source": [
    "## Task 5.2: Base Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nR2HytxsbAK2"
   },
   "source": [
    "### a) Base Model creation\n",
    "In this code we create our base model and train it afterwards using the PyTorch library. We use a simple Neural Network to build this model. You can read about Neural Networks here in case you are not familiar with them.\n",
    "You can get a basic intuition for Neural Networks [here](https://medium.com/@shaistha24/basic-concepts-you-should-know-before-starting-with-the-neural-networks-nn-3-6db79028e56d) [5].\n",
    "\n",
    "For the base model we have our 512 dimensional input layer. Then we have a fully connected layer with 100 nodes and with a dropout rate of 0.5 is added. For now, you do not need to know what dropout is. After the dropout, another fully connected layer with 50 nodes is added and we once again add a 0.5 dropout rate.\n",
    "Our output layer has 3 nodes: One for \"sexism\", \"none\" and \"racism\". The computed values for these last 3 nodes correspond to the probability of belonging to either one of our categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "x32XCE3RbAK2"
   },
   "source": [
    "![title](./img/base_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zngyVOwzdv6l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "ENCODING_DIM = 512\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(ENCODING_DIM, 100)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(50, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHO6sHjr3rMv",
    "outputId": "42912f76-3421-4bb7-808a-6b4ada4c605b"
   },
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "#summary(model, input_size = (0,512), batch_size = 32, device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3y-JP2SL45Cy"
   },
   "source": [
    "### b) Train-Test split for Base Model\n",
    "Splitting our labeled data into a train test and validation set is a common practice.\n",
    "* Train set: This set is used to train our model on. The model will try to learn from it.\n",
    "* Validation set: This set is used to choose hyper parameters. Since creating good models requires to find the right parameters (e.g. what kind of activation function, how many epochs etc.) this set is used to maximize the performance of a model for a fixed choice of parameters.\n",
    "* Test set: This set is used to evaluate our final model on. After the model has been trained and a final decision for hyper parameters has been made, the model will be evaluated on this set only. No more parameters should be changed after that.\n",
    "\n",
    "This rather strange seeming approach helps to identify models that actually generalize well and not just perform very good because we adapted the parameters to maximize the performance on one particular set.\n",
    "\n",
    "We will use 60% of our dataset to train our model (the train set) and the remaining 20% to evaluate our model (the test set)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PfkUVBhIIZfd"
   },
   "source": [
    "**Hint:** The sklearn library offers a function that could help you out with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXW6xZoH45Cz",
    "outputId": "d141039e-f7b0-4aa7-fa80-b1a9d9a856ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (10109, 512), Labels shape: (10109,)\n",
      "Test data shape: (3370, 512), Labels shape: (3370,)\n",
      "Validation data shape: (3370, 512), Labels shape: (3370,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# DONE: Split tweets and labels in Train/Test/Validation 60/20/20\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_tweets, labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training data shape: {}, Labels shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Test data shape: {}, Labels shape: {}\".format(X_test.shape, y_test.shape))\n",
    "print(\"Validation data shape: {}, Labels shape: {}\".format(X_val.shape, y_val.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cbMJFUqg5Xg7"
   },
   "source": [
    "In order to feed the data into the model, we must create Dataset objects for it, allowing the creation of Dataloaders. The Dataset retrieves both the features and labels of the data. While training a model, we want to feed the data in batches and reshuffle it at every epoch to reduce model overfitting. Dataloaders offer an API to do that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qCp0TtD3qr8J"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# DONE: Create a CustomDataset class for our Tweet data\n",
    "\n",
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.encodings[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "# Create the Datasets\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "# DataLoader for batching and parallel data loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DJZFPf_WwGSO"
   },
   "source": [
    "### c) Train the Base Model\n",
    "\n",
    "**1.** Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bI2QIPUGexBa",
    "outputId": "ce6f7b2d-0634-4695-c21e-b38bb9b19093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/316], Loss: 0.0059\n",
      "Epoch [1/20], Step [2/316], Loss: 0.0025\n",
      "Epoch [1/20], Step [3/316], Loss: 0.0092\n",
      "Epoch [1/20], Step [4/316], Loss: 0.0184\n",
      "Epoch [1/20], Step [5/316], Loss: 0.1388\n",
      "Epoch [1/20], Step [6/316], Loss: 0.0101\n",
      "Epoch [1/20], Step [7/316], Loss: 0.0110\n",
      "Epoch [1/20], Step [8/316], Loss: 0.0020\n",
      "Epoch [1/20], Step [9/316], Loss: 0.0024\n",
      "Epoch [1/20], Step [10/316], Loss: 0.0062\n",
      "Epoch [1/20], Step [11/316], Loss: 0.0087\n",
      "Epoch [1/20], Step [12/316], Loss: 0.0133\n",
      "Epoch [1/20], Step [13/316], Loss: 0.0439\n",
      "Epoch [1/20], Step [14/316], Loss: 0.0097\n",
      "Epoch [1/20], Step [15/316], Loss: 0.0023\n",
      "Epoch [1/20], Step [16/316], Loss: 0.0925\n",
      "Epoch [1/20], Step [17/316], Loss: 0.0009\n",
      "Epoch [1/20], Step [18/316], Loss: 0.0037\n",
      "Epoch [1/20], Step [19/316], Loss: 0.0045\n",
      "Epoch [1/20], Step [20/316], Loss: 0.0023\n",
      "Epoch [1/20], Step [21/316], Loss: 0.1510\n",
      "Epoch [1/20], Step [22/316], Loss: 0.0004\n",
      "Epoch [1/20], Step [23/316], Loss: 0.0067\n",
      "Epoch [1/20], Step [24/316], Loss: 0.0023\n",
      "Epoch [1/20], Step [25/316], Loss: 0.0074\n",
      "Epoch [1/20], Step [26/316], Loss: 0.0039\n",
      "Epoch [1/20], Step [27/316], Loss: 0.0013\n",
      "Epoch [1/20], Step [28/316], Loss: 0.1488\n",
      "Epoch [1/20], Step [29/316], Loss: 0.0120\n",
      "Epoch [1/20], Step [30/316], Loss: 0.0021\n",
      "Epoch [1/20], Step [31/316], Loss: 0.0069\n",
      "Epoch [1/20], Step [32/316], Loss: 0.0076\n",
      "Epoch [1/20], Step [33/316], Loss: 0.0025\n",
      "Epoch [1/20], Step [34/316], Loss: 0.0034\n",
      "Epoch [1/20], Step [35/316], Loss: 0.0099\n",
      "Epoch [1/20], Step [36/316], Loss: 0.0058\n",
      "Epoch [1/20], Step [37/316], Loss: 0.1722\n",
      "Epoch [1/20], Step [38/316], Loss: 0.0463\n",
      "Epoch [1/20], Step [39/316], Loss: 0.0038\n",
      "Epoch [1/20], Step [40/316], Loss: 0.0068\n",
      "Epoch [1/20], Step [41/316], Loss: 0.0007\n",
      "Epoch [1/20], Step [42/316], Loss: 0.0055\n",
      "Epoch [1/20], Step [43/316], Loss: 0.0052\n",
      "Epoch [1/20], Step [44/316], Loss: 0.0084\n",
      "Epoch [1/20], Step [45/316], Loss: 0.0016\n",
      "Epoch [1/20], Step [46/316], Loss: 0.0057\n",
      "Epoch [1/20], Step [47/316], Loss: 0.0065\n",
      "Epoch [1/20], Step [48/316], Loss: 0.0192\n",
      "Epoch [1/20], Step [49/316], Loss: 0.0326\n",
      "Epoch [1/20], Step [50/316], Loss: 0.0828\n",
      "Epoch [1/20], Step [51/316], Loss: 0.1056\n",
      "Epoch [1/20], Step [52/316], Loss: 0.0007\n",
      "Epoch [1/20], Step [53/316], Loss: 0.0045\n",
      "Epoch [1/20], Step [54/316], Loss: 0.0030\n",
      "Epoch [1/20], Step [55/316], Loss: 0.0043\n",
      "Epoch [1/20], Step [56/316], Loss: 0.0047\n",
      "Epoch [1/20], Step [57/316], Loss: 0.0014\n",
      "Epoch [1/20], Step [58/316], Loss: 0.0026\n",
      "Epoch [1/20], Step [59/316], Loss: 0.0036\n",
      "Epoch [1/20], Step [60/316], Loss: 0.0117\n",
      "Epoch [1/20], Step [61/316], Loss: 0.0042\n",
      "Epoch [1/20], Step [62/316], Loss: 0.0019\n",
      "Epoch [1/20], Step [63/316], Loss: 0.1617\n",
      "Epoch [1/20], Step [64/316], Loss: 0.0144\n",
      "Epoch [1/20], Step [65/316], Loss: 0.0171\n",
      "Epoch [1/20], Step [66/316], Loss: 0.0031\n",
      "Epoch [1/20], Step [67/316], Loss: 0.1024\n",
      "Epoch [1/20], Step [68/316], Loss: 0.0061\n",
      "Epoch [1/20], Step [69/316], Loss: 0.1192\n",
      "Epoch [1/20], Step [70/316], Loss: 0.0058\n",
      "Epoch [1/20], Step [71/316], Loss: 0.0198\n",
      "Epoch [1/20], Step [72/316], Loss: 0.0041\n",
      "Epoch [1/20], Step [73/316], Loss: 0.0129\n",
      "Epoch [1/20], Step [74/316], Loss: 0.0054\n",
      "Epoch [1/20], Step [75/316], Loss: 0.0076\n",
      "Epoch [1/20], Step [76/316], Loss: 0.1332\n",
      "Epoch [1/20], Step [77/316], Loss: 0.0003\n",
      "Epoch [1/20], Step [78/316], Loss: 0.1348\n",
      "Epoch [1/20], Step [79/316], Loss: 0.0583\n",
      "Epoch [1/20], Step [80/316], Loss: 0.0181\n",
      "Epoch [1/20], Step [81/316], Loss: 0.0535\n",
      "Epoch [1/20], Step [82/316], Loss: 0.0836\n",
      "Epoch [1/20], Step [83/316], Loss: 0.0562\n",
      "Epoch [1/20], Step [84/316], Loss: 0.0140\n",
      "Epoch [1/20], Step [85/316], Loss: 0.0083\n",
      "Epoch [1/20], Step [86/316], Loss: 0.0358\n",
      "Epoch [1/20], Step [87/316], Loss: 0.0078\n",
      "Epoch [1/20], Step [88/316], Loss: 0.1517\n",
      "Epoch [1/20], Step [89/316], Loss: 0.0488\n",
      "Epoch [1/20], Step [90/316], Loss: 0.1282\n",
      "Epoch [1/20], Step [91/316], Loss: 0.0106\n",
      "Epoch [1/20], Step [92/316], Loss: 0.0090\n",
      "Epoch [1/20], Step [93/316], Loss: 0.2264\n",
      "Epoch [1/20], Step [94/316], Loss: 0.0115\n",
      "Epoch [1/20], Step [95/316], Loss: 0.0063\n",
      "Epoch [1/20], Step [96/316], Loss: 0.1311\n",
      "Epoch [1/20], Step [97/316], Loss: 0.0056\n",
      "Epoch [1/20], Step [98/316], Loss: 0.1058\n",
      "Epoch [1/20], Step [99/316], Loss: 0.0021\n",
      "Epoch [1/20], Step [100/316], Loss: 0.0053\n",
      "Epoch [1/20], Step [101/316], Loss: 0.0073\n",
      "Epoch [1/20], Step [102/316], Loss: 0.0021\n",
      "Epoch [1/20], Step [103/316], Loss: 0.0005\n",
      "Epoch [1/20], Step [104/316], Loss: 0.0111\n",
      "Epoch [1/20], Step [105/316], Loss: 0.0184\n",
      "Epoch [1/20], Step [106/316], Loss: 0.0073\n",
      "Epoch [1/20], Step [107/316], Loss: 0.0250\n",
      "Epoch [1/20], Step [108/316], Loss: 0.0044\n",
      "Epoch [1/20], Step [109/316], Loss: 0.0093\n",
      "Epoch [1/20], Step [110/316], Loss: 0.0284\n",
      "Epoch [1/20], Step [111/316], Loss: 0.0063\n",
      "Epoch [1/20], Step [112/316], Loss: 0.0050\n",
      "Epoch [1/20], Step [113/316], Loss: 0.0444\n",
      "Epoch [1/20], Step [114/316], Loss: 0.0236\n",
      "Epoch [1/20], Step [115/316], Loss: 0.0068\n",
      "Epoch [1/20], Step [116/316], Loss: 0.3449\n",
      "Epoch [1/20], Step [117/316], Loss: 0.0019\n",
      "Epoch [1/20], Step [118/316], Loss: 0.0071\n",
      "Epoch [1/20], Step [119/316], Loss: 0.0123\n",
      "Epoch [1/20], Step [120/316], Loss: 0.0039\n",
      "Epoch [1/20], Step [121/316], Loss: 0.0052\n",
      "Epoch [1/20], Step [122/316], Loss: 0.0043\n",
      "Epoch [1/20], Step [123/316], Loss: 0.0059\n",
      "Epoch [1/20], Step [124/316], Loss: 0.0044\n",
      "Epoch [1/20], Step [125/316], Loss: 0.0049\n",
      "Epoch [1/20], Step [126/316], Loss: 0.0023\n",
      "Epoch [1/20], Step [127/316], Loss: 0.0081\n",
      "Epoch [1/20], Step [128/316], Loss: 0.0017\n",
      "Epoch [1/20], Step [129/316], Loss: 0.0180\n",
      "Epoch [1/20], Step [130/316], Loss: 0.0024\n",
      "Epoch [1/20], Step [131/316], Loss: 0.0044\n",
      "Epoch [1/20], Step [132/316], Loss: 0.0063\n",
      "Epoch [1/20], Step [133/316], Loss: 0.0076\n",
      "Epoch [1/20], Step [134/316], Loss: 0.0043\n",
      "Epoch [1/20], Step [135/316], Loss: 0.0029\n",
      "Epoch [1/20], Step [136/316], Loss: 0.0121\n",
      "Epoch [1/20], Step [137/316], Loss: 0.0224\n",
      "Epoch [1/20], Step [138/316], Loss: 0.0072\n",
      "Epoch [1/20], Step [139/316], Loss: 0.0013\n",
      "Epoch [1/20], Step [140/316], Loss: 0.0539\n",
      "Epoch [1/20], Step [141/316], Loss: 0.0067\n",
      "Epoch [1/20], Step [142/316], Loss: 0.0091\n",
      "Epoch [1/20], Step [143/316], Loss: 0.1333\n",
      "Epoch [1/20], Step [144/316], Loss: 0.0037\n",
      "Epoch [1/20], Step [145/316], Loss: 0.0029\n",
      "Epoch [1/20], Step [146/316], Loss: 0.0022\n",
      "Epoch [1/20], Step [147/316], Loss: 0.0059\n",
      "Epoch [1/20], Step [148/316], Loss: 0.1777\n",
      "Epoch [1/20], Step [149/316], Loss: 0.1366\n",
      "Epoch [1/20], Step [150/316], Loss: 0.0010\n",
      "Epoch [1/20], Step [151/316], Loss: 0.1309\n",
      "Epoch [1/20], Step [152/316], Loss: 0.0052\n",
      "Epoch [1/20], Step [153/316], Loss: 0.0076\n",
      "Epoch [1/20], Step [154/316], Loss: 0.0041\n",
      "Epoch [1/20], Step [155/316], Loss: 0.0052\n",
      "Epoch [1/20], Step [156/316], Loss: 0.0204\n",
      "Epoch [1/20], Step [157/316], Loss: 0.0279\n",
      "Epoch [1/20], Step [158/316], Loss: 0.0014\n",
      "Epoch [1/20], Step [159/316], Loss: 0.0045\n",
      "Epoch [1/20], Step [160/316], Loss: 0.0494\n",
      "Epoch [1/20], Step [161/316], Loss: 0.0035\n",
      "Epoch [1/20], Step [162/316], Loss: 0.0240\n",
      "Epoch [1/20], Step [163/316], Loss: 0.0047\n",
      "Epoch [1/20], Step [164/316], Loss: 0.0046\n",
      "Epoch [1/20], Step [165/316], Loss: 0.0601\n",
      "Epoch [1/20], Step [166/316], Loss: 0.0034\n",
      "Epoch [1/20], Step [167/316], Loss: 0.0380\n",
      "Epoch [1/20], Step [168/316], Loss: 0.0076\n",
      "Epoch [1/20], Step [169/316], Loss: 0.0182\n",
      "Epoch [1/20], Step [170/316], Loss: 0.0031\n",
      "Epoch [1/20], Step [171/316], Loss: 0.0053\n",
      "Epoch [1/20], Step [172/316], Loss: 0.0071\n",
      "Epoch [1/20], Step [173/316], Loss: 0.0045\n",
      "Epoch [1/20], Step [174/316], Loss: 0.0056\n",
      "Epoch [1/20], Step [175/316], Loss: 0.0061\n",
      "Epoch [1/20], Step [176/316], Loss: 0.0025\n",
      "Epoch [1/20], Step [177/316], Loss: 0.0082\n",
      "Epoch [1/20], Step [178/316], Loss: 0.0073\n",
      "Epoch [1/20], Step [179/316], Loss: 0.0750\n",
      "Epoch [1/20], Step [180/316], Loss: 0.0607\n",
      "Epoch [1/20], Step [181/316], Loss: 0.1383\n",
      "Epoch [1/20], Step [182/316], Loss: 0.0334\n",
      "Epoch [1/20], Step [183/316], Loss: 0.0065\n",
      "Epoch [1/20], Step [184/316], Loss: 0.0051\n",
      "Epoch [1/20], Step [185/316], Loss: 0.0032\n",
      "Epoch [1/20], Step [186/316], Loss: 0.0083\n",
      "Epoch [1/20], Step [187/316], Loss: 0.0253\n",
      "Epoch [1/20], Step [188/316], Loss: 0.0157\n",
      "Epoch [1/20], Step [189/316], Loss: 0.0444\n",
      "Epoch [1/20], Step [190/316], Loss: 0.0115\n",
      "Epoch [1/20], Step [191/316], Loss: 0.0139\n",
      "Epoch [1/20], Step [192/316], Loss: 0.0036\n",
      "Epoch [1/20], Step [193/316], Loss: 0.0064\n",
      "Epoch [1/20], Step [194/316], Loss: 0.0103\n",
      "Epoch [1/20], Step [195/316], Loss: 0.0082\n",
      "Epoch [1/20], Step [196/316], Loss: 0.0150\n",
      "Epoch [1/20], Step [197/316], Loss: 0.0835\n",
      "Epoch [1/20], Step [198/316], Loss: 0.0227\n",
      "Epoch [1/20], Step [199/316], Loss: 0.0043\n",
      "Epoch [1/20], Step [200/316], Loss: 0.0037\n",
      "Epoch [1/20], Step [201/316], Loss: 0.0201\n",
      "Epoch [1/20], Step [202/316], Loss: 0.0220\n",
      "Epoch [1/20], Step [203/316], Loss: 0.0018\n",
      "Epoch [1/20], Step [204/316], Loss: 0.0021\n",
      "Epoch [1/20], Step [205/316], Loss: 0.0085\n",
      "Epoch [1/20], Step [206/316], Loss: 0.0083\n",
      "Epoch [1/20], Step [207/316], Loss: 0.0034\n",
      "Epoch [1/20], Step [208/316], Loss: 0.0052\n",
      "Epoch [1/20], Step [209/316], Loss: 0.0011\n",
      "Epoch [1/20], Step [210/316], Loss: 0.1632\n",
      "Epoch [1/20], Step [211/316], Loss: 0.0081\n",
      "Epoch [1/20], Step [212/316], Loss: 0.0040\n",
      "Epoch [1/20], Step [213/316], Loss: 0.0073\n",
      "Epoch [1/20], Step [214/316], Loss: 0.0041\n",
      "Epoch [1/20], Step [215/316], Loss: 0.0218\n",
      "Epoch [1/20], Step [216/316], Loss: 0.0761\n",
      "Epoch [1/20], Step [217/316], Loss: 0.0975\n",
      "Epoch [1/20], Step [218/316], Loss: 0.2106\n",
      "Epoch [1/20], Step [219/316], Loss: 0.0055\n",
      "Epoch [1/20], Step [220/316], Loss: 0.0106\n",
      "Epoch [1/20], Step [221/316], Loss: 0.1619\n",
      "Epoch [1/20], Step [222/316], Loss: 0.0050\n",
      "Epoch [1/20], Step [223/316], Loss: 0.0102\n",
      "Epoch [1/20], Step [224/316], Loss: 0.1009\n",
      "Epoch [1/20], Step [225/316], Loss: 0.0018\n",
      "Epoch [1/20], Step [226/316], Loss: 0.0834\n",
      "Epoch [1/20], Step [227/316], Loss: 0.1604\n",
      "Epoch [1/20], Step [228/316], Loss: 0.0659\n",
      "Epoch [1/20], Step [229/316], Loss: 0.0109\n",
      "Epoch [1/20], Step [230/316], Loss: 0.0100\n",
      "Epoch [1/20], Step [231/316], Loss: 0.0035\n",
      "Epoch [1/20], Step [232/316], Loss: 0.0060\n",
      "Epoch [1/20], Step [233/316], Loss: 0.0038\n",
      "Epoch [1/20], Step [234/316], Loss: 0.1526\n",
      "Epoch [1/20], Step [235/316], Loss: 0.0126\n",
      "Epoch [1/20], Step [236/316], Loss: 0.0257\n",
      "Epoch [1/20], Step [237/316], Loss: 0.0074\n",
      "Epoch [1/20], Step [238/316], Loss: 0.0060\n",
      "Epoch [1/20], Step [239/316], Loss: 0.0037\n",
      "Epoch [1/20], Step [240/316], Loss: 0.0029\n",
      "Epoch [1/20], Step [241/316], Loss: 0.0142\n",
      "Epoch [1/20], Step [242/316], Loss: 0.0593\n",
      "Epoch [1/20], Step [243/316], Loss: 0.0293\n",
      "Epoch [1/20], Step [244/316], Loss: 0.0067\n",
      "Epoch [1/20], Step [245/316], Loss: 0.1113\n",
      "Epoch [1/20], Step [246/316], Loss: 0.0045\n",
      "Epoch [1/20], Step [247/316], Loss: 0.0091\n",
      "Epoch [1/20], Step [248/316], Loss: 0.1486\n",
      "Epoch [1/20], Step [249/316], Loss: 0.0729\n",
      "Epoch [1/20], Step [250/316], Loss: 0.0128\n",
      "Epoch [1/20], Step [251/316], Loss: 0.0026\n",
      "Epoch [1/20], Step [252/316], Loss: 0.0049\n",
      "Epoch [1/20], Step [253/316], Loss: 0.0037\n",
      "Epoch [1/20], Step [254/316], Loss: 0.0136\n",
      "Epoch [1/20], Step [255/316], Loss: 0.0044\n",
      "Epoch [1/20], Step [256/316], Loss: 0.1136\n",
      "Epoch [1/20], Step [257/316], Loss: 0.0043\n",
      "Epoch [1/20], Step [258/316], Loss: 0.0347\n",
      "Epoch [1/20], Step [259/316], Loss: 0.0084\n",
      "Epoch [1/20], Step [260/316], Loss: 0.0089\n",
      "Epoch [1/20], Step [261/316], Loss: 0.0122\n",
      "Epoch [1/20], Step [262/316], Loss: 0.0042\n",
      "Epoch [1/20], Step [263/316], Loss: 0.0083\n",
      "Epoch [1/20], Step [264/316], Loss: 0.0058\n",
      "Epoch [1/20], Step [265/316], Loss: 0.1231\n",
      "Epoch [1/20], Step [266/316], Loss: 0.0511\n",
      "Epoch [1/20], Step [267/316], Loss: 0.0513\n",
      "Epoch [1/20], Step [268/316], Loss: 0.1162\n",
      "Epoch [1/20], Step [269/316], Loss: 0.0032\n",
      "Epoch [1/20], Step [270/316], Loss: 0.0046\n",
      "Epoch [1/20], Step [271/316], Loss: 0.0058\n",
      "Epoch [1/20], Step [272/316], Loss: 0.0062\n",
      "Epoch [1/20], Step [273/316], Loss: 0.0089\n",
      "Epoch [1/20], Step [274/316], Loss: 0.0399\n",
      "Epoch [1/20], Step [275/316], Loss: 0.0038\n",
      "Epoch [1/20], Step [276/316], Loss: 0.0292\n",
      "Epoch [1/20], Step [277/316], Loss: 0.0059\n",
      "Epoch [1/20], Step [278/316], Loss: 0.1241\n",
      "Epoch [1/20], Step [279/316], Loss: 0.0026\n",
      "Epoch [1/20], Step [280/316], Loss: 0.0216\n",
      "Epoch [1/20], Step [281/316], Loss: 0.0074\n",
      "Epoch [1/20], Step [282/316], Loss: 0.0081\n",
      "Epoch [1/20], Step [283/316], Loss: 0.0044\n",
      "Epoch [1/20], Step [284/316], Loss: 0.0042\n",
      "Epoch [1/20], Step [285/316], Loss: 0.0019\n",
      "Epoch [1/20], Step [286/316], Loss: 0.0050\n",
      "Epoch [1/20], Step [287/316], Loss: 0.0420\n",
      "Epoch [1/20], Step [288/316], Loss: 0.1637\n",
      "Epoch [1/20], Step [289/316], Loss: 0.0016\n",
      "Epoch [1/20], Step [290/316], Loss: 0.0942\n",
      "Epoch [1/20], Step [291/316], Loss: 0.0046\n",
      "Epoch [1/20], Step [292/316], Loss: 0.0075\n",
      "Epoch [1/20], Step [293/316], Loss: 0.0117\n",
      "Epoch [1/20], Step [294/316], Loss: 0.0054\n",
      "Epoch [1/20], Step [295/316], Loss: 0.0058\n",
      "Epoch [1/20], Step [296/316], Loss: 0.0807\n",
      "Epoch [1/20], Step [297/316], Loss: 0.0044\n",
      "Epoch [1/20], Step [298/316], Loss: 0.0044\n",
      "Epoch [1/20], Step [299/316], Loss: 0.0027\n",
      "Epoch [1/20], Step [300/316], Loss: 0.0029\n",
      "Epoch [1/20], Step [301/316], Loss: 0.0031\n",
      "Epoch [1/20], Step [302/316], Loss: 0.0246\n",
      "Epoch [1/20], Step [303/316], Loss: 0.0043\n",
      "Epoch [1/20], Step [304/316], Loss: 0.0064\n",
      "Epoch [1/20], Step [305/316], Loss: 0.0036\n",
      "Epoch [1/20], Step [306/316], Loss: 0.0029\n",
      "Epoch [1/20], Step [307/316], Loss: 0.0037\n",
      "Epoch [1/20], Step [308/316], Loss: 0.0052\n",
      "Epoch [1/20], Step [309/316], Loss: 0.1426\n",
      "Epoch [1/20], Step [310/316], Loss: 0.1502\n",
      "Epoch [1/20], Step [311/316], Loss: 0.0110\n",
      "Epoch [1/20], Step [312/316], Loss: 0.0048\n",
      "Epoch [1/20], Step [313/316], Loss: 0.0030\n",
      "Epoch [1/20], Step [314/316], Loss: 0.0047\n",
      "Epoch [1/20], Step [315/316], Loss: 0.0020\n",
      "Epoch [1/20], Step [316/316], Loss: 0.0034\n",
      "Epoch [1/20], Train Loss: 0.0290\n",
      "Epoch [1/20], Validation Loss: 0.1555\n",
      "Epoch [1/20], Validation Accuracy: 0.8122\n",
      "Epoch [2/20], Step [1/316], Loss: 0.2629\n",
      "Epoch [2/20], Step [2/316], Loss: 0.0092\n",
      "Epoch [2/20], Step [3/316], Loss: 0.0017\n",
      "Epoch [2/20], Step [4/316], Loss: 0.0658\n",
      "Epoch [2/20], Step [5/316], Loss: 0.0029\n",
      "Epoch [2/20], Step [6/316], Loss: 0.0031\n",
      "Epoch [2/20], Step [7/316], Loss: 0.1227\n",
      "Epoch [2/20], Step [8/316], Loss: 0.0025\n",
      "Epoch [2/20], Step [9/316], Loss: 0.0052\n",
      "Epoch [2/20], Step [10/316], Loss: 0.0043\n",
      "Epoch [2/20], Step [11/316], Loss: 0.0037\n",
      "Epoch [2/20], Step [12/316], Loss: 0.0131\n",
      "Epoch [2/20], Step [13/316], Loss: 0.0054\n",
      "Epoch [2/20], Step [14/316], Loss: 0.0065\n",
      "Epoch [2/20], Step [15/316], Loss: 0.0056\n",
      "Epoch [2/20], Step [16/316], Loss: 0.0056\n",
      "Epoch [2/20], Step [17/316], Loss: 0.0040\n",
      "Epoch [2/20], Step [18/316], Loss: 0.0020\n",
      "Epoch [2/20], Step [19/316], Loss: 0.0062\n",
      "Epoch [2/20], Step [20/316], Loss: 0.0116\n",
      "Epoch [2/20], Step [21/316], Loss: 0.0052\n",
      "Epoch [2/20], Step [22/316], Loss: 0.1662\n",
      "Epoch [2/20], Step [23/316], Loss: 0.0099\n",
      "Epoch [2/20], Step [24/316], Loss: 0.0029\n",
      "Epoch [2/20], Step [25/316], Loss: 0.0011\n",
      "Epoch [2/20], Step [26/316], Loss: 0.0221\n",
      "Epoch [2/20], Step [27/316], Loss: 0.0112\n",
      "Epoch [2/20], Step [28/316], Loss: 0.0023\n",
      "Epoch [2/20], Step [29/316], Loss: 0.1572\n",
      "Epoch [2/20], Step [30/316], Loss: 0.0030\n",
      "Epoch [2/20], Step [31/316], Loss: 0.0880\n",
      "Epoch [2/20], Step [32/316], Loss: 0.1729\n",
      "Epoch [2/20], Step [33/316], Loss: 0.0013\n",
      "Epoch [2/20], Step [34/316], Loss: 0.1027\n",
      "Epoch [2/20], Step [35/316], Loss: 0.0708\n",
      "Epoch [2/20], Step [36/316], Loss: 0.0027\n",
      "Epoch [2/20], Step [37/316], Loss: 0.0186\n",
      "Epoch [2/20], Step [38/316], Loss: 0.1074\n",
      "Epoch [2/20], Step [39/316], Loss: 0.0040\n",
      "Epoch [2/20], Step [40/316], Loss: 0.0068\n",
      "Epoch [2/20], Step [41/316], Loss: 0.0255\n",
      "Epoch [2/20], Step [42/316], Loss: 0.0073\n",
      "Epoch [2/20], Step [43/316], Loss: 0.0804\n",
      "Epoch [2/20], Step [44/316], Loss: 0.0416\n",
      "Epoch [2/20], Step [45/316], Loss: 0.2046\n",
      "Epoch [2/20], Step [46/316], Loss: 0.0101\n",
      "Epoch [2/20], Step [47/316], Loss: 0.0050\n",
      "Epoch [2/20], Step [48/316], Loss: 0.0069\n",
      "Epoch [2/20], Step [49/316], Loss: 0.0864\n",
      "Epoch [2/20], Step [50/316], Loss: 0.0070\n",
      "Epoch [2/20], Step [51/316], Loss: 0.0058\n",
      "Epoch [2/20], Step [52/316], Loss: 0.1536\n",
      "Epoch [2/20], Step [53/316], Loss: 0.0630\n",
      "Epoch [2/20], Step [54/316], Loss: 0.0982\n",
      "Epoch [2/20], Step [55/316], Loss: 0.0291\n",
      "Epoch [2/20], Step [56/316], Loss: 0.0257\n",
      "Epoch [2/20], Step [57/316], Loss: 0.0338\n",
      "Epoch [2/20], Step [58/316], Loss: 0.0194\n",
      "Epoch [2/20], Step [59/316], Loss: 0.1248\n",
      "Epoch [2/20], Step [60/316], Loss: 0.0031\n",
      "Epoch [2/20], Step [61/316], Loss: 0.1241\n",
      "Epoch [2/20], Step [62/316], Loss: 0.1354\n",
      "Epoch [2/20], Step [63/316], Loss: 0.0000\n",
      "Epoch [2/20], Step [64/316], Loss: 0.0130\n",
      "Epoch [2/20], Step [65/316], Loss: 0.0102\n",
      "Epoch [2/20], Step [66/316], Loss: 0.0214\n",
      "Epoch [2/20], Step [67/316], Loss: 0.0065\n",
      "Epoch [2/20], Step [68/316], Loss: 0.1143\n",
      "Epoch [2/20], Step [69/316], Loss: 0.0105\n",
      "Epoch [2/20], Step [70/316], Loss: 0.0070\n",
      "Epoch [2/20], Step [71/316], Loss: 0.0310\n",
      "Epoch [2/20], Step [72/316], Loss: 0.0097\n",
      "Epoch [2/20], Step [73/316], Loss: 0.0396\n",
      "Epoch [2/20], Step [74/316], Loss: 0.0457\n",
      "Epoch [2/20], Step [75/316], Loss: 0.0077\n",
      "Epoch [2/20], Step [76/316], Loss: 0.0238\n",
      "Epoch [2/20], Step [77/316], Loss: 0.0082\n",
      "Epoch [2/20], Step [78/316], Loss: 0.0040\n",
      "Epoch [2/20], Step [79/316], Loss: 0.0533\n",
      "Epoch [2/20], Step [80/316], Loss: 0.0062\n",
      "Epoch [2/20], Step [81/316], Loss: 0.1484\n",
      "Epoch [2/20], Step [82/316], Loss: 0.0205\n",
      "Epoch [2/20], Step [83/316], Loss: 0.1258\n",
      "Epoch [2/20], Step [84/316], Loss: 0.0824\n",
      "Epoch [2/20], Step [85/316], Loss: 0.0035\n",
      "Epoch [2/20], Step [86/316], Loss: 0.0888\n",
      "Epoch [2/20], Step [87/316], Loss: 0.0038\n",
      "Epoch [2/20], Step [88/316], Loss: 0.0037\n",
      "Epoch [2/20], Step [89/316], Loss: 0.1068\n",
      "Epoch [2/20], Step [90/316], Loss: 0.2305\n",
      "Epoch [2/20], Step [91/316], Loss: 0.0022\n",
      "Epoch [2/20], Step [92/316], Loss: 0.1244\n",
      "Epoch [2/20], Step [93/316], Loss: 0.1276\n",
      "Epoch [2/20], Step [94/316], Loss: 0.0063\n",
      "Epoch [2/20], Step [95/316], Loss: 0.0106\n",
      "Epoch [2/20], Step [96/316], Loss: 0.0084\n",
      "Epoch [2/20], Step [97/316], Loss: 0.0078\n",
      "Epoch [2/20], Step [98/316], Loss: 0.0057\n",
      "Epoch [2/20], Step [99/316], Loss: 0.0909\n",
      "Epoch [2/20], Step [100/316], Loss: 0.0269\n",
      "Epoch [2/20], Step [101/316], Loss: 0.0058\n",
      "Epoch [2/20], Step [102/316], Loss: 0.0069\n",
      "Epoch [2/20], Step [103/316], Loss: 0.0041\n",
      "Epoch [2/20], Step [104/316], Loss: 0.0141\n",
      "Epoch [2/20], Step [105/316], Loss: 0.1028\n",
      "Epoch [2/20], Step [106/316], Loss: 0.0085\n",
      "Epoch [2/20], Step [107/316], Loss: 0.1175\n",
      "Epoch [2/20], Step [108/316], Loss: 0.0089\n",
      "Epoch [2/20], Step [109/316], Loss: 0.0096\n",
      "Epoch [2/20], Step [110/316], Loss: 0.0106\n",
      "Epoch [2/20], Step [111/316], Loss: 0.0353\n",
      "Epoch [2/20], Step [112/316], Loss: 0.0132\n",
      "Epoch [2/20], Step [113/316], Loss: 0.0004\n",
      "Epoch [2/20], Step [114/316], Loss: 0.0113\n",
      "Epoch [2/20], Step [115/316], Loss: 0.0020\n",
      "Epoch [2/20], Step [116/316], Loss: 0.0249\n",
      "Epoch [2/20], Step [117/316], Loss: 0.0119\n",
      "Epoch [2/20], Step [118/316], Loss: 0.0067\n",
      "Epoch [2/20], Step [119/316], Loss: 0.2665\n",
      "Epoch [2/20], Step [120/316], Loss: 0.0557\n",
      "Epoch [2/20], Step [121/316], Loss: 0.0721\n",
      "Epoch [2/20], Step [122/316], Loss: 0.0190\n",
      "Epoch [2/20], Step [123/316], Loss: 0.0032\n",
      "Epoch [2/20], Step [124/316], Loss: 0.0082\n",
      "Epoch [2/20], Step [125/316], Loss: 0.0079\n",
      "Epoch [2/20], Step [126/316], Loss: 0.0099\n",
      "Epoch [2/20], Step [127/316], Loss: 0.0025\n",
      "Epoch [2/20], Step [128/316], Loss: 0.0021\n",
      "Epoch [2/20], Step [129/316], Loss: 0.0938\n",
      "Epoch [2/20], Step [130/316], Loss: 0.0149\n",
      "Epoch [2/20], Step [131/316], Loss: 0.0045\n",
      "Epoch [2/20], Step [132/316], Loss: 0.0240\n",
      "Epoch [2/20], Step [133/316], Loss: 0.2389\n",
      "Epoch [2/20], Step [134/316], Loss: 0.0081\n",
      "Epoch [2/20], Step [135/316], Loss: 0.0424\n",
      "Epoch [2/20], Step [136/316], Loss: 0.0018\n",
      "Epoch [2/20], Step [137/316], Loss: 0.0030\n",
      "Epoch [2/20], Step [138/316], Loss: 0.0015\n",
      "Epoch [2/20], Step [139/316], Loss: 0.0044\n",
      "Epoch [2/20], Step [140/316], Loss: 0.0313\n",
      "Epoch [2/20], Step [141/316], Loss: 0.0030\n",
      "Epoch [2/20], Step [142/316], Loss: 0.0058\n",
      "Epoch [2/20], Step [143/316], Loss: 0.1103\n",
      "Epoch [2/20], Step [144/316], Loss: 0.0055\n",
      "Epoch [2/20], Step [145/316], Loss: 0.1520\n",
      "Epoch [2/20], Step [146/316], Loss: 0.0992\n",
      "Epoch [2/20], Step [147/316], Loss: 0.0147\n",
      "Epoch [2/20], Step [148/316], Loss: 0.0084\n",
      "Epoch [2/20], Step [149/316], Loss: 0.0033\n",
      "Epoch [2/20], Step [150/316], Loss: 0.0295\n",
      "Epoch [2/20], Step [151/316], Loss: 0.1313\n",
      "Epoch [2/20], Step [152/316], Loss: 0.0130\n",
      "Epoch [2/20], Step [153/316], Loss: 0.0794\n",
      "Epoch [2/20], Step [154/316], Loss: 0.0326\n",
      "Epoch [2/20], Step [155/316], Loss: 0.0080\n",
      "Epoch [2/20], Step [156/316], Loss: 0.0067\n",
      "Epoch [2/20], Step [157/316], Loss: 0.0238\n",
      "Epoch [2/20], Step [158/316], Loss: 0.0099\n",
      "Epoch [2/20], Step [159/316], Loss: 0.0063\n",
      "Epoch [2/20], Step [160/316], Loss: 0.0066\n",
      "Epoch [2/20], Step [161/316], Loss: 0.1922\n",
      "Epoch [2/20], Step [162/316], Loss: 0.0007\n",
      "Epoch [2/20], Step [163/316], Loss: 0.0376\n",
      "Epoch [2/20], Step [164/316], Loss: 0.0021\n",
      "Epoch [2/20], Step [165/316], Loss: 0.0389\n",
      "Epoch [2/20], Step [166/316], Loss: 0.0108\n",
      "Epoch [2/20], Step [167/316], Loss: 0.0049\n",
      "Epoch [2/20], Step [168/316], Loss: 0.0206\n",
      "Epoch [2/20], Step [169/316], Loss: 0.1537\n",
      "Epoch [2/20], Step [170/316], Loss: 0.0111\n",
      "Epoch [2/20], Step [171/316], Loss: 0.1847\n",
      "Epoch [2/20], Step [172/316], Loss: 0.0062\n",
      "Epoch [2/20], Step [173/316], Loss: 0.0492\n",
      "Epoch [2/20], Step [174/316], Loss: 0.0062\n",
      "Epoch [2/20], Step [175/316], Loss: 0.0152\n",
      "Epoch [2/20], Step [176/316], Loss: 0.2106\n",
      "Epoch [2/20], Step [177/316], Loss: 0.0102\n",
      "Epoch [2/20], Step [178/316], Loss: 0.0385\n",
      "Epoch [2/20], Step [179/316], Loss: 0.0034\n",
      "Epoch [2/20], Step [180/316], Loss: 0.0021\n",
      "Epoch [2/20], Step [181/316], Loss: 0.0369\n",
      "Epoch [2/20], Step [182/316], Loss: 0.0079\n",
      "Epoch [2/20], Step [183/316], Loss: 0.0282\n",
      "Epoch [2/20], Step [184/316], Loss: 0.0090\n",
      "Epoch [2/20], Step [185/316], Loss: 0.0027\n",
      "Epoch [2/20], Step [186/316], Loss: 0.0116\n",
      "Epoch [2/20], Step [187/316], Loss: 0.2355\n",
      "Epoch [2/20], Step [188/316], Loss: 0.0084\n",
      "Epoch [2/20], Step [189/316], Loss: 0.0198\n",
      "Epoch [2/20], Step [190/316], Loss: 0.0042\n",
      "Epoch [2/20], Step [191/316], Loss: 0.0676\n",
      "Epoch [2/20], Step [192/316], Loss: 0.0046\n",
      "Epoch [2/20], Step [193/316], Loss: 0.0083\n",
      "Epoch [2/20], Step [194/316], Loss: 0.0730\n",
      "Epoch [2/20], Step [195/316], Loss: 0.0160\n",
      "Epoch [2/20], Step [196/316], Loss: 0.0176\n",
      "Epoch [2/20], Step [197/316], Loss: 0.0062\n",
      "Epoch [2/20], Step [198/316], Loss: 0.0035\n",
      "Epoch [2/20], Step [199/316], Loss: 0.0044\n",
      "Epoch [2/20], Step [200/316], Loss: 0.0064\n",
      "Epoch [2/20], Step [201/316], Loss: 0.0284\n",
      "Epoch [2/20], Step [202/316], Loss: 0.0063\n",
      "Epoch [2/20], Step [203/316], Loss: 0.0124\n",
      "Epoch [2/20], Step [204/316], Loss: 0.1344\n",
      "Epoch [2/20], Step [205/316], Loss: 0.0076\n",
      "Epoch [2/20], Step [206/316], Loss: 0.0083\n",
      "Epoch [2/20], Step [207/316], Loss: 0.0034\n",
      "Epoch [2/20], Step [208/316], Loss: 0.0054\n",
      "Epoch [2/20], Step [209/316], Loss: 0.0250\n",
      "Epoch [2/20], Step [210/316], Loss: 0.0027\n",
      "Epoch [2/20], Step [211/316], Loss: 0.0478\n",
      "Epoch [2/20], Step [212/316], Loss: 0.0023\n",
      "Epoch [2/20], Step [213/316], Loss: 0.0516\n",
      "Epoch [2/20], Step [214/316], Loss: 0.0612\n",
      "Epoch [2/20], Step [215/316], Loss: 0.0052\n",
      "Epoch [2/20], Step [216/316], Loss: 0.0691\n",
      "Epoch [2/20], Step [217/316], Loss: 0.0091\n",
      "Epoch [2/20], Step [218/316], Loss: 0.0009\n",
      "Epoch [2/20], Step [219/316], Loss: 0.2376\n",
      "Epoch [2/20], Step [220/316], Loss: 0.0173\n",
      "Epoch [2/20], Step [221/316], Loss: 0.0152\n",
      "Epoch [2/20], Step [222/316], Loss: 0.0903\n",
      "Epoch [2/20], Step [223/316], Loss: 0.0019\n",
      "Epoch [2/20], Step [224/316], Loss: 0.0032\n",
      "Epoch [2/20], Step [225/316], Loss: 0.0197\n",
      "Epoch [2/20], Step [226/316], Loss: 0.0035\n",
      "Epoch [2/20], Step [227/316], Loss: 0.1491\n",
      "Epoch [2/20], Step [228/316], Loss: 0.1069\n",
      "Epoch [2/20], Step [229/316], Loss: 0.1059\n",
      "Epoch [2/20], Step [230/316], Loss: 0.0157\n",
      "Epoch [2/20], Step [231/316], Loss: 0.0059\n",
      "Epoch [2/20], Step [232/316], Loss: 0.0407\n",
      "Epoch [2/20], Step [233/316], Loss: 0.0285\n",
      "Epoch [2/20], Step [234/316], Loss: 0.0020\n",
      "Epoch [2/20], Step [235/316], Loss: 0.0088\n",
      "Epoch [2/20], Step [236/316], Loss: 0.0157\n",
      "Epoch [2/20], Step [237/316], Loss: 0.0172\n",
      "Epoch [2/20], Step [238/316], Loss: 0.0157\n",
      "Epoch [2/20], Step [239/316], Loss: 0.0256\n",
      "Epoch [2/20], Step [240/316], Loss: 0.0016\n",
      "Epoch [2/20], Step [241/316], Loss: 0.0116\n",
      "Epoch [2/20], Step [242/316], Loss: 0.0156\n",
      "Epoch [2/20], Step [243/316], Loss: 0.0061\n",
      "Epoch [2/20], Step [244/316], Loss: 0.0150\n",
      "Epoch [2/20], Step [245/316], Loss: 0.1047\n",
      "Epoch [2/20], Step [246/316], Loss: 0.0050\n",
      "Epoch [2/20], Step [247/316], Loss: 0.0163\n",
      "Epoch [2/20], Step [248/316], Loss: 0.0095\n",
      "Epoch [2/20], Step [249/316], Loss: 0.0790\n",
      "Epoch [2/20], Step [250/316], Loss: 0.0036\n",
      "Epoch [2/20], Step [251/316], Loss: 0.0558\n",
      "Epoch [2/20], Step [252/316], Loss: 0.0055\n",
      "Epoch [2/20], Step [253/316], Loss: 0.0992\n",
      "Epoch [2/20], Step [254/316], Loss: 0.0051\n",
      "Epoch [2/20], Step [255/316], Loss: 0.0847\n",
      "Epoch [2/20], Step [256/316], Loss: 0.1013\n",
      "Epoch [2/20], Step [257/316], Loss: 0.0047\n",
      "Epoch [2/20], Step [258/316], Loss: 0.0260\n",
      "Epoch [2/20], Step [259/316], Loss: 0.0040\n",
      "Epoch [2/20], Step [260/316], Loss: 0.0360\n",
      "Epoch [2/20], Step [261/316], Loss: 0.0022\n",
      "Epoch [2/20], Step [262/316], Loss: 0.0039\n",
      "Epoch [2/20], Step [263/316], Loss: 0.0974\n",
      "Epoch [2/20], Step [264/316], Loss: 0.0226\n",
      "Epoch [2/20], Step [265/316], Loss: 0.0152\n",
      "Epoch [2/20], Step [266/316], Loss: 0.0238\n",
      "Epoch [2/20], Step [267/316], Loss: 0.0069\n",
      "Epoch [2/20], Step [268/316], Loss: 0.1021\n",
      "Epoch [2/20], Step [269/316], Loss: 0.0035\n",
      "Epoch [2/20], Step [270/316], Loss: 0.0040\n",
      "Epoch [2/20], Step [271/316], Loss: 0.0108\n",
      "Epoch [2/20], Step [272/316], Loss: 0.0023\n",
      "Epoch [2/20], Step [273/316], Loss: 0.0035\n",
      "Epoch [2/20], Step [274/316], Loss: 0.0134\n",
      "Epoch [2/20], Step [275/316], Loss: 0.0127\n",
      "Epoch [2/20], Step [276/316], Loss: 0.0076\n",
      "Epoch [2/20], Step [277/316], Loss: 0.0094\n",
      "Epoch [2/20], Step [278/316], Loss: 0.0028\n",
      "Epoch [2/20], Step [279/316], Loss: 0.0153\n",
      "Epoch [2/20], Step [280/316], Loss: 0.0029\n",
      "Epoch [2/20], Step [281/316], Loss: 0.1539\n",
      "Epoch [2/20], Step [282/316], Loss: 0.0016\n",
      "Epoch [2/20], Step [283/316], Loss: 0.0045\n",
      "Epoch [2/20], Step [284/316], Loss: 0.0168\n",
      "Epoch [2/20], Step [285/316], Loss: 0.0018\n",
      "Epoch [2/20], Step [286/316], Loss: 0.0054\n",
      "Epoch [2/20], Step [287/316], Loss: 0.0305\n",
      "Epoch [2/20], Step [288/316], Loss: 0.0059\n",
      "Epoch [2/20], Step [289/316], Loss: 0.0040\n",
      "Epoch [2/20], Step [290/316], Loss: 0.0287\n",
      "Epoch [2/20], Step [291/316], Loss: 0.0151\n",
      "Epoch [2/20], Step [292/316], Loss: 0.1393\n",
      "Epoch [2/20], Step [293/316], Loss: 0.0152\n",
      "Epoch [2/20], Step [294/316], Loss: 0.1028\n",
      "Epoch [2/20], Step [295/316], Loss: 0.0058\n",
      "Epoch [2/20], Step [296/316], Loss: 0.0065\n",
      "Epoch [2/20], Step [297/316], Loss: 0.0561\n",
      "Epoch [2/20], Step [298/316], Loss: 0.0218\n",
      "Epoch [2/20], Step [299/316], Loss: 0.0021\n",
      "Epoch [2/20], Step [300/316], Loss: 0.0044\n",
      "Epoch [2/20], Step [301/316], Loss: 0.0046\n",
      "Epoch [2/20], Step [302/316], Loss: 0.0012\n",
      "Epoch [2/20], Step [303/316], Loss: 0.0024\n",
      "Epoch [2/20], Step [304/316], Loss: 0.0050\n",
      "Epoch [2/20], Step [305/316], Loss: 0.0801\n",
      "Epoch [2/20], Step [306/316], Loss: 0.1925\n",
      "Epoch [2/20], Step [307/316], Loss: 0.0523\n",
      "Epoch [2/20], Step [308/316], Loss: 0.0041\n",
      "Epoch [2/20], Step [309/316], Loss: 0.0025\n",
      "Epoch [2/20], Step [310/316], Loss: 0.0034\n",
      "Epoch [2/20], Step [311/316], Loss: 0.0026\n",
      "Epoch [2/20], Step [312/316], Loss: 0.0038\n",
      "Epoch [2/20], Step [313/316], Loss: 0.0635\n",
      "Epoch [2/20], Step [314/316], Loss: 0.0033\n",
      "Epoch [2/20], Step [315/316], Loss: 0.0084\n",
      "Epoch [2/20], Step [316/316], Loss: 0.0065\n",
      "Epoch [2/20], Train Loss: 0.0366\n",
      "Epoch [2/20], Validation Loss: 0.1570\n",
      "Epoch [2/20], Validation Accuracy: 0.8104\n",
      "Epoch [3/20], Step [1/316], Loss: 0.0048\n",
      "Epoch [3/20], Step [2/316], Loss: 0.0051\n",
      "Epoch [3/20], Step [3/316], Loss: 0.0307\n",
      "Epoch [3/20], Step [4/316], Loss: 0.1065\n",
      "Epoch [3/20], Step [5/316], Loss: 0.0045\n",
      "Epoch [3/20], Step [6/316], Loss: 0.0914\n",
      "Epoch [3/20], Step [7/316], Loss: 0.0040\n",
      "Epoch [3/20], Step [8/316], Loss: 0.0013\n",
      "Epoch [3/20], Step [9/316], Loss: 0.0025\n",
      "Epoch [3/20], Step [10/316], Loss: 0.0005\n",
      "Epoch [3/20], Step [11/316], Loss: 0.0056\n",
      "Epoch [3/20], Step [12/316], Loss: 0.0035\n",
      "Epoch [3/20], Step [13/316], Loss: 0.0044\n",
      "Epoch [3/20], Step [14/316], Loss: 0.0051\n",
      "Epoch [3/20], Step [15/316], Loss: 0.0073\n",
      "Epoch [3/20], Step [16/316], Loss: 0.0018\n",
      "Epoch [3/20], Step [17/316], Loss: 0.0041\n",
      "Epoch [3/20], Step [18/316], Loss: 0.0283\n",
      "Epoch [3/20], Step [19/316], Loss: 0.0054\n",
      "Epoch [3/20], Step [20/316], Loss: 0.0026\n",
      "Epoch [3/20], Step [21/316], Loss: 0.0221\n",
      "Epoch [3/20], Step [22/316], Loss: 0.0022\n",
      "Epoch [3/20], Step [23/316], Loss: 0.0048\n",
      "Epoch [3/20], Step [24/316], Loss: 0.1209\n",
      "Epoch [3/20], Step [25/316], Loss: 0.0274\n",
      "Epoch [3/20], Step [26/316], Loss: 0.0065\n",
      "Epoch [3/20], Step [27/316], Loss: 0.0028\n",
      "Epoch [3/20], Step [28/316], Loss: 0.0016\n",
      "Epoch [3/20], Step [29/316], Loss: 0.0019\n",
      "Epoch [3/20], Step [30/316], Loss: 0.0030\n",
      "Epoch [3/20], Step [31/316], Loss: 0.0038\n",
      "Epoch [3/20], Step [32/316], Loss: 0.0133\n",
      "Epoch [3/20], Step [33/316], Loss: 0.0062\n",
      "Epoch [3/20], Step [34/316], Loss: 0.0106\n",
      "Epoch [3/20], Step [35/316], Loss: 0.0051\n",
      "Epoch [3/20], Step [36/316], Loss: 0.0022\n",
      "Epoch [3/20], Step [37/316], Loss: 0.0035\n",
      "Epoch [3/20], Step [38/316], Loss: 0.0054\n",
      "Epoch [3/20], Step [39/316], Loss: 0.0763\n",
      "Epoch [3/20], Step [40/316], Loss: 0.0105\n",
      "Epoch [3/20], Step [41/316], Loss: 0.0040\n",
      "Epoch [3/20], Step [42/316], Loss: 0.0084\n",
      "Epoch [3/20], Step [43/316], Loss: 0.0049\n",
      "Epoch [3/20], Step [44/316], Loss: 0.0046\n",
      "Epoch [3/20], Step [45/316], Loss: 0.0014\n",
      "Epoch [3/20], Step [46/316], Loss: 0.0020\n",
      "Epoch [3/20], Step [47/316], Loss: 0.0025\n",
      "Epoch [3/20], Step [48/316], Loss: 0.0141\n",
      "Epoch [3/20], Step [49/316], Loss: 0.0031\n",
      "Epoch [3/20], Step [50/316], Loss: 0.0096\n",
      "Epoch [3/20], Step [51/316], Loss: 0.0874\n",
      "Epoch [3/20], Step [52/316], Loss: 0.0100\n",
      "Epoch [3/20], Step [53/316], Loss: 0.0010\n",
      "Epoch [3/20], Step [54/316], Loss: 0.0166\n",
      "Epoch [3/20], Step [55/316], Loss: 0.0038\n",
      "Epoch [3/20], Step [56/316], Loss: 0.0019\n",
      "Epoch [3/20], Step [57/316], Loss: 0.0028\n",
      "Epoch [3/20], Step [58/316], Loss: 0.0006\n",
      "Epoch [3/20], Step [59/316], Loss: 0.0042\n",
      "Epoch [3/20], Step [60/316], Loss: 0.0046\n",
      "Epoch [3/20], Step [61/316], Loss: 0.2032\n",
      "Epoch [3/20], Step [62/316], Loss: 0.0015\n",
      "Epoch [3/20], Step [63/316], Loss: 0.0317\n",
      "Epoch [3/20], Step [64/316], Loss: 0.0034\n",
      "Epoch [3/20], Step [65/316], Loss: 0.1323\n",
      "Epoch [3/20], Step [66/316], Loss: 0.0025\n",
      "Epoch [3/20], Step [67/316], Loss: 0.0042\n",
      "Epoch [3/20], Step [68/316], Loss: 0.0013\n",
      "Epoch [3/20], Step [69/316], Loss: 0.0061\n",
      "Epoch [3/20], Step [70/316], Loss: 0.0040\n",
      "Epoch [3/20], Step [71/316], Loss: 0.0964\n",
      "Epoch [3/20], Step [72/316], Loss: 0.0035\n",
      "Epoch [3/20], Step [73/316], Loss: 0.0025\n",
      "Epoch [3/20], Step [74/316], Loss: 0.0024\n",
      "Epoch [3/20], Step [75/316], Loss: 0.1633\n",
      "Epoch [3/20], Step [76/316], Loss: 0.0008\n",
      "Epoch [3/20], Step [77/316], Loss: 0.0035\n",
      "Epoch [3/20], Step [78/316], Loss: 0.0129\n",
      "Epoch [3/20], Step [79/316], Loss: 0.1090\n",
      "Epoch [3/20], Step [80/316], Loss: 0.0125\n",
      "Epoch [3/20], Step [81/316], Loss: 0.0496\n",
      "Epoch [3/20], Step [82/316], Loss: 0.0353\n",
      "Epoch [3/20], Step [83/316], Loss: 0.0031\n",
      "Epoch [3/20], Step [84/316], Loss: 0.0027\n",
      "Epoch [3/20], Step [85/316], Loss: 0.0044\n",
      "Epoch [3/20], Step [86/316], Loss: 0.0012\n",
      "Epoch [3/20], Step [87/316], Loss: 0.0026\n",
      "Epoch [3/20], Step [88/316], Loss: 0.0217\n",
      "Epoch [3/20], Step [89/316], Loss: 0.0238\n",
      "Epoch [3/20], Step [90/316], Loss: 0.0039\n",
      "Epoch [3/20], Step [91/316], Loss: 0.0684\n",
      "Epoch [3/20], Step [92/316], Loss: 0.0035\n",
      "Epoch [3/20], Step [93/316], Loss: 0.0035\n",
      "Epoch [3/20], Step [94/316], Loss: 0.0040\n",
      "Epoch [3/20], Step [95/316], Loss: 0.0043\n",
      "Epoch [3/20], Step [96/316], Loss: 0.1228\n",
      "Epoch [3/20], Step [97/316], Loss: 0.0024\n",
      "Epoch [3/20], Step [98/316], Loss: 0.1040\n",
      "Epoch [3/20], Step [99/316], Loss: 0.0062\n",
      "Epoch [3/20], Step [100/316], Loss: 0.0029\n",
      "Epoch [3/20], Step [101/316], Loss: 0.0083\n",
      "Epoch [3/20], Step [102/316], Loss: 0.0187\n",
      "Epoch [3/20], Step [103/316], Loss: 0.0117\n",
      "Epoch [3/20], Step [104/316], Loss: 0.0771\n",
      "Epoch [3/20], Step [105/316], Loss: 0.0164\n",
      "Epoch [3/20], Step [106/316], Loss: 0.1175\n",
      "Epoch [3/20], Step [107/316], Loss: 0.0874\n",
      "Epoch [3/20], Step [108/316], Loss: 0.0093\n",
      "Epoch [3/20], Step [109/316], Loss: 0.0964\n",
      "Epoch [3/20], Step [110/316], Loss: 0.0030\n",
      "Epoch [3/20], Step [111/316], Loss: 0.0083\n",
      "Epoch [3/20], Step [112/316], Loss: 0.0043\n",
      "Epoch [3/20], Step [113/316], Loss: 0.0075\n",
      "Epoch [3/20], Step [114/316], Loss: 0.0064\n",
      "Epoch [3/20], Step [115/316], Loss: 0.0026\n",
      "Epoch [3/20], Step [116/316], Loss: 0.0282\n",
      "Epoch [3/20], Step [117/316], Loss: 0.0092\n",
      "Epoch [3/20], Step [118/316], Loss: 0.0004\n",
      "Epoch [3/20], Step [119/316], Loss: 0.0360\n",
      "Epoch [3/20], Step [120/316], Loss: 0.0023\n",
      "Epoch [3/20], Step [121/316], Loss: 0.0266\n",
      "Epoch [3/20], Step [122/316], Loss: 0.0057\n",
      "Epoch [3/20], Step [123/316], Loss: 0.0130\n",
      "Epoch [3/20], Step [124/316], Loss: 0.0014\n",
      "Epoch [3/20], Step [125/316], Loss: 0.0864\n",
      "Epoch [3/20], Step [126/316], Loss: 0.0074\n",
      "Epoch [3/20], Step [127/316], Loss: 0.0213\n",
      "Epoch [3/20], Step [128/316], Loss: 0.0094\n",
      "Epoch [3/20], Step [129/316], Loss: 0.0898\n",
      "Epoch [3/20], Step [130/316], Loss: 0.0017\n",
      "Epoch [3/20], Step [131/316], Loss: 0.0012\n",
      "Epoch [3/20], Step [132/316], Loss: 0.1118\n",
      "Epoch [3/20], Step [133/316], Loss: 0.0067\n",
      "Epoch [3/20], Step [134/316], Loss: 0.0198\n",
      "Epoch [3/20], Step [135/316], Loss: 0.0042\n",
      "Epoch [3/20], Step [136/316], Loss: 0.0964\n",
      "Epoch [3/20], Step [137/316], Loss: 0.0064\n",
      "Epoch [3/20], Step [138/316], Loss: 0.0030\n",
      "Epoch [3/20], Step [139/316], Loss: 0.0042\n",
      "Epoch [3/20], Step [140/316], Loss: 0.0045\n",
      "Epoch [3/20], Step [141/316], Loss: 0.0015\n",
      "Epoch [3/20], Step [142/316], Loss: 0.0071\n",
      "Epoch [3/20], Step [143/316], Loss: 0.0055\n",
      "Epoch [3/20], Step [144/316], Loss: 0.0048\n",
      "Epoch [3/20], Step [145/316], Loss: 0.1091\n",
      "Epoch [3/20], Step [146/316], Loss: 0.0010\n",
      "Epoch [3/20], Step [147/316], Loss: 0.0028\n",
      "Epoch [3/20], Step [148/316], Loss: 0.0747\n",
      "Epoch [3/20], Step [149/316], Loss: 0.0203\n",
      "Epoch [3/20], Step [150/316], Loss: 0.0019\n",
      "Epoch [3/20], Step [151/316], Loss: 0.0001\n",
      "Epoch [3/20], Step [152/316], Loss: 0.0083\n",
      "Epoch [3/20], Step [153/316], Loss: 0.0055\n",
      "Epoch [3/20], Step [154/316], Loss: 0.0025\n",
      "Epoch [3/20], Step [155/316], Loss: 0.0045\n",
      "Epoch [3/20], Step [156/316], Loss: 0.0363\n",
      "Epoch [3/20], Step [157/316], Loss: 0.0070\n",
      "Epoch [3/20], Step [158/316], Loss: 0.0034\n",
      "Epoch [3/20], Step [159/316], Loss: 0.0102\n",
      "Epoch [3/20], Step [160/316], Loss: 0.0014\n",
      "Epoch [3/20], Step [161/316], Loss: 0.0062\n",
      "Epoch [3/20], Step [162/316], Loss: 0.0110\n",
      "Epoch [3/20], Step [163/316], Loss: 0.0666\n",
      "Epoch [3/20], Step [164/316], Loss: 0.0220\n",
      "Epoch [3/20], Step [165/316], Loss: 0.0197\n",
      "Epoch [3/20], Step [166/316], Loss: 0.0029\n",
      "Epoch [3/20], Step [167/316], Loss: 0.0328\n",
      "Epoch [3/20], Step [168/316], Loss: 0.0082\n",
      "Epoch [3/20], Step [169/316], Loss: 0.0042\n",
      "Epoch [3/20], Step [170/316], Loss: 0.0018\n",
      "Epoch [3/20], Step [171/316], Loss: 0.0017\n",
      "Epoch [3/20], Step [172/316], Loss: 0.0044\n",
      "Epoch [3/20], Step [173/316], Loss: 0.2112\n",
      "Epoch [3/20], Step [174/316], Loss: 0.0035\n",
      "Epoch [3/20], Step [175/316], Loss: 0.0576\n",
      "Epoch [3/20], Step [176/316], Loss: 0.0030\n",
      "Epoch [3/20], Step [177/316], Loss: 0.0771\n",
      "Epoch [3/20], Step [178/316], Loss: 0.0924\n",
      "Epoch [3/20], Step [179/316], Loss: 0.0036\n",
      "Epoch [3/20], Step [180/316], Loss: 0.0012\n",
      "Epoch [3/20], Step [181/316], Loss: 0.0047\n",
      "Epoch [3/20], Step [182/316], Loss: 0.1143\n",
      "Epoch [3/20], Step [183/316], Loss: 0.0038\n",
      "Epoch [3/20], Step [184/316], Loss: 0.0032\n",
      "Epoch [3/20], Step [185/316], Loss: 0.0044\n",
      "Epoch [3/20], Step [186/316], Loss: 0.0047\n",
      "Epoch [3/20], Step [187/316], Loss: 0.0051\n",
      "Epoch [3/20], Step [188/316], Loss: 0.0026\n",
      "Epoch [3/20], Step [189/316], Loss: 0.0081\n",
      "Epoch [3/20], Step [190/316], Loss: 0.0547\n",
      "Epoch [3/20], Step [191/316], Loss: 0.0044\n",
      "Epoch [3/20], Step [192/316], Loss: 0.0027\n",
      "Epoch [3/20], Step [193/316], Loss: 0.0227\n",
      "Epoch [3/20], Step [194/316], Loss: 0.0021\n",
      "Epoch [3/20], Step [195/316], Loss: 0.1030\n",
      "Epoch [3/20], Step [196/316], Loss: 0.0038\n",
      "Epoch [3/20], Step [197/316], Loss: 0.0032\n",
      "Epoch [3/20], Step [198/316], Loss: 0.0016\n",
      "Epoch [3/20], Step [199/316], Loss: 0.0032\n",
      "Epoch [3/20], Step [200/316], Loss: 0.0038\n",
      "Epoch [3/20], Step [201/316], Loss: 0.0155\n",
      "Epoch [3/20], Step [202/316], Loss: 0.0843\n",
      "Epoch [3/20], Step [203/316], Loss: 0.0510\n",
      "Epoch [3/20], Step [204/316], Loss: 0.0029\n",
      "Epoch [3/20], Step [205/316], Loss: 0.0025\n",
      "Epoch [3/20], Step [206/316], Loss: 0.0060\n",
      "Epoch [3/20], Step [207/316], Loss: 0.0088\n",
      "Epoch [3/20], Step [208/316], Loss: 0.0109\n",
      "Epoch [3/20], Step [209/316], Loss: 0.2489\n",
      "Epoch [3/20], Step [210/316], Loss: 0.0005\n",
      "Epoch [3/20], Step [211/316], Loss: 0.0168\n",
      "Epoch [3/20], Step [212/316], Loss: 0.0450\n",
      "Epoch [3/20], Step [213/316], Loss: 0.0152\n",
      "Epoch [3/20], Step [214/316], Loss: 0.0008\n",
      "Epoch [3/20], Step [215/316], Loss: 0.0017\n",
      "Epoch [3/20], Step [216/316], Loss: 0.0838\n",
      "Epoch [3/20], Step [217/316], Loss: 0.0053\n",
      "Epoch [3/20], Step [218/316], Loss: 0.0366\n",
      "Epoch [3/20], Step [219/316], Loss: 0.0022\n",
      "Epoch [3/20], Step [220/316], Loss: 0.0037\n",
      "Epoch [3/20], Step [221/316], Loss: 0.0075\n",
      "Epoch [3/20], Step [222/316], Loss: 0.0727\n",
      "Epoch [3/20], Step [223/316], Loss: 0.2303\n",
      "Epoch [3/20], Step [224/316], Loss: 0.0048\n",
      "Epoch [3/20], Step [225/316], Loss: 0.2455\n",
      "Epoch [3/20], Step [226/316], Loss: 0.0069\n",
      "Epoch [3/20], Step [227/316], Loss: 0.0029\n",
      "Epoch [3/20], Step [228/316], Loss: 0.0114\n",
      "Epoch [3/20], Step [229/316], Loss: 0.0691\n",
      "Epoch [3/20], Step [230/316], Loss: 0.2774\n",
      "Epoch [3/20], Step [231/316], Loss: 0.0061\n",
      "Epoch [3/20], Step [232/316], Loss: 0.0042\n",
      "Epoch [3/20], Step [233/316], Loss: 0.1090\n",
      "Epoch [3/20], Step [234/316], Loss: 0.0046\n",
      "Epoch [3/20], Step [235/316], Loss: 0.1917\n",
      "Epoch [3/20], Step [236/316], Loss: 0.0066\n",
      "Epoch [3/20], Step [237/316], Loss: 0.0049\n",
      "Epoch [3/20], Step [238/316], Loss: 0.0146\n",
      "Epoch [3/20], Step [239/316], Loss: 0.0061\n",
      "Epoch [3/20], Step [240/316], Loss: 0.0149\n",
      "Epoch [3/20], Step [241/316], Loss: 0.0049\n",
      "Epoch [3/20], Step [242/316], Loss: 0.0157\n",
      "Epoch [3/20], Step [243/316], Loss: 0.0030\n",
      "Epoch [3/20], Step [244/316], Loss: 0.0033\n",
      "Epoch [3/20], Step [245/316], Loss: 0.0822\n",
      "Epoch [3/20], Step [246/316], Loss: 0.0058\n",
      "Epoch [3/20], Step [247/316], Loss: 0.0068\n",
      "Epoch [3/20], Step [248/316], Loss: 0.0062\n",
      "Epoch [3/20], Step [249/316], Loss: 0.0390\n",
      "Epoch [3/20], Step [250/316], Loss: 0.0066\n",
      "Epoch [3/20], Step [251/316], Loss: 0.0041\n",
      "Epoch [3/20], Step [252/316], Loss: 0.0091\n",
      "Epoch [3/20], Step [253/316], Loss: 0.0024\n",
      "Epoch [3/20], Step [254/316], Loss: 0.0056\n",
      "Epoch [3/20], Step [255/316], Loss: 0.2050\n",
      "Epoch [3/20], Step [256/316], Loss: 0.0773\n",
      "Epoch [3/20], Step [257/316], Loss: 0.0625\n",
      "Epoch [3/20], Step [258/316], Loss: 0.0029\n",
      "Epoch [3/20], Step [259/316], Loss: 0.0022\n",
      "Epoch [3/20], Step [260/316], Loss: 0.1193\n",
      "Epoch [3/20], Step [261/316], Loss: 0.0461\n",
      "Epoch [3/20], Step [262/316], Loss: 0.0011\n",
      "Epoch [3/20], Step [263/316], Loss: 0.0069\n",
      "Epoch [3/20], Step [264/316], Loss: 0.0042\n",
      "Epoch [3/20], Step [265/316], Loss: 0.0018\n",
      "Epoch [3/20], Step [266/316], Loss: 0.1948\n",
      "Epoch [3/20], Step [267/316], Loss: 0.1853\n",
      "Epoch [3/20], Step [268/316], Loss: 0.1320\n",
      "Epoch [3/20], Step [269/316], Loss: 0.0048\n",
      "Epoch [3/20], Step [270/316], Loss: 0.1167\n",
      "Epoch [3/20], Step [271/316], Loss: 0.0039\n",
      "Epoch [3/20], Step [272/316], Loss: 0.0058\n",
      "Epoch [3/20], Step [273/316], Loss: 0.0263\n",
      "Epoch [3/20], Step [274/316], Loss: 0.0155\n",
      "Epoch [3/20], Step [275/316], Loss: 0.0137\n",
      "Epoch [3/20], Step [276/316], Loss: 0.0146\n",
      "Epoch [3/20], Step [277/316], Loss: 0.0231\n",
      "Epoch [3/20], Step [278/316], Loss: 0.0068\n",
      "Epoch [3/20], Step [279/316], Loss: 0.0016\n",
      "Epoch [3/20], Step [280/316], Loss: 0.0173\n",
      "Epoch [3/20], Step [281/316], Loss: 0.0024\n",
      "Epoch [3/20], Step [282/316], Loss: 0.0071\n",
      "Epoch [3/20], Step [283/316], Loss: 0.0595\n",
      "Epoch [3/20], Step [284/316], Loss: 0.0047\n",
      "Epoch [3/20], Step [285/316], Loss: 0.0145\n",
      "Epoch [3/20], Step [286/316], Loss: 0.0071\n",
      "Epoch [3/20], Step [287/316], Loss: 0.0037\n",
      "Epoch [3/20], Step [288/316], Loss: 0.0833\n",
      "Epoch [3/20], Step [289/316], Loss: 0.0946\n",
      "Epoch [3/20], Step [290/316], Loss: 0.0147\n",
      "Epoch [3/20], Step [291/316], Loss: 0.0672\n",
      "Epoch [3/20], Step [292/316], Loss: 0.0013\n",
      "Epoch [3/20], Step [293/316], Loss: 0.0210\n",
      "Epoch [3/20], Step [294/316], Loss: 0.0215\n",
      "Epoch [3/20], Step [295/316], Loss: 0.0092\n",
      "Epoch [3/20], Step [296/316], Loss: 0.0098\n",
      "Epoch [3/20], Step [297/316], Loss: 0.0746\n",
      "Epoch [3/20], Step [298/316], Loss: 0.0079\n",
      "Epoch [3/20], Step [299/316], Loss: 0.0014\n",
      "Epoch [3/20], Step [300/316], Loss: 0.0058\n",
      "Epoch [3/20], Step [301/316], Loss: 0.0041\n",
      "Epoch [3/20], Step [302/316], Loss: 0.0030\n",
      "Epoch [3/20], Step [303/316], Loss: 0.0138\n",
      "Epoch [3/20], Step [304/316], Loss: 0.0070\n",
      "Epoch [3/20], Step [305/316], Loss: 0.0076\n",
      "Epoch [3/20], Step [306/316], Loss: 0.0520\n",
      "Epoch [3/20], Step [307/316], Loss: 0.0744\n",
      "Epoch [3/20], Step [308/316], Loss: 0.0129\n",
      "Epoch [3/20], Step [309/316], Loss: 0.0004\n",
      "Epoch [3/20], Step [310/316], Loss: 0.0054\n",
      "Epoch [3/20], Step [311/316], Loss: 0.1976\n",
      "Epoch [3/20], Step [312/316], Loss: 0.0082\n",
      "Epoch [3/20], Step [313/316], Loss: 0.0172\n",
      "Epoch [3/20], Step [314/316], Loss: 0.0038\n",
      "Epoch [3/20], Step [315/316], Loss: 0.0080\n",
      "Epoch [3/20], Step [316/316], Loss: 0.0026\n",
      "Epoch [3/20], Train Loss: 0.0279\n",
      "Epoch [3/20], Validation Loss: 0.1559\n",
      "Epoch [3/20], Validation Accuracy: 0.8068\n",
      "Epoch [4/20], Step [1/316], Loss: 0.0016\n",
      "Epoch [4/20], Step [2/316], Loss: 0.0122\n",
      "Epoch [4/20], Step [3/316], Loss: 0.0054\n",
      "Epoch [4/20], Step [4/316], Loss: 0.0050\n",
      "Epoch [4/20], Step [5/316], Loss: 0.1511\n",
      "Epoch [4/20], Step [6/316], Loss: 0.0012\n",
      "Epoch [4/20], Step [7/316], Loss: 0.0033\n",
      "Epoch [4/20], Step [8/316], Loss: 0.0248\n",
      "Epoch [4/20], Step [9/316], Loss: 0.0053\n",
      "Epoch [4/20], Step [10/316], Loss: 0.0053\n",
      "Epoch [4/20], Step [11/316], Loss: 0.0061\n",
      "Epoch [4/20], Step [12/316], Loss: 0.0053\n",
      "Epoch [4/20], Step [13/316], Loss: 0.0086\n",
      "Epoch [4/20], Step [14/316], Loss: 0.0852\n",
      "Epoch [4/20], Step [15/316], Loss: 0.0137\n",
      "Epoch [4/20], Step [16/316], Loss: 0.0370\n",
      "Epoch [4/20], Step [17/316], Loss: 0.0821\n",
      "Epoch [4/20], Step [18/316], Loss: 0.0371\n",
      "Epoch [4/20], Step [19/316], Loss: 0.0054\n",
      "Epoch [4/20], Step [20/316], Loss: 0.0038\n",
      "Epoch [4/20], Step [21/316], Loss: 0.0975\n",
      "Epoch [4/20], Step [22/316], Loss: 0.0290\n",
      "Epoch [4/20], Step [23/316], Loss: 0.0027\n",
      "Epoch [4/20], Step [24/316], Loss: 0.0074\n",
      "Epoch [4/20], Step [25/316], Loss: 0.1239\n",
      "Epoch [4/20], Step [26/316], Loss: 0.0319\n",
      "Epoch [4/20], Step [27/316], Loss: 0.0028\n",
      "Epoch [4/20], Step [28/316], Loss: 0.0096\n",
      "Epoch [4/20], Step [29/316], Loss: 0.0087\n",
      "Epoch [4/20], Step [30/316], Loss: 0.0235\n",
      "Epoch [4/20], Step [31/316], Loss: 0.0062\n",
      "Epoch [4/20], Step [32/316], Loss: 0.0398\n",
      "Epoch [4/20], Step [33/316], Loss: 0.0054\n",
      "Epoch [4/20], Step [34/316], Loss: 0.0064\n",
      "Epoch [4/20], Step [35/316], Loss: 0.1177\n",
      "Epoch [4/20], Step [36/316], Loss: 0.0131\n",
      "Epoch [4/20], Step [37/316], Loss: 0.0803\n",
      "Epoch [4/20], Step [38/316], Loss: 0.0063\n",
      "Epoch [4/20], Step [39/316], Loss: 0.0048\n",
      "Epoch [4/20], Step [40/316], Loss: 0.0108\n",
      "Epoch [4/20], Step [41/316], Loss: 0.0089\n",
      "Epoch [4/20], Step [42/316], Loss: 0.0015\n",
      "Epoch [4/20], Step [43/316], Loss: 0.0104\n",
      "Epoch [4/20], Step [44/316], Loss: 0.0051\n",
      "Epoch [4/20], Step [45/316], Loss: 0.0085\n",
      "Epoch [4/20], Step [46/316], Loss: 0.0047\n",
      "Epoch [4/20], Step [47/316], Loss: 0.1255\n",
      "Epoch [4/20], Step [48/316], Loss: 0.0071\n",
      "Epoch [4/20], Step [49/316], Loss: 0.0072\n",
      "Epoch [4/20], Step [50/316], Loss: 0.0058\n",
      "Epoch [4/20], Step [51/316], Loss: 0.0045\n",
      "Epoch [4/20], Step [52/316], Loss: 0.0128\n",
      "Epoch [4/20], Step [53/316], Loss: 0.0224\n",
      "Epoch [4/20], Step [54/316], Loss: 0.0061\n",
      "Epoch [4/20], Step [55/316], Loss: 0.0040\n",
      "Epoch [4/20], Step [56/316], Loss: 0.0451\n",
      "Epoch [4/20], Step [57/316], Loss: 0.0294\n",
      "Epoch [4/20], Step [58/316], Loss: 0.0073\n",
      "Epoch [4/20], Step [59/316], Loss: 0.0983\n",
      "Epoch [4/20], Step [60/316], Loss: 0.0785\n",
      "Epoch [4/20], Step [61/316], Loss: 0.0086\n",
      "Epoch [4/20], Step [62/316], Loss: 0.0077\n",
      "Epoch [4/20], Step [63/316], Loss: 0.0056\n",
      "Epoch [4/20], Step [64/316], Loss: 0.1748\n",
      "Epoch [4/20], Step [65/316], Loss: 0.0814\n",
      "Epoch [4/20], Step [66/316], Loss: 0.0975\n",
      "Epoch [4/20], Step [67/316], Loss: 0.0077\n",
      "Epoch [4/20], Step [68/316], Loss: 0.0088\n",
      "Epoch [4/20], Step [69/316], Loss: 0.0047\n",
      "Epoch [4/20], Step [70/316], Loss: 0.0068\n",
      "Epoch [4/20], Step [71/316], Loss: 0.1341\n",
      "Epoch [4/20], Step [72/316], Loss: 0.0120\n",
      "Epoch [4/20], Step [73/316], Loss: 0.0189\n",
      "Epoch [4/20], Step [74/316], Loss: 0.1043\n",
      "Epoch [4/20], Step [75/316], Loss: 0.0053\n",
      "Epoch [4/20], Step [76/316], Loss: 0.0094\n",
      "Epoch [4/20], Step [77/316], Loss: 0.0926\n",
      "Epoch [4/20], Step [78/316], Loss: 0.0038\n",
      "Epoch [4/20], Step [79/316], Loss: 0.0024\n",
      "Epoch [4/20], Step [80/316], Loss: 0.0424\n",
      "Epoch [4/20], Step [81/316], Loss: 0.0021\n",
      "Epoch [4/20], Step [82/316], Loss: 0.0064\n",
      "Epoch [4/20], Step [83/316], Loss: 0.0049\n",
      "Epoch [4/20], Step [84/316], Loss: 0.0190\n",
      "Epoch [4/20], Step [85/316], Loss: 0.0375\n",
      "Epoch [4/20], Step [86/316], Loss: 0.0076\n",
      "Epoch [4/20], Step [87/316], Loss: 0.1023\n",
      "Epoch [4/20], Step [88/316], Loss: 0.0154\n",
      "Epoch [4/20], Step [89/316], Loss: 0.0293\n",
      "Epoch [4/20], Step [90/316], Loss: 0.0020\n",
      "Epoch [4/20], Step [91/316], Loss: 0.0006\n",
      "Epoch [4/20], Step [92/316], Loss: 0.0046\n",
      "Epoch [4/20], Step [93/316], Loss: 0.0158\n",
      "Epoch [4/20], Step [94/316], Loss: 0.0046\n",
      "Epoch [4/20], Step [95/316], Loss: 0.0021\n",
      "Epoch [4/20], Step [96/316], Loss: 0.0084\n",
      "Epoch [4/20], Step [97/316], Loss: 0.0395\n",
      "Epoch [4/20], Step [98/316], Loss: 0.0220\n",
      "Epoch [4/20], Step [99/316], Loss: 0.0020\n",
      "Epoch [4/20], Step [100/316], Loss: 0.0088\n",
      "Epoch [4/20], Step [101/316], Loss: 0.0035\n",
      "Epoch [4/20], Step [102/316], Loss: 0.0236\n",
      "Epoch [4/20], Step [103/316], Loss: 0.0109\n",
      "Epoch [4/20], Step [104/316], Loss: 0.0033\n",
      "Epoch [4/20], Step [105/316], Loss: 0.0055\n",
      "Epoch [4/20], Step [106/316], Loss: 0.1824\n",
      "Epoch [4/20], Step [107/316], Loss: 0.0058\n",
      "Epoch [4/20], Step [108/316], Loss: 0.1252\n",
      "Epoch [4/20], Step [109/316], Loss: 0.0025\n",
      "Epoch [4/20], Step [110/316], Loss: 0.0036\n",
      "Epoch [4/20], Step [111/316], Loss: 0.0035\n",
      "Epoch [4/20], Step [112/316], Loss: 0.0081\n",
      "Epoch [4/20], Step [113/316], Loss: 0.0166\n",
      "Epoch [4/20], Step [114/316], Loss: 0.0013\n",
      "Epoch [4/20], Step [115/316], Loss: 0.1319\n",
      "Epoch [4/20], Step [116/316], Loss: 0.0066\n",
      "Epoch [4/20], Step [117/316], Loss: 0.0069\n",
      "Epoch [4/20], Step [118/316], Loss: 0.0121\n",
      "Epoch [4/20], Step [119/316], Loss: 0.0043\n",
      "Epoch [4/20], Step [120/316], Loss: 0.0019\n",
      "Epoch [4/20], Step [121/316], Loss: 0.0063\n",
      "Epoch [4/20], Step [122/316], Loss: 0.0041\n",
      "Epoch [4/20], Step [123/316], Loss: 0.0356\n",
      "Epoch [4/20], Step [124/316], Loss: 0.0042\n",
      "Epoch [4/20], Step [125/316], Loss: 0.0225\n",
      "Epoch [4/20], Step [126/316], Loss: 0.0064\n",
      "Epoch [4/20], Step [127/316], Loss: 0.0023\n",
      "Epoch [4/20], Step [128/316], Loss: 0.0021\n",
      "Epoch [4/20], Step [129/316], Loss: 0.0033\n",
      "Epoch [4/20], Step [130/316], Loss: 0.0309\n",
      "Epoch [4/20], Step [131/316], Loss: 0.0022\n",
      "Epoch [4/20], Step [132/316], Loss: 0.0087\n",
      "Epoch [4/20], Step [133/316], Loss: 0.0214\n",
      "Epoch [4/20], Step [134/316], Loss: 0.1375\n",
      "Epoch [4/20], Step [135/316], Loss: 0.0891\n",
      "Epoch [4/20], Step [136/316], Loss: 0.0319\n",
      "Epoch [4/20], Step [137/316], Loss: 0.0054\n",
      "Epoch [4/20], Step [138/316], Loss: 0.0780\n",
      "Epoch [4/20], Step [139/316], Loss: 0.0096\n",
      "Epoch [4/20], Step [140/316], Loss: 0.0065\n",
      "Epoch [4/20], Step [141/316], Loss: 0.0340\n",
      "Epoch [4/20], Step [142/316], Loss: 0.0014\n",
      "Epoch [4/20], Step [143/316], Loss: 0.0048\n",
      "Epoch [4/20], Step [144/316], Loss: 0.0038\n",
      "Epoch [4/20], Step [145/316], Loss: 0.0094\n",
      "Epoch [4/20], Step [146/316], Loss: 0.0040\n",
      "Epoch [4/20], Step [147/316], Loss: 0.0053\n",
      "Epoch [4/20], Step [148/316], Loss: 0.0050\n",
      "Epoch [4/20], Step [149/316], Loss: 0.0056\n",
      "Epoch [4/20], Step [150/316], Loss: 0.0036\n",
      "Epoch [4/20], Step [151/316], Loss: 0.2476\n",
      "Epoch [4/20], Step [152/316], Loss: 0.0097\n",
      "Epoch [4/20], Step [153/316], Loss: 0.0033\n",
      "Epoch [4/20], Step [154/316], Loss: 0.1157\n",
      "Epoch [4/20], Step [155/316], Loss: 0.0051\n",
      "Epoch [4/20], Step [156/316], Loss: 0.0045\n",
      "Epoch [4/20], Step [157/316], Loss: 0.0155\n",
      "Epoch [4/20], Step [158/316], Loss: 0.4919\n",
      "Epoch [4/20], Step [159/316], Loss: 0.0078\n",
      "Epoch [4/20], Step [160/316], Loss: 0.1442\n",
      "Epoch [4/20], Step [161/316], Loss: 0.0027\n",
      "Epoch [4/20], Step [162/316], Loss: 0.0034\n",
      "Epoch [4/20], Step [163/316], Loss: 0.1094\n",
      "Epoch [4/20], Step [164/316], Loss: 0.0023\n",
      "Epoch [4/20], Step [165/316], Loss: 0.0061\n",
      "Epoch [4/20], Step [166/316], Loss: 0.1172\n",
      "Epoch [4/20], Step [167/316], Loss: 0.0729\n",
      "Epoch [4/20], Step [168/316], Loss: 0.0068\n",
      "Epoch [4/20], Step [169/316], Loss: 0.0051\n",
      "Epoch [4/20], Step [170/316], Loss: 0.0029\n",
      "Epoch [4/20], Step [171/316], Loss: 0.0228\n",
      "Epoch [4/20], Step [172/316], Loss: 0.0172\n",
      "Epoch [4/20], Step [173/316], Loss: 0.0016\n",
      "Epoch [4/20], Step [174/316], Loss: 0.0070\n",
      "Epoch [4/20], Step [175/316], Loss: 0.0080\n",
      "Epoch [4/20], Step [176/316], Loss: 0.0052\n",
      "Epoch [4/20], Step [177/316], Loss: 0.1918\n",
      "Epoch [4/20], Step [178/316], Loss: 0.0099\n",
      "Epoch [4/20], Step [179/316], Loss: 0.0101\n",
      "Epoch [4/20], Step [180/316], Loss: 0.0054\n",
      "Epoch [4/20], Step [181/316], Loss: 0.0038\n",
      "Epoch [4/20], Step [182/316], Loss: 0.0375\n",
      "Epoch [4/20], Step [183/316], Loss: 0.0423\n",
      "Epoch [4/20], Step [184/316], Loss: 0.0013\n",
      "Epoch [4/20], Step [185/316], Loss: 0.0055\n",
      "Epoch [4/20], Step [186/316], Loss: 0.0238\n",
      "Epoch [4/20], Step [187/316], Loss: 0.8912\n",
      "Epoch [4/20], Step [188/316], Loss: 0.0063\n",
      "Epoch [4/20], Step [189/316], Loss: 0.0037\n",
      "Epoch [4/20], Step [190/316], Loss: 0.0052\n",
      "Epoch [4/20], Step [191/316], Loss: 0.0041\n",
      "Epoch [4/20], Step [192/316], Loss: 0.1189\n",
      "Epoch [4/20], Step [193/316], Loss: 0.0832\n",
      "Epoch [4/20], Step [194/316], Loss: 0.0197\n",
      "Epoch [4/20], Step [195/316], Loss: 0.0031\n",
      "Epoch [4/20], Step [196/316], Loss: 0.0314\n",
      "Epoch [4/20], Step [197/316], Loss: 0.0094\n",
      "Epoch [4/20], Step [198/316], Loss: 0.1144\n",
      "Epoch [4/20], Step [199/316], Loss: 0.1247\n",
      "Epoch [4/20], Step [200/316], Loss: 0.0041\n",
      "Epoch [4/20], Step [201/316], Loss: 0.0055\n",
      "Epoch [4/20], Step [202/316], Loss: 0.0146\n",
      "Epoch [4/20], Step [203/316], Loss: 0.0021\n",
      "Epoch [4/20], Step [204/316], Loss: 0.0230\n",
      "Epoch [4/20], Step [205/316], Loss: 0.0067\n",
      "Epoch [4/20], Step [206/316], Loss: 0.0597\n",
      "Epoch [4/20], Step [207/316], Loss: 0.0051\n",
      "Epoch [4/20], Step [208/316], Loss: 0.0201\n",
      "Epoch [4/20], Step [209/316], Loss: 0.0063\n",
      "Epoch [4/20], Step [210/316], Loss: 0.0359\n",
      "Epoch [4/20], Step [211/316], Loss: 0.0121\n",
      "Epoch [4/20], Step [212/316], Loss: 0.0038\n",
      "Epoch [4/20], Step [213/316], Loss: 0.0229\n",
      "Epoch [4/20], Step [214/316], Loss: 0.1202\n",
      "Epoch [4/20], Step [215/316], Loss: 0.1080\n",
      "Epoch [4/20], Step [216/316], Loss: 0.0843\n",
      "Epoch [4/20], Step [217/316], Loss: 0.0027\n",
      "Epoch [4/20], Step [218/316], Loss: 0.0064\n",
      "Epoch [4/20], Step [219/316], Loss: 0.0057\n",
      "Epoch [4/20], Step [220/316], Loss: 0.0212\n",
      "Epoch [4/20], Step [221/316], Loss: 0.1418\n",
      "Epoch [4/20], Step [222/316], Loss: 0.0171\n",
      "Epoch [4/20], Step [223/316], Loss: 0.0068\n",
      "Epoch [4/20], Step [224/316], Loss: 0.0596\n",
      "Epoch [4/20], Step [225/316], Loss: 0.0061\n",
      "Epoch [4/20], Step [226/316], Loss: 0.0062\n",
      "Epoch [4/20], Step [227/316], Loss: 0.0097\n",
      "Epoch [4/20], Step [228/316], Loss: 0.0278\n",
      "Epoch [4/20], Step [229/316], Loss: 0.0263\n",
      "Epoch [4/20], Step [230/316], Loss: 0.0122\n",
      "Epoch [4/20], Step [231/316], Loss: 0.0191\n",
      "Epoch [4/20], Step [232/316], Loss: 0.0679\n",
      "Epoch [4/20], Step [233/316], Loss: 0.0416\n",
      "Epoch [4/20], Step [234/316], Loss: 0.0084\n",
      "Epoch [4/20], Step [235/316], Loss: 0.0068\n",
      "Epoch [4/20], Step [236/316], Loss: 0.0089\n",
      "Epoch [4/20], Step [237/316], Loss: 0.0220\n",
      "Epoch [4/20], Step [238/316], Loss: 0.0042\n",
      "Epoch [4/20], Step [239/316], Loss: 0.0081\n",
      "Epoch [4/20], Step [240/316], Loss: 0.0001\n",
      "Epoch [4/20], Step [241/316], Loss: 0.0016\n",
      "Epoch [4/20], Step [242/316], Loss: 0.0108\n",
      "Epoch [4/20], Step [243/316], Loss: 0.0063\n",
      "Epoch [4/20], Step [244/316], Loss: 0.0069\n",
      "Epoch [4/20], Step [245/316], Loss: 0.0298\n",
      "Epoch [4/20], Step [246/316], Loss: 0.0246\n",
      "Epoch [4/20], Step [247/316], Loss: 0.0024\n",
      "Epoch [4/20], Step [248/316], Loss: 0.0024\n",
      "Epoch [4/20], Step [249/316], Loss: 0.0066\n",
      "Epoch [4/20], Step [250/316], Loss: 0.0063\n",
      "Epoch [4/20], Step [251/316], Loss: 0.0086\n",
      "Epoch [4/20], Step [252/316], Loss: 0.0108\n",
      "Epoch [4/20], Step [253/316], Loss: 0.0815\n",
      "Epoch [4/20], Step [254/316], Loss: 0.0796\n",
      "Epoch [4/20], Step [255/316], Loss: 0.0110\n",
      "Epoch [4/20], Step [256/316], Loss: 0.0247\n",
      "Epoch [4/20], Step [257/316], Loss: 0.0117\n",
      "Epoch [4/20], Step [258/316], Loss: 0.0487\n",
      "Epoch [4/20], Step [259/316], Loss: 0.0144\n",
      "Epoch [4/20], Step [260/316], Loss: 0.0064\n",
      "Epoch [4/20], Step [261/316], Loss: 0.0065\n",
      "Epoch [4/20], Step [262/316], Loss: 0.0057\n",
      "Epoch [4/20], Step [263/316], Loss: 0.0049\n",
      "Epoch [4/20], Step [264/316], Loss: 0.0037\n",
      "Epoch [4/20], Step [265/316], Loss: 0.0012\n",
      "Epoch [4/20], Step [266/316], Loss: 0.0002\n",
      "Epoch [4/20], Step [267/316], Loss: 0.1085\n",
      "Epoch [4/20], Step [268/316], Loss: 0.1211\n",
      "Epoch [4/20], Step [269/316], Loss: 0.0065\n",
      "Epoch [4/20], Step [270/316], Loss: 0.1275\n",
      "Epoch [4/20], Step [271/316], Loss: 0.0035\n",
      "Epoch [4/20], Step [272/316], Loss: 0.0026\n",
      "Epoch [4/20], Step [273/316], Loss: 0.0046\n",
      "Epoch [4/20], Step [274/316], Loss: 0.0073\n",
      "Epoch [4/20], Step [275/316], Loss: 0.0046\n",
      "Epoch [4/20], Step [276/316], Loss: 0.0037\n",
      "Epoch [4/20], Step [277/316], Loss: 0.0032\n",
      "Epoch [4/20], Step [278/316], Loss: 0.0679\n",
      "Epoch [4/20], Step [279/316], Loss: 0.0129\n",
      "Epoch [4/20], Step [280/316], Loss: 0.0734\n",
      "Epoch [4/20], Step [281/316], Loss: 0.0040\n",
      "Epoch [4/20], Step [282/316], Loss: 0.0246\n",
      "Epoch [4/20], Step [283/316], Loss: 0.4142\n",
      "Epoch [4/20], Step [284/316], Loss: 0.0110\n",
      "Epoch [4/20], Step [285/316], Loss: 0.0076\n",
      "Epoch [4/20], Step [286/316], Loss: 0.0018\n",
      "Epoch [4/20], Step [287/316], Loss: 0.0065\n",
      "Epoch [4/20], Step [288/316], Loss: 0.0135\n",
      "Epoch [4/20], Step [289/316], Loss: 0.0219\n",
      "Epoch [4/20], Step [290/316], Loss: 0.0020\n",
      "Epoch [4/20], Step [291/316], Loss: 0.0034\n",
      "Epoch [4/20], Step [292/316], Loss: 0.1077\n",
      "Epoch [4/20], Step [293/316], Loss: 0.0052\n",
      "Epoch [4/20], Step [294/316], Loss: 0.0038\n",
      "Epoch [4/20], Step [295/316], Loss: 0.0031\n",
      "Epoch [4/20], Step [296/316], Loss: 0.0048\n",
      "Epoch [4/20], Step [297/316], Loss: 0.0089\n",
      "Epoch [4/20], Step [298/316], Loss: 0.0034\n",
      "Epoch [4/20], Step [299/316], Loss: 0.0009\n",
      "Epoch [4/20], Step [300/316], Loss: 0.0013\n",
      "Epoch [4/20], Step [301/316], Loss: 0.0106\n",
      "Epoch [4/20], Step [302/316], Loss: 0.0024\n",
      "Epoch [4/20], Step [303/316], Loss: 0.0115\n",
      "Epoch [4/20], Step [304/316], Loss: 0.1117\n",
      "Epoch [4/20], Step [305/316], Loss: 0.0141\n",
      "Epoch [4/20], Step [306/316], Loss: 0.1463\n",
      "Epoch [4/20], Step [307/316], Loss: 0.1698\n",
      "Epoch [4/20], Step [308/316], Loss: 0.0031\n",
      "Epoch [4/20], Step [309/316], Loss: 0.1264\n",
      "Epoch [4/20], Step [310/316], Loss: 0.0081\n",
      "Epoch [4/20], Step [311/316], Loss: 0.0065\n",
      "Epoch [4/20], Step [312/316], Loss: 0.0033\n",
      "Epoch [4/20], Step [313/316], Loss: 0.1741\n",
      "Epoch [4/20], Step [314/316], Loss: 0.0010\n",
      "Epoch [4/20], Step [315/316], Loss: 0.0240\n",
      "Epoch [4/20], Step [316/316], Loss: 0.0033\n",
      "Epoch [4/20], Train Loss: 0.0336\n",
      "Epoch [4/20], Validation Loss: 0.1631\n",
      "Epoch [4/20], Validation Accuracy: 0.8101\n",
      "Epoch [5/20], Step [1/316], Loss: 0.0062\n",
      "Epoch [5/20], Step [2/316], Loss: 0.0127\n",
      "Epoch [5/20], Step [3/316], Loss: 0.0078\n",
      "Epoch [5/20], Step [4/316], Loss: 0.0311\n",
      "Epoch [5/20], Step [5/316], Loss: 0.0031\n",
      "Epoch [5/20], Step [6/316], Loss: 0.0072\n",
      "Epoch [5/20], Step [7/316], Loss: 0.0820\n",
      "Epoch [5/20], Step [8/316], Loss: 0.0046\n",
      "Epoch [5/20], Step [9/316], Loss: 0.0039\n",
      "Epoch [5/20], Step [10/316], Loss: 0.0701\n",
      "Epoch [5/20], Step [11/316], Loss: 0.0144\n",
      "Epoch [5/20], Step [12/316], Loss: 0.0081\n",
      "Epoch [5/20], Step [13/316], Loss: 0.0614\n",
      "Epoch [5/20], Step [14/316], Loss: 0.0028\n",
      "Epoch [5/20], Step [15/316], Loss: 0.0139\n",
      "Epoch [5/20], Step [16/316], Loss: 0.0273\n",
      "Epoch [5/20], Step [17/316], Loss: 0.0647\n",
      "Epoch [5/20], Step [18/316], Loss: 0.0076\n",
      "Epoch [5/20], Step [19/316], Loss: 0.0118\n",
      "Epoch [5/20], Step [20/316], Loss: 0.0123\n",
      "Epoch [5/20], Step [21/316], Loss: 0.0082\n",
      "Epoch [5/20], Step [22/316], Loss: 0.0148\n",
      "Epoch [5/20], Step [23/316], Loss: 0.0604\n",
      "Epoch [5/20], Step [24/316], Loss: 0.0704\n",
      "Epoch [5/20], Step [25/316], Loss: 0.0156\n",
      "Epoch [5/20], Step [26/316], Loss: 0.0158\n",
      "Epoch [5/20], Step [27/316], Loss: 0.0913\n",
      "Epoch [5/20], Step [28/316], Loss: 0.0006\n",
      "Epoch [5/20], Step [29/316], Loss: 0.0030\n",
      "Epoch [5/20], Step [30/316], Loss: 0.2128\n",
      "Epoch [5/20], Step [31/316], Loss: 0.0188\n",
      "Epoch [5/20], Step [32/316], Loss: 0.0036\n",
      "Epoch [5/20], Step [33/316], Loss: 0.0069\n",
      "Epoch [5/20], Step [34/316], Loss: 0.0025\n",
      "Epoch [5/20], Step [35/316], Loss: 0.0014\n",
      "Epoch [5/20], Step [36/316], Loss: 0.0080\n",
      "Epoch [5/20], Step [37/316], Loss: 0.0066\n",
      "Epoch [5/20], Step [38/316], Loss: 0.0057\n",
      "Epoch [5/20], Step [39/316], Loss: 0.0048\n",
      "Epoch [5/20], Step [40/316], Loss: 0.1414\n",
      "Epoch [5/20], Step [41/316], Loss: 0.0225\n",
      "Epoch [5/20], Step [42/316], Loss: 0.0153\n",
      "Epoch [5/20], Step [43/316], Loss: 0.0047\n",
      "Epoch [5/20], Step [44/316], Loss: 0.0096\n",
      "Epoch [5/20], Step [45/316], Loss: 0.2249\n",
      "Epoch [5/20], Step [46/316], Loss: 0.0042\n",
      "Epoch [5/20], Step [47/316], Loss: 0.0265\n",
      "Epoch [5/20], Step [48/316], Loss: 0.0128\n",
      "Epoch [5/20], Step [49/316], Loss: 0.0432\n",
      "Epoch [5/20], Step [50/316], Loss: 0.0451\n",
      "Epoch [5/20], Step [51/316], Loss: 0.1540\n",
      "Epoch [5/20], Step [52/316], Loss: 0.1368\n",
      "Epoch [5/20], Step [53/316], Loss: 0.0030\n",
      "Epoch [5/20], Step [54/316], Loss: 0.0163\n",
      "Epoch [5/20], Step [55/316], Loss: 0.0248\n",
      "Epoch [5/20], Step [56/316], Loss: 0.0190\n",
      "Epoch [5/20], Step [57/316], Loss: 0.0176\n",
      "Epoch [5/20], Step [58/316], Loss: 0.0106\n",
      "Epoch [5/20], Step [59/316], Loss: 0.1045\n",
      "Epoch [5/20], Step [60/316], Loss: 0.0072\n",
      "Epoch [5/20], Step [61/316], Loss: 0.0047\n",
      "Epoch [5/20], Step [62/316], Loss: 0.0072\n",
      "Epoch [5/20], Step [63/316], Loss: 0.1623\n",
      "Epoch [5/20], Step [64/316], Loss: 0.0096\n",
      "Epoch [5/20], Step [65/316], Loss: 0.0011\n",
      "Epoch [5/20], Step [66/316], Loss: 0.0024\n",
      "Epoch [5/20], Step [67/316], Loss: 0.0023\n",
      "Epoch [5/20], Step [68/316], Loss: 0.0021\n",
      "Epoch [5/20], Step [69/316], Loss: 0.0017\n",
      "Epoch [5/20], Step [70/316], Loss: 0.0067\n",
      "Epoch [5/20], Step [71/316], Loss: 0.0029\n",
      "Epoch [5/20], Step [72/316], Loss: 0.0139\n",
      "Epoch [5/20], Step [73/316], Loss: 0.0032\n",
      "Epoch [5/20], Step [74/316], Loss: 0.0482\n",
      "Epoch [5/20], Step [75/316], Loss: 0.0038\n",
      "Epoch [5/20], Step [76/316], Loss: 0.0031\n",
      "Epoch [5/20], Step [77/316], Loss: 0.0022\n",
      "Epoch [5/20], Step [78/316], Loss: 0.0349\n",
      "Epoch [5/20], Step [79/316], Loss: 0.0100\n",
      "Epoch [5/20], Step [80/316], Loss: 0.0030\n",
      "Epoch [5/20], Step [81/316], Loss: 0.0021\n",
      "Epoch [5/20], Step [82/316], Loss: 0.0052\n",
      "Epoch [5/20], Step [83/316], Loss: 0.0038\n",
      "Epoch [5/20], Step [84/316], Loss: 0.0065\n",
      "Epoch [5/20], Step [85/316], Loss: 0.1246\n",
      "Epoch [5/20], Step [86/316], Loss: 0.0014\n",
      "Epoch [5/20], Step [87/316], Loss: 0.0014\n",
      "Epoch [5/20], Step [88/316], Loss: 0.0300\n",
      "Epoch [5/20], Step [89/316], Loss: 0.0266\n",
      "Epoch [5/20], Step [90/316], Loss: 0.2309\n",
      "Epoch [5/20], Step [91/316], Loss: 0.0039\n",
      "Epoch [5/20], Step [92/316], Loss: 0.0087\n",
      "Epoch [5/20], Step [93/316], Loss: 0.0046\n",
      "Epoch [5/20], Step [94/316], Loss: 0.0032\n",
      "Epoch [5/20], Step [95/316], Loss: 0.0088\n",
      "Epoch [5/20], Step [96/316], Loss: 0.0265\n",
      "Epoch [5/20], Step [97/316], Loss: 0.2199\n",
      "Epoch [5/20], Step [98/316], Loss: 0.0042\n",
      "Epoch [5/20], Step [99/316], Loss: 0.0014\n",
      "Epoch [5/20], Step [100/316], Loss: 0.0054\n",
      "Epoch [5/20], Step [101/316], Loss: 0.0098\n",
      "Epoch [5/20], Step [102/316], Loss: 0.0037\n",
      "Epoch [5/20], Step [103/316], Loss: 0.0056\n",
      "Epoch [5/20], Step [104/316], Loss: 0.0336\n",
      "Epoch [5/20], Step [105/316], Loss: 0.0022\n",
      "Epoch [5/20], Step [106/316], Loss: 0.1113\n",
      "Epoch [5/20], Step [107/316], Loss: 0.0049\n",
      "Epoch [5/20], Step [108/316], Loss: 0.0148\n",
      "Epoch [5/20], Step [109/316], Loss: 0.0117\n",
      "Epoch [5/20], Step [110/316], Loss: 0.0153\n",
      "Epoch [5/20], Step [111/316], Loss: 0.0038\n",
      "Epoch [5/20], Step [112/316], Loss: 0.0021\n",
      "Epoch [5/20], Step [113/316], Loss: 0.0021\n",
      "Epoch [5/20], Step [114/316], Loss: 0.0022\n",
      "Epoch [5/20], Step [115/316], Loss: 0.0043\n",
      "Epoch [5/20], Step [116/316], Loss: 0.0064\n",
      "Epoch [5/20], Step [117/316], Loss: 0.0040\n",
      "Epoch [5/20], Step [118/316], Loss: 0.0383\n",
      "Epoch [5/20], Step [119/316], Loss: 0.1618\n",
      "Epoch [5/20], Step [120/316], Loss: 0.0021\n",
      "Epoch [5/20], Step [121/316], Loss: 0.0022\n",
      "Epoch [5/20], Step [122/316], Loss: 0.0455\n",
      "Epoch [5/20], Step [123/316], Loss: 0.0052\n",
      "Epoch [5/20], Step [124/316], Loss: 0.1169\n",
      "Epoch [5/20], Step [125/316], Loss: 0.0084\n",
      "Epoch [5/20], Step [126/316], Loss: 0.0078\n",
      "Epoch [5/20], Step [127/316], Loss: 0.0035\n",
      "Epoch [5/20], Step [128/316], Loss: 0.0049\n",
      "Epoch [5/20], Step [129/316], Loss: 0.1293\n",
      "Epoch [5/20], Step [130/316], Loss: 0.0045\n",
      "Epoch [5/20], Step [131/316], Loss: 0.0034\n",
      "Epoch [5/20], Step [132/316], Loss: 0.0066\n",
      "Epoch [5/20], Step [133/316], Loss: 0.0025\n",
      "Epoch [5/20], Step [134/316], Loss: 0.0369\n",
      "Epoch [5/20], Step [135/316], Loss: 0.0050\n",
      "Epoch [5/20], Step [136/316], Loss: 0.0017\n",
      "Epoch [5/20], Step [137/316], Loss: 0.0140\n",
      "Epoch [5/20], Step [138/316], Loss: 0.0038\n",
      "Epoch [5/20], Step [139/316], Loss: 0.0143\n",
      "Epoch [5/20], Step [140/316], Loss: 0.0066\n",
      "Epoch [5/20], Step [141/316], Loss: 0.0943\n",
      "Epoch [5/20], Step [142/316], Loss: 0.0039\n",
      "Epoch [5/20], Step [143/316], Loss: 0.0093\n",
      "Epoch [5/20], Step [144/316], Loss: 0.0190\n",
      "Epoch [5/20], Step [145/316], Loss: 0.0005\n",
      "Epoch [5/20], Step [146/316], Loss: 0.0026\n",
      "Epoch [5/20], Step [147/316], Loss: 0.0732\n",
      "Epoch [5/20], Step [148/316], Loss: 0.1133\n",
      "Epoch [5/20], Step [149/316], Loss: 0.0019\n",
      "Epoch [5/20], Step [150/316], Loss: 0.0035\n",
      "Epoch [5/20], Step [151/316], Loss: 0.0023\n",
      "Epoch [5/20], Step [152/316], Loss: 0.1191\n",
      "Epoch [5/20], Step [153/316], Loss: 0.0024\n",
      "Epoch [5/20], Step [154/316], Loss: 0.1149\n",
      "Epoch [5/20], Step [155/316], Loss: 0.0054\n",
      "Epoch [5/20], Step [156/316], Loss: 0.0165\n",
      "Epoch [5/20], Step [157/316], Loss: 0.0333\n",
      "Epoch [5/20], Step [158/316], Loss: 0.0068\n",
      "Epoch [5/20], Step [159/316], Loss: 0.0112\n",
      "Epoch [5/20], Step [160/316], Loss: 0.1164\n",
      "Epoch [5/20], Step [161/316], Loss: 0.0121\n",
      "Epoch [5/20], Step [162/316], Loss: 0.0326\n",
      "Epoch [5/20], Step [163/316], Loss: 0.0154\n",
      "Epoch [5/20], Step [164/316], Loss: 0.0128\n",
      "Epoch [5/20], Step [165/316], Loss: 0.0147\n",
      "Epoch [5/20], Step [166/316], Loss: 0.0110\n",
      "Epoch [5/20], Step [167/316], Loss: 0.0126\n",
      "Epoch [5/20], Step [168/316], Loss: 0.0433\n",
      "Epoch [5/20], Step [169/316], Loss: 0.0079\n",
      "Epoch [5/20], Step [170/316], Loss: 0.1364\n",
      "Epoch [5/20], Step [171/316], Loss: 0.0081\n",
      "Epoch [5/20], Step [172/316], Loss: 0.1312\n",
      "Epoch [5/20], Step [173/316], Loss: 0.0058\n",
      "Epoch [5/20], Step [174/316], Loss: 0.0049\n",
      "Epoch [5/20], Step [175/316], Loss: 0.0072\n",
      "Epoch [5/20], Step [176/316], Loss: 0.0108\n",
      "Epoch [5/20], Step [177/316], Loss: 0.1733\n",
      "Epoch [5/20], Step [178/316], Loss: 0.0104\n",
      "Epoch [5/20], Step [179/316], Loss: 0.2313\n",
      "Epoch [5/20], Step [180/316], Loss: 0.0040\n",
      "Epoch [5/20], Step [181/316], Loss: 0.0085\n",
      "Epoch [5/20], Step [182/316], Loss: 0.0062\n",
      "Epoch [5/20], Step [183/316], Loss: 0.0054\n",
      "Epoch [5/20], Step [184/316], Loss: 0.0172\n",
      "Epoch [5/20], Step [185/316], Loss: 0.0038\n",
      "Epoch [5/20], Step [186/316], Loss: 0.0219\n",
      "Epoch [5/20], Step [187/316], Loss: 0.0084\n",
      "Epoch [5/20], Step [188/316], Loss: 0.1293\n",
      "Epoch [5/20], Step [189/316], Loss: 0.0212\n",
      "Epoch [5/20], Step [190/316], Loss: 0.0087\n",
      "Epoch [5/20], Step [191/316], Loss: 0.0150\n",
      "Epoch [5/20], Step [192/316], Loss: 0.0561\n",
      "Epoch [5/20], Step [193/316], Loss: 0.0078\n",
      "Epoch [5/20], Step [194/316], Loss: 0.0166\n",
      "Epoch [5/20], Step [195/316], Loss: 0.0058\n",
      "Epoch [5/20], Step [196/316], Loss: 0.0876\n",
      "Epoch [5/20], Step [197/316], Loss: 0.0137\n",
      "Epoch [5/20], Step [198/316], Loss: 0.0024\n",
      "Epoch [5/20], Step [199/316], Loss: 0.0057\n",
      "Epoch [5/20], Step [200/316], Loss: 0.1031\n",
      "Epoch [5/20], Step [201/316], Loss: 0.0036\n",
      "Epoch [5/20], Step [202/316], Loss: 0.0125\n",
      "Epoch [5/20], Step [203/316], Loss: 0.0513\n",
      "Epoch [5/20], Step [204/316], Loss: 0.2135\n",
      "Epoch [5/20], Step [205/316], Loss: 0.0122\n",
      "Epoch [5/20], Step [206/316], Loss: 0.0050\n",
      "Epoch [5/20], Step [207/316], Loss: 0.0085\n",
      "Epoch [5/20], Step [208/316], Loss: 0.0120\n",
      "Epoch [5/20], Step [209/316], Loss: 0.0053\n",
      "Epoch [5/20], Step [210/316], Loss: 0.0032\n",
      "Epoch [5/20], Step [211/316], Loss: 0.0351\n",
      "Epoch [5/20], Step [212/316], Loss: 0.0020\n",
      "Epoch [5/20], Step [213/316], Loss: 0.0031\n",
      "Epoch [5/20], Step [214/316], Loss: 0.0087\n",
      "Epoch [5/20], Step [215/316], Loss: 0.0620\n",
      "Epoch [5/20], Step [216/316], Loss: 0.0701\n",
      "Epoch [5/20], Step [217/316], Loss: 0.0162\n",
      "Epoch [5/20], Step [218/316], Loss: 0.0057\n",
      "Epoch [5/20], Step [219/316], Loss: 0.0081\n",
      "Epoch [5/20], Step [220/316], Loss: 0.0110\n",
      "Epoch [5/20], Step [221/316], Loss: 0.0367\n",
      "Epoch [5/20], Step [222/316], Loss: 0.0080\n",
      "Epoch [5/20], Step [223/316], Loss: 0.0056\n",
      "Epoch [5/20], Step [224/316], Loss: 0.0043\n",
      "Epoch [5/20], Step [225/316], Loss: 0.0022\n",
      "Epoch [5/20], Step [226/316], Loss: 0.0109\n",
      "Epoch [5/20], Step [227/316], Loss: 0.0492\n",
      "Epoch [5/20], Step [228/316], Loss: 0.0041\n",
      "Epoch [5/20], Step [229/316], Loss: 0.0102\n",
      "Epoch [5/20], Step [230/316], Loss: 0.0137\n",
      "Epoch [5/20], Step [231/316], Loss: 0.0079\n",
      "Epoch [5/20], Step [232/316], Loss: 0.0036\n",
      "Epoch [5/20], Step [233/316], Loss: 0.0050\n",
      "Epoch [5/20], Step [234/316], Loss: 0.0061\n",
      "Epoch [5/20], Step [235/316], Loss: 0.0072\n",
      "Epoch [5/20], Step [236/316], Loss: 0.1076\n",
      "Epoch [5/20], Step [237/316], Loss: 0.0102\n",
      "Epoch [5/20], Step [238/316], Loss: 0.0072\n",
      "Epoch [5/20], Step [239/316], Loss: 0.0051\n",
      "Epoch [5/20], Step [240/316], Loss: 0.0038\n",
      "Epoch [5/20], Step [241/316], Loss: 0.0085\n",
      "Epoch [5/20], Step [242/316], Loss: 0.0025\n",
      "Epoch [5/20], Step [243/316], Loss: 0.0079\n",
      "Epoch [5/20], Step [244/316], Loss: 0.0201\n",
      "Epoch [5/20], Step [245/316], Loss: 0.0148\n",
      "Epoch [5/20], Step [246/316], Loss: 0.0062\n",
      "Epoch [5/20], Step [247/316], Loss: 0.0501\n",
      "Epoch [5/20], Step [248/316], Loss: 0.0128\n",
      "Epoch [5/20], Step [249/316], Loss: 0.0714\n",
      "Epoch [5/20], Step [250/316], Loss: 0.0299\n",
      "Epoch [5/20], Step [251/316], Loss: 0.0037\n",
      "Epoch [5/20], Step [252/316], Loss: 0.0022\n",
      "Epoch [5/20], Step [253/316], Loss: 0.0036\n",
      "Epoch [5/20], Step [254/316], Loss: 0.0038\n",
      "Epoch [5/20], Step [255/316], Loss: 0.0168\n",
      "Epoch [5/20], Step [256/316], Loss: 0.0937\n",
      "Epoch [5/20], Step [257/316], Loss: 0.0036\n",
      "Epoch [5/20], Step [258/316], Loss: 0.0076\n",
      "Epoch [5/20], Step [259/316], Loss: 0.0029\n",
      "Epoch [5/20], Step [260/316], Loss: 0.0036\n",
      "Epoch [5/20], Step [261/316], Loss: 0.1640\n",
      "Epoch [5/20], Step [262/316], Loss: 0.0114\n",
      "Epoch [5/20], Step [263/316], Loss: 0.0065\n",
      "Epoch [5/20], Step [264/316], Loss: 0.0065\n",
      "Epoch [5/20], Step [265/316], Loss: 0.0031\n",
      "Epoch [5/20], Step [266/316], Loss: 0.0014\n",
      "Epoch [5/20], Step [267/316], Loss: 0.0022\n",
      "Epoch [5/20], Step [268/316], Loss: 0.0074\n",
      "Epoch [5/20], Step [269/316], Loss: 0.0056\n",
      "Epoch [5/20], Step [270/316], Loss: 0.0502\n",
      "Epoch [5/20], Step [271/316], Loss: 0.0010\n",
      "Epoch [5/20], Step [272/316], Loss: 0.0159\n",
      "Epoch [5/20], Step [273/316], Loss: 0.0260\n",
      "Epoch [5/20], Step [274/316], Loss: 0.0044\n",
      "Epoch [5/20], Step [275/316], Loss: 0.0147\n",
      "Epoch [5/20], Step [276/316], Loss: 0.0051\n",
      "Epoch [5/20], Step [277/316], Loss: 0.0033\n",
      "Epoch [5/20], Step [278/316], Loss: 0.0037\n",
      "Epoch [5/20], Step [279/316], Loss: 0.0030\n",
      "Epoch [5/20], Step [280/316], Loss: 0.0173\n",
      "Epoch [5/20], Step [281/316], Loss: 0.0620\n",
      "Epoch [5/20], Step [282/316], Loss: 0.0801\n",
      "Epoch [5/20], Step [283/316], Loss: 0.0012\n",
      "Epoch [5/20], Step [284/316], Loss: 0.0042\n",
      "Epoch [5/20], Step [285/316], Loss: 0.0001\n",
      "Epoch [5/20], Step [286/316], Loss: 0.0069\n",
      "Epoch [5/20], Step [287/316], Loss: 0.0076\n",
      "Epoch [5/20], Step [288/316], Loss: 0.0010\n",
      "Epoch [5/20], Step [289/316], Loss: 0.0021\n",
      "Epoch [5/20], Step [290/316], Loss: 0.0569\n",
      "Epoch [5/20], Step [291/316], Loss: 0.0052\n",
      "Epoch [5/20], Step [292/316], Loss: 0.0073\n",
      "Epoch [5/20], Step [293/316], Loss: 0.0074\n",
      "Epoch [5/20], Step [294/316], Loss: 0.0125\n",
      "Epoch [5/20], Step [295/316], Loss: 0.0002\n",
      "Epoch [5/20], Step [296/316], Loss: 0.0027\n",
      "Epoch [5/20], Step [297/316], Loss: 0.0438\n",
      "Epoch [5/20], Step [298/316], Loss: 0.0089\n",
      "Epoch [5/20], Step [299/316], Loss: 0.0027\n",
      "Epoch [5/20], Step [300/316], Loss: 0.0067\n",
      "Epoch [5/20], Step [301/316], Loss: 0.0046\n",
      "Epoch [5/20], Step [302/316], Loss: 0.0033\n",
      "Epoch [5/20], Step [303/316], Loss: 0.0053\n",
      "Epoch [5/20], Step [304/316], Loss: 0.0077\n",
      "Epoch [5/20], Step [305/316], Loss: 0.0045\n",
      "Epoch [5/20], Step [306/316], Loss: 0.0113\n",
      "Epoch [5/20], Step [307/316], Loss: 0.0910\n",
      "Epoch [5/20], Step [308/316], Loss: 0.0724\n",
      "Epoch [5/20], Step [309/316], Loss: 0.0234\n",
      "Epoch [5/20], Step [310/316], Loss: 0.0034\n",
      "Epoch [5/20], Step [311/316], Loss: 0.0080\n",
      "Epoch [5/20], Step [312/316], Loss: 0.0080\n",
      "Epoch [5/20], Step [313/316], Loss: 0.0055\n",
      "Epoch [5/20], Step [314/316], Loss: 0.0045\n",
      "Epoch [5/20], Step [315/316], Loss: 0.0048\n",
      "Epoch [5/20], Step [316/316], Loss: 0.0095\n",
      "Epoch [5/20], Train Loss: 0.0263\n",
      "Epoch [5/20], Validation Loss: 0.1628\n",
      "Epoch [5/20], Validation Accuracy: 0.8119\n",
      "Epoch [6/20], Step [1/316], Loss: 0.0563\n",
      "Epoch [6/20], Step [2/316], Loss: 0.0037\n",
      "Epoch [6/20], Step [3/316], Loss: 0.0036\n",
      "Epoch [6/20], Step [4/316], Loss: 0.0059\n",
      "Epoch [6/20], Step [5/316], Loss: 0.0677\n",
      "Epoch [6/20], Step [6/316], Loss: 0.0274\n",
      "Epoch [6/20], Step [7/316], Loss: 0.0042\n",
      "Epoch [6/20], Step [8/316], Loss: 0.0028\n",
      "Epoch [6/20], Step [9/316], Loss: 0.0012\n",
      "Epoch [6/20], Step [10/316], Loss: 0.0105\n",
      "Epoch [6/20], Step [11/316], Loss: 0.0039\n",
      "Epoch [6/20], Step [12/316], Loss: 0.0714\n",
      "Epoch [6/20], Step [13/316], Loss: 0.0042\n",
      "Epoch [6/20], Step [14/316], Loss: 0.0692\n",
      "Epoch [6/20], Step [15/316], Loss: 0.0014\n",
      "Epoch [6/20], Step [16/316], Loss: 0.0247\n",
      "Epoch [6/20], Step [17/316], Loss: 0.1343\n",
      "Epoch [6/20], Step [18/316], Loss: 0.0602\n",
      "Epoch [6/20], Step [19/316], Loss: 0.0048\n",
      "Epoch [6/20], Step [20/316], Loss: 0.0283\n",
      "Epoch [6/20], Step [21/316], Loss: 0.0093\n",
      "Epoch [6/20], Step [22/316], Loss: 0.0019\n",
      "Epoch [6/20], Step [23/316], Loss: 0.0094\n",
      "Epoch [6/20], Step [24/316], Loss: 0.0052\n",
      "Epoch [6/20], Step [25/316], Loss: 0.0382\n",
      "Epoch [6/20], Step [26/316], Loss: 0.0061\n",
      "Epoch [6/20], Step [27/316], Loss: 0.0559\n",
      "Epoch [6/20], Step [28/316], Loss: 0.0004\n",
      "Epoch [6/20], Step [29/316], Loss: 0.0141\n",
      "Epoch [6/20], Step [30/316], Loss: 0.0032\n",
      "Epoch [6/20], Step [31/316], Loss: 0.0072\n",
      "Epoch [6/20], Step [32/316], Loss: 0.0066\n",
      "Epoch [6/20], Step [33/316], Loss: 0.0550\n",
      "Epoch [6/20], Step [34/316], Loss: 0.0037\n",
      "Epoch [6/20], Step [35/316], Loss: 0.0027\n",
      "Epoch [6/20], Step [36/316], Loss: 0.0012\n",
      "Epoch [6/20], Step [37/316], Loss: 0.0996\n",
      "Epoch [6/20], Step [38/316], Loss: 0.1408\n",
      "Epoch [6/20], Step [39/316], Loss: 0.0071\n",
      "Epoch [6/20], Step [40/316], Loss: 0.0048\n",
      "Epoch [6/20], Step [41/316], Loss: 0.0128\n",
      "Epoch [6/20], Step [42/316], Loss: 0.0043\n",
      "Epoch [6/20], Step [43/316], Loss: 0.0046\n",
      "Epoch [6/20], Step [44/316], Loss: 0.1662\n",
      "Epoch [6/20], Step [45/316], Loss: 0.0036\n",
      "Epoch [6/20], Step [46/316], Loss: 0.0093\n",
      "Epoch [6/20], Step [47/316], Loss: 0.0036\n",
      "Epoch [6/20], Step [48/316], Loss: 0.0055\n",
      "Epoch [6/20], Step [49/316], Loss: 0.0075\n",
      "Epoch [6/20], Step [50/316], Loss: 0.1599\n",
      "Epoch [6/20], Step [51/316], Loss: 0.0087\n",
      "Epoch [6/20], Step [52/316], Loss: 0.0077\n",
      "Epoch [6/20], Step [53/316], Loss: 0.0047\n",
      "Epoch [6/20], Step [54/316], Loss: 0.0054\n",
      "Epoch [6/20], Step [55/316], Loss: 0.0557\n",
      "Epoch [6/20], Step [56/316], Loss: 0.0098\n",
      "Epoch [6/20], Step [57/316], Loss: 0.0037\n",
      "Epoch [6/20], Step [58/316], Loss: 0.0043\n",
      "Epoch [6/20], Step [59/316], Loss: 0.0061\n",
      "Epoch [6/20], Step [60/316], Loss: 0.0050\n",
      "Epoch [6/20], Step [61/316], Loss: 0.0048\n",
      "Epoch [6/20], Step [62/316], Loss: 0.0325\n",
      "Epoch [6/20], Step [63/316], Loss: 0.0013\n",
      "Epoch [6/20], Step [64/316], Loss: 0.0089\n",
      "Epoch [6/20], Step [65/316], Loss: 0.0023\n",
      "Epoch [6/20], Step [66/316], Loss: 0.0460\n",
      "Epoch [6/20], Step [67/316], Loss: 0.0044\n",
      "Epoch [6/20], Step [68/316], Loss: 0.0018\n",
      "Epoch [6/20], Step [69/316], Loss: 0.0155\n",
      "Epoch [6/20], Step [70/316], Loss: 0.1391\n",
      "Epoch [6/20], Step [71/316], Loss: 0.0084\n",
      "Epoch [6/20], Step [72/316], Loss: 0.0158\n",
      "Epoch [6/20], Step [73/316], Loss: 0.0084\n",
      "Epoch [6/20], Step [74/316], Loss: 0.1220\n",
      "Epoch [6/20], Step [75/316], Loss: 0.0077\n",
      "Epoch [6/20], Step [76/316], Loss: 0.0033\n",
      "Epoch [6/20], Step [77/316], Loss: 0.0014\n",
      "Epoch [6/20], Step [78/316], Loss: 0.0187\n",
      "Epoch [6/20], Step [79/316], Loss: 0.0017\n",
      "Epoch [6/20], Step [80/316], Loss: 0.0605\n",
      "Epoch [6/20], Step [81/316], Loss: 0.0001\n",
      "Epoch [6/20], Step [82/316], Loss: 0.0049\n",
      "Epoch [6/20], Step [83/316], Loss: 0.0023\n",
      "Epoch [6/20], Step [84/316], Loss: 0.0046\n",
      "Epoch [6/20], Step [85/316], Loss: 0.0064\n",
      "Epoch [6/20], Step [86/316], Loss: 0.0025\n",
      "Epoch [6/20], Step [87/316], Loss: 0.0036\n",
      "Epoch [6/20], Step [88/316], Loss: 0.0118\n",
      "Epoch [6/20], Step [89/316], Loss: 0.0088\n",
      "Epoch [6/20], Step [90/316], Loss: 0.0079\n",
      "Epoch [6/20], Step [91/316], Loss: 0.0224\n",
      "Epoch [6/20], Step [92/316], Loss: 0.0022\n",
      "Epoch [6/20], Step [93/316], Loss: 0.0136\n",
      "Epoch [6/20], Step [94/316], Loss: 0.2460\n",
      "Epoch [6/20], Step [95/316], Loss: 0.0034\n",
      "Epoch [6/20], Step [96/316], Loss: 0.0032\n",
      "Epoch [6/20], Step [97/316], Loss: 0.0016\n",
      "Epoch [6/20], Step [98/316], Loss: 0.0496\n",
      "Epoch [6/20], Step [99/316], Loss: 0.0193\n",
      "Epoch [6/20], Step [100/316], Loss: 0.0050\n",
      "Epoch [6/20], Step [101/316], Loss: 0.0187\n",
      "Epoch [6/20], Step [102/316], Loss: 0.0017\n",
      "Epoch [6/20], Step [103/316], Loss: 0.0066\n",
      "Epoch [6/20], Step [104/316], Loss: 0.0112\n",
      "Epoch [6/20], Step [105/316], Loss: 0.0073\n",
      "Epoch [6/20], Step [106/316], Loss: 0.0039\n",
      "Epoch [6/20], Step [107/316], Loss: 0.0074\n",
      "Epoch [6/20], Step [108/316], Loss: 0.0157\n",
      "Epoch [6/20], Step [109/316], Loss: 0.0112\n",
      "Epoch [6/20], Step [110/316], Loss: 0.0060\n",
      "Epoch [6/20], Step [111/316], Loss: 0.0093\n",
      "Epoch [6/20], Step [112/316], Loss: 0.0990\n",
      "Epoch [6/20], Step [113/316], Loss: 0.0083\n",
      "Epoch [6/20], Step [114/316], Loss: 0.0058\n",
      "Epoch [6/20], Step [115/316], Loss: 0.1009\n",
      "Epoch [6/20], Step [116/316], Loss: 0.0042\n",
      "Epoch [6/20], Step [117/316], Loss: 0.0754\n",
      "Epoch [6/20], Step [118/316], Loss: 0.0023\n",
      "Epoch [6/20], Step [119/316], Loss: 0.0046\n",
      "Epoch [6/20], Step [120/316], Loss: 0.0057\n",
      "Epoch [6/20], Step [121/316], Loss: 0.0015\n",
      "Epoch [6/20], Step [122/316], Loss: 0.0072\n",
      "Epoch [6/20], Step [123/316], Loss: 0.1350\n",
      "Epoch [6/20], Step [124/316], Loss: 0.0548\n",
      "Epoch [6/20], Step [125/316], Loss: 0.0059\n",
      "Epoch [6/20], Step [126/316], Loss: 0.0066\n",
      "Epoch [6/20], Step [127/316], Loss: 0.2165\n",
      "Epoch [6/20], Step [128/316], Loss: 0.0071\n",
      "Epoch [6/20], Step [129/316], Loss: 0.0064\n",
      "Epoch [6/20], Step [130/316], Loss: 0.0016\n",
      "Epoch [6/20], Step [131/316], Loss: 0.0631\n",
      "Epoch [6/20], Step [132/316], Loss: 0.0026\n",
      "Epoch [6/20], Step [133/316], Loss: 0.0039\n",
      "Epoch [6/20], Step [134/316], Loss: 0.0154\n",
      "Epoch [6/20], Step [135/316], Loss: 0.0141\n",
      "Epoch [6/20], Step [136/316], Loss: 0.0031\n",
      "Epoch [6/20], Step [137/316], Loss: 0.1539\n",
      "Epoch [6/20], Step [138/316], Loss: 0.0008\n",
      "Epoch [6/20], Step [139/316], Loss: 0.1266\n",
      "Epoch [6/20], Step [140/316], Loss: 0.0756\n",
      "Epoch [6/20], Step [141/316], Loss: 0.0721\n",
      "Epoch [6/20], Step [142/316], Loss: 0.0033\n",
      "Epoch [6/20], Step [143/316], Loss: 0.0042\n",
      "Epoch [6/20], Step [144/316], Loss: 0.1229\n",
      "Epoch [6/20], Step [145/316], Loss: 0.0041\n",
      "Epoch [6/20], Step [146/316], Loss: 0.0037\n",
      "Epoch [6/20], Step [147/316], Loss: 0.0126\n",
      "Epoch [6/20], Step [148/316], Loss: 0.1387\n",
      "Epoch [6/20], Step [149/316], Loss: 0.0101\n",
      "Epoch [6/20], Step [150/316], Loss: 0.1135\n",
      "Epoch [6/20], Step [151/316], Loss: 0.1192\n",
      "Epoch [6/20], Step [152/316], Loss: 0.0029\n",
      "Epoch [6/20], Step [153/316], Loss: 0.0905\n",
      "Epoch [6/20], Step [154/316], Loss: 0.0031\n",
      "Epoch [6/20], Step [155/316], Loss: 0.0076\n",
      "Epoch [6/20], Step [156/316], Loss: 0.0187\n",
      "Epoch [6/20], Step [157/316], Loss: 0.0763\n",
      "Epoch [6/20], Step [158/316], Loss: 0.1383\n",
      "Epoch [6/20], Step [159/316], Loss: 0.0804\n",
      "Epoch [6/20], Step [160/316], Loss: 0.0053\n",
      "Epoch [6/20], Step [161/316], Loss: 0.0031\n",
      "Epoch [6/20], Step [162/316], Loss: 0.0085\n",
      "Epoch [6/20], Step [163/316], Loss: 0.0667\n",
      "Epoch [6/20], Step [164/316], Loss: 0.0032\n",
      "Epoch [6/20], Step [165/316], Loss: 0.0106\n",
      "Epoch [6/20], Step [166/316], Loss: 0.0083\n",
      "Epoch [6/20], Step [167/316], Loss: 0.0049\n",
      "Epoch [6/20], Step [168/316], Loss: 0.0096\n",
      "Epoch [6/20], Step [169/316], Loss: 0.1675\n",
      "Epoch [6/20], Step [170/316], Loss: 0.0081\n",
      "Epoch [6/20], Step [171/316], Loss: 0.0013\n",
      "Epoch [6/20], Step [172/316], Loss: 0.0031\n",
      "Epoch [6/20], Step [173/316], Loss: 0.0133\n",
      "Epoch [6/20], Step [174/316], Loss: 0.1030\n",
      "Epoch [6/20], Step [175/316], Loss: 0.0057\n",
      "Epoch [6/20], Step [176/316], Loss: 0.0047\n",
      "Epoch [6/20], Step [177/316], Loss: 0.0035\n",
      "Epoch [6/20], Step [178/316], Loss: 0.0293\n",
      "Epoch [6/20], Step [179/316], Loss: 0.0049\n",
      "Epoch [6/20], Step [180/316], Loss: 0.0166\n",
      "Epoch [6/20], Step [181/316], Loss: 0.1262\n",
      "Epoch [6/20], Step [182/316], Loss: 0.0404\n",
      "Epoch [6/20], Step [183/316], Loss: 0.0074\n",
      "Epoch [6/20], Step [184/316], Loss: 0.0042\n",
      "Epoch [6/20], Step [185/316], Loss: 0.0078\n",
      "Epoch [6/20], Step [186/316], Loss: 0.0929\n",
      "Epoch [6/20], Step [187/316], Loss: 0.0022\n",
      "Epoch [6/20], Step [188/316], Loss: 0.0196\n",
      "Epoch [6/20], Step [189/316], Loss: 0.0061\n",
      "Epoch [6/20], Step [190/316], Loss: 0.1034\n",
      "Epoch [6/20], Step [191/316], Loss: 0.0073\n",
      "Epoch [6/20], Step [192/316], Loss: 0.0869\n",
      "Epoch [6/20], Step [193/316], Loss: 0.0050\n",
      "Epoch [6/20], Step [194/316], Loss: 0.0113\n",
      "Epoch [6/20], Step [195/316], Loss: 0.0067\n",
      "Epoch [6/20], Step [196/316], Loss: 0.0790\n",
      "Epoch [6/20], Step [197/316], Loss: 0.0601\n",
      "Epoch [6/20], Step [198/316], Loss: 0.0066\n",
      "Epoch [6/20], Step [199/316], Loss: 0.0233\n",
      "Epoch [6/20], Step [200/316], Loss: 0.0147\n",
      "Epoch [6/20], Step [201/316], Loss: 0.0035\n",
      "Epoch [6/20], Step [202/316], Loss: 0.0049\n",
      "Epoch [6/20], Step [203/316], Loss: 0.0261\n",
      "Epoch [6/20], Step [204/316], Loss: 0.0078\n",
      "Epoch [6/20], Step [205/316], Loss: 0.0071\n",
      "Epoch [6/20], Step [206/316], Loss: 0.0053\n",
      "Epoch [6/20], Step [207/316], Loss: 0.0075\n",
      "Epoch [6/20], Step [208/316], Loss: 0.0072\n",
      "Epoch [6/20], Step [209/316], Loss: 0.0176\n",
      "Epoch [6/20], Step [210/316], Loss: 0.0073\n",
      "Epoch [6/20], Step [211/316], Loss: 0.0044\n",
      "Epoch [6/20], Step [212/316], Loss: 0.0391\n",
      "Epoch [6/20], Step [213/316], Loss: 0.0030\n",
      "Epoch [6/20], Step [214/316], Loss: 0.0057\n",
      "Epoch [6/20], Step [215/316], Loss: 0.0022\n",
      "Epoch [6/20], Step [216/316], Loss: 0.0087\n",
      "Epoch [6/20], Step [217/316], Loss: 0.0058\n",
      "Epoch [6/20], Step [218/316], Loss: 0.0186\n",
      "Epoch [6/20], Step [219/316], Loss: 0.0278\n",
      "Epoch [6/20], Step [220/316], Loss: 0.0005\n",
      "Epoch [6/20], Step [221/316], Loss: 0.0049\n",
      "Epoch [6/20], Step [222/316], Loss: 0.0055\n",
      "Epoch [6/20], Step [223/316], Loss: 0.0089\n",
      "Epoch [6/20], Step [224/316], Loss: 0.0053\n",
      "Epoch [6/20], Step [225/316], Loss: 0.1144\n",
      "Epoch [6/20], Step [226/316], Loss: 0.0028\n",
      "Epoch [6/20], Step [227/316], Loss: 0.0048\n",
      "Epoch [6/20], Step [228/316], Loss: 0.0421\n",
      "Epoch [6/20], Step [229/316], Loss: 0.0339\n",
      "Epoch [6/20], Step [230/316], Loss: 0.0129\n",
      "Epoch [6/20], Step [231/316], Loss: 0.0070\n",
      "Epoch [6/20], Step [232/316], Loss: 0.0077\n",
      "Epoch [6/20], Step [233/316], Loss: 0.0058\n",
      "Epoch [6/20], Step [234/316], Loss: 0.0232\n",
      "Epoch [6/20], Step [235/316], Loss: 0.0043\n",
      "Epoch [6/20], Step [236/316], Loss: 0.0107\n",
      "Epoch [6/20], Step [237/316], Loss: 0.0063\n",
      "Epoch [6/20], Step [238/316], Loss: 0.0115\n",
      "Epoch [6/20], Step [239/316], Loss: 0.1229\n",
      "Epoch [6/20], Step [240/316], Loss: 0.0023\n",
      "Epoch [6/20], Step [241/316], Loss: 0.0295\n",
      "Epoch [6/20], Step [242/316], Loss: 0.0092\n",
      "Epoch [6/20], Step [243/316], Loss: 0.0032\n",
      "Epoch [6/20], Step [244/316], Loss: 0.1449\n",
      "Epoch [6/20], Step [245/316], Loss: 0.0095\n",
      "Epoch [6/20], Step [246/316], Loss: 0.1428\n",
      "Epoch [6/20], Step [247/316], Loss: 0.0196\n",
      "Epoch [6/20], Step [248/316], Loss: 0.0039\n",
      "Epoch [6/20], Step [249/316], Loss: 0.0031\n",
      "Epoch [6/20], Step [250/316], Loss: 0.1229\n",
      "Epoch [6/20], Step [251/316], Loss: 0.0020\n",
      "Epoch [6/20], Step [252/316], Loss: 0.0050\n",
      "Epoch [6/20], Step [253/316], Loss: 0.0048\n",
      "Epoch [6/20], Step [254/316], Loss: 0.0083\n",
      "Epoch [6/20], Step [255/316], Loss: 0.2163\n",
      "Epoch [6/20], Step [256/316], Loss: 0.0049\n",
      "Epoch [6/20], Step [257/316], Loss: 0.0069\n",
      "Epoch [6/20], Step [258/316], Loss: 0.0401\n",
      "Epoch [6/20], Step [259/316], Loss: 0.0267\n",
      "Epoch [6/20], Step [260/316], Loss: 0.0085\n",
      "Epoch [6/20], Step [261/316], Loss: 0.1231\n",
      "Epoch [6/20], Step [262/316], Loss: 0.0081\n",
      "Epoch [6/20], Step [263/316], Loss: 0.0375\n",
      "Epoch [6/20], Step [264/316], Loss: 0.0077\n",
      "Epoch [6/20], Step [265/316], Loss: 0.0012\n",
      "Epoch [6/20], Step [266/316], Loss: 0.0041\n",
      "Epoch [6/20], Step [267/316], Loss: 0.0100\n",
      "Epoch [6/20], Step [268/316], Loss: 0.0023\n",
      "Epoch [6/20], Step [269/316], Loss: 0.0946\n",
      "Epoch [6/20], Step [270/316], Loss: 0.0191\n",
      "Epoch [6/20], Step [271/316], Loss: 0.0049\n",
      "Epoch [6/20], Step [272/316], Loss: 0.0195\n",
      "Epoch [6/20], Step [273/316], Loss: 0.1346\n",
      "Epoch [6/20], Step [274/316], Loss: 0.0129\n",
      "Epoch [6/20], Step [275/316], Loss: 0.0438\n",
      "Epoch [6/20], Step [276/316], Loss: 0.0079\n",
      "Epoch [6/20], Step [277/316], Loss: 0.0423\n",
      "Epoch [6/20], Step [278/316], Loss: 0.0066\n",
      "Epoch [6/20], Step [279/316], Loss: 0.0512\n",
      "Epoch [6/20], Step [280/316], Loss: 0.0158\n",
      "Epoch [6/20], Step [281/316], Loss: 0.0058\n",
      "Epoch [6/20], Step [282/316], Loss: 0.0085\n",
      "Epoch [6/20], Step [283/316], Loss: 0.0055\n",
      "Epoch [6/20], Step [284/316], Loss: 0.0062\n",
      "Epoch [6/20], Step [285/316], Loss: 0.0461\n",
      "Epoch [6/20], Step [286/316], Loss: 0.0033\n",
      "Epoch [6/20], Step [287/316], Loss: 0.0294\n",
      "Epoch [6/20], Step [288/316], Loss: 0.0098\n",
      "Epoch [6/20], Step [289/316], Loss: 0.0068\n",
      "Epoch [6/20], Step [290/316], Loss: 0.0950\n",
      "Epoch [6/20], Step [291/316], Loss: 0.0052\n",
      "Epoch [6/20], Step [292/316], Loss: 0.0376\n",
      "Epoch [6/20], Step [293/316], Loss: 0.0025\n",
      "Epoch [6/20], Step [294/316], Loss: 0.1554\n",
      "Epoch [6/20], Step [295/316], Loss: 0.0099\n",
      "Epoch [6/20], Step [296/316], Loss: 0.1660\n",
      "Epoch [6/20], Step [297/316], Loss: 0.1671\n",
      "Epoch [6/20], Step [298/316], Loss: 0.0061\n",
      "Epoch [6/20], Step [299/316], Loss: 0.0048\n",
      "Epoch [6/20], Step [300/316], Loss: 0.0046\n",
      "Epoch [6/20], Step [301/316], Loss: 0.0023\n",
      "Epoch [6/20], Step [302/316], Loss: 0.0171\n",
      "Epoch [6/20], Step [303/316], Loss: 0.0154\n",
      "Epoch [6/20], Step [304/316], Loss: 0.0506\n",
      "Epoch [6/20], Step [305/316], Loss: 0.0078\n",
      "Epoch [6/20], Step [306/316], Loss: 0.0087\n",
      "Epoch [6/20], Step [307/316], Loss: 0.1800\n",
      "Epoch [6/20], Step [308/316], Loss: 0.0054\n",
      "Epoch [6/20], Step [309/316], Loss: 0.0054\n",
      "Epoch [6/20], Step [310/316], Loss: 0.1374\n",
      "Epoch [6/20], Step [311/316], Loss: 0.0226\n",
      "Epoch [6/20], Step [312/316], Loss: 0.3385\n",
      "Epoch [6/20], Step [313/316], Loss: 0.1022\n",
      "Epoch [6/20], Step [314/316], Loss: 0.0455\n",
      "Epoch [6/20], Step [315/316], Loss: 0.1170\n",
      "Epoch [6/20], Step [316/316], Loss: 0.0081\n",
      "Epoch [6/20], Train Loss: 0.0314\n",
      "Epoch [6/20], Validation Loss: 0.1649\n",
      "Epoch [6/20], Validation Accuracy: 0.8119\n",
      "Epoch [7/20], Step [1/316], Loss: 0.0060\n",
      "Epoch [7/20], Step [2/316], Loss: 0.0006\n",
      "Epoch [7/20], Step [3/316], Loss: 0.0050\n",
      "Epoch [7/20], Step [4/316], Loss: 0.0038\n",
      "Epoch [7/20], Step [5/316], Loss: 0.0112\n",
      "Epoch [7/20], Step [6/316], Loss: 0.0066\n",
      "Epoch [7/20], Step [7/316], Loss: 0.0026\n",
      "Epoch [7/20], Step [8/316], Loss: 0.0057\n",
      "Epoch [7/20], Step [9/316], Loss: 0.0039\n",
      "Epoch [7/20], Step [10/316], Loss: 0.0164\n",
      "Epoch [7/20], Step [11/316], Loss: 0.0047\n",
      "Epoch [7/20], Step [12/316], Loss: 0.0080\n",
      "Epoch [7/20], Step [13/316], Loss: 0.0736\n",
      "Epoch [7/20], Step [14/316], Loss: 0.0125\n",
      "Epoch [7/20], Step [15/316], Loss: 0.0382\n",
      "Epoch [7/20], Step [16/316], Loss: 0.0027\n",
      "Epoch [7/20], Step [17/316], Loss: 0.0087\n",
      "Epoch [7/20], Step [18/316], Loss: 0.0033\n",
      "Epoch [7/20], Step [19/316], Loss: 0.0615\n",
      "Epoch [7/20], Step [20/316], Loss: 0.1236\n",
      "Epoch [7/20], Step [21/316], Loss: 0.0568\n",
      "Epoch [7/20], Step [22/316], Loss: 0.0017\n",
      "Epoch [7/20], Step [23/316], Loss: 0.0888\n",
      "Epoch [7/20], Step [24/316], Loss: 0.0063\n",
      "Epoch [7/20], Step [25/316], Loss: 0.0077\n",
      "Epoch [7/20], Step [26/316], Loss: 0.0983\n",
      "Epoch [7/20], Step [27/316], Loss: 0.0037\n",
      "Epoch [7/20], Step [28/316], Loss: 0.0077\n",
      "Epoch [7/20], Step [29/316], Loss: 0.0035\n",
      "Epoch [7/20], Step [30/316], Loss: 0.0074\n",
      "Epoch [7/20], Step [31/316], Loss: 0.0010\n",
      "Epoch [7/20], Step [32/316], Loss: 0.0021\n",
      "Epoch [7/20], Step [33/316], Loss: 0.0035\n",
      "Epoch [7/20], Step [34/316], Loss: 0.0155\n",
      "Epoch [7/20], Step [35/316], Loss: 0.0035\n",
      "Epoch [7/20], Step [36/316], Loss: 0.0050\n",
      "Epoch [7/20], Step [37/316], Loss: 0.0008\n",
      "Epoch [7/20], Step [38/316], Loss: 0.0044\n",
      "Epoch [7/20], Step [39/316], Loss: 0.0994\n",
      "Epoch [7/20], Step [40/316], Loss: 0.0121\n",
      "Epoch [7/20], Step [41/316], Loss: 0.0176\n",
      "Epoch [7/20], Step [42/316], Loss: 0.0047\n",
      "Epoch [7/20], Step [43/316], Loss: 0.0056\n",
      "Epoch [7/20], Step [44/316], Loss: 0.1429\n",
      "Epoch [7/20], Step [45/316], Loss: 0.0013\n",
      "Epoch [7/20], Step [46/316], Loss: 0.0038\n",
      "Epoch [7/20], Step [47/316], Loss: 0.0086\n",
      "Epoch [7/20], Step [48/316], Loss: 0.1151\n",
      "Epoch [7/20], Step [49/316], Loss: 0.0065\n",
      "Epoch [7/20], Step [50/316], Loss: 0.0034\n",
      "Epoch [7/20], Step [51/316], Loss: 0.0287\n",
      "Epoch [7/20], Step [52/316], Loss: 0.0090\n",
      "Epoch [7/20], Step [53/316], Loss: 0.0972\n",
      "Epoch [7/20], Step [54/316], Loss: 0.0209\n",
      "Epoch [7/20], Step [55/316], Loss: 0.1192\n",
      "Epoch [7/20], Step [56/316], Loss: 0.0017\n",
      "Epoch [7/20], Step [57/316], Loss: 0.1432\n",
      "Epoch [7/20], Step [58/316], Loss: 0.0255\n",
      "Epoch [7/20], Step [59/316], Loss: 0.0056\n",
      "Epoch [7/20], Step [60/316], Loss: 0.0202\n",
      "Epoch [7/20], Step [61/316], Loss: 0.0047\n",
      "Epoch [7/20], Step [62/316], Loss: 0.0032\n",
      "Epoch [7/20], Step [63/316], Loss: 0.0567\n",
      "Epoch [7/20], Step [64/316], Loss: 0.0034\n",
      "Epoch [7/20], Step [65/316], Loss: 0.0178\n",
      "Epoch [7/20], Step [66/316], Loss: 0.0008\n",
      "Epoch [7/20], Step [67/316], Loss: 0.0035\n",
      "Epoch [7/20], Step [68/316], Loss: 0.0795\n",
      "Epoch [7/20], Step [69/316], Loss: 0.0066\n",
      "Epoch [7/20], Step [70/316], Loss: 0.0170\n",
      "Epoch [7/20], Step [71/316], Loss: 0.0034\n",
      "Epoch [7/20], Step [72/316], Loss: 0.0071\n",
      "Epoch [7/20], Step [73/316], Loss: 0.0054\n",
      "Epoch [7/20], Step [74/316], Loss: 0.0443\n",
      "Epoch [7/20], Step [75/316], Loss: 0.0120\n",
      "Epoch [7/20], Step [76/316], Loss: 0.0092\n",
      "Epoch [7/20], Step [77/316], Loss: 0.0104\n",
      "Epoch [7/20], Step [78/316], Loss: 0.0030\n",
      "Epoch [7/20], Step [79/316], Loss: 0.0035\n",
      "Epoch [7/20], Step [80/316], Loss: 0.0711\n",
      "Epoch [7/20], Step [81/316], Loss: 0.0124\n",
      "Epoch [7/20], Step [82/316], Loss: 0.0033\n",
      "Epoch [7/20], Step [83/316], Loss: 0.0135\n",
      "Epoch [7/20], Step [84/316], Loss: 0.0078\n",
      "Epoch [7/20], Step [85/316], Loss: 0.0126\n",
      "Epoch [7/20], Step [86/316], Loss: 0.0027\n",
      "Epoch [7/20], Step [87/316], Loss: 0.0049\n",
      "Epoch [7/20], Step [88/316], Loss: 0.0075\n",
      "Epoch [7/20], Step [89/316], Loss: 0.0108\n",
      "Epoch [7/20], Step [90/316], Loss: 0.0527\n",
      "Epoch [7/20], Step [91/316], Loss: 0.0146\n",
      "Epoch [7/20], Step [92/316], Loss: 0.0042\n",
      "Epoch [7/20], Step [93/316], Loss: 0.0013\n",
      "Epoch [7/20], Step [94/316], Loss: 0.0026\n",
      "Epoch [7/20], Step [95/316], Loss: 0.0151\n",
      "Epoch [7/20], Step [96/316], Loss: 0.0058\n",
      "Epoch [7/20], Step [97/316], Loss: 0.0244\n",
      "Epoch [7/20], Step [98/316], Loss: 0.0039\n",
      "Epoch [7/20], Step [99/316], Loss: 0.0082\n",
      "Epoch [7/20], Step [100/316], Loss: 0.0071\n",
      "Epoch [7/20], Step [101/316], Loss: 0.0058\n",
      "Epoch [7/20], Step [102/316], Loss: 0.0006\n",
      "Epoch [7/20], Step [103/316], Loss: 0.0400\n",
      "Epoch [7/20], Step [104/316], Loss: 0.0050\n",
      "Epoch [7/20], Step [105/316], Loss: 0.0023\n",
      "Epoch [7/20], Step [106/316], Loss: 0.0192\n",
      "Epoch [7/20], Step [107/316], Loss: 0.0210\n",
      "Epoch [7/20], Step [108/316], Loss: 0.0544\n",
      "Epoch [7/20], Step [109/316], Loss: 0.0056\n",
      "Epoch [7/20], Step [110/316], Loss: 0.0048\n",
      "Epoch [7/20], Step [111/316], Loss: 0.0048\n",
      "Epoch [7/20], Step [112/316], Loss: 0.0024\n",
      "Epoch [7/20], Step [113/316], Loss: 0.0041\n",
      "Epoch [7/20], Step [114/316], Loss: 0.0033\n",
      "Epoch [7/20], Step [115/316], Loss: 0.0003\n",
      "Epoch [7/20], Step [116/316], Loss: 0.0453\n",
      "Epoch [7/20], Step [117/316], Loss: 0.0889\n",
      "Epoch [7/20], Step [118/316], Loss: 0.0041\n",
      "Epoch [7/20], Step [119/316], Loss: 0.0001\n",
      "Epoch [7/20], Step [120/316], Loss: 0.0072\n",
      "Epoch [7/20], Step [121/316], Loss: 0.0134\n",
      "Epoch [7/20], Step [122/316], Loss: 0.0069\n",
      "Epoch [7/20], Step [123/316], Loss: 0.0036\n",
      "Epoch [7/20], Step [124/316], Loss: 0.0145\n",
      "Epoch [7/20], Step [125/316], Loss: 0.0400\n",
      "Epoch [7/20], Step [126/316], Loss: 0.0498\n",
      "Epoch [7/20], Step [127/316], Loss: 0.0093\n",
      "Epoch [7/20], Step [128/316], Loss: 0.0084\n",
      "Epoch [7/20], Step [129/316], Loss: 0.0713\n",
      "Epoch [7/20], Step [130/316], Loss: 0.0034\n",
      "Epoch [7/20], Step [131/316], Loss: 0.0026\n",
      "Epoch [7/20], Step [132/316], Loss: 0.0004\n",
      "Epoch [7/20], Step [133/316], Loss: 0.0011\n",
      "Epoch [7/20], Step [134/316], Loss: 0.0838\n",
      "Epoch [7/20], Step [135/316], Loss: 0.0422\n",
      "Epoch [7/20], Step [136/316], Loss: 0.0057\n",
      "Epoch [7/20], Step [137/316], Loss: 0.0061\n",
      "Epoch [7/20], Step [138/316], Loss: 0.0089\n",
      "Epoch [7/20], Step [139/316], Loss: 0.0014\n",
      "Epoch [7/20], Step [140/316], Loss: 0.1072\n",
      "Epoch [7/20], Step [141/316], Loss: 0.0039\n",
      "Epoch [7/20], Step [142/316], Loss: 0.0045\n",
      "Epoch [7/20], Step [143/316], Loss: 0.0082\n",
      "Epoch [7/20], Step [144/316], Loss: 0.0104\n",
      "Epoch [7/20], Step [145/316], Loss: 0.0030\n",
      "Epoch [7/20], Step [146/316], Loss: 0.0931\n",
      "Epoch [7/20], Step [147/316], Loss: 0.0082\n",
      "Epoch [7/20], Step [148/316], Loss: 0.0928\n",
      "Epoch [7/20], Step [149/316], Loss: 0.0013\n",
      "Epoch [7/20], Step [150/316], Loss: 0.0188\n",
      "Epoch [7/20], Step [151/316], Loss: 0.1496\n",
      "Epoch [7/20], Step [152/316], Loss: 0.0019\n",
      "Epoch [7/20], Step [153/316], Loss: 0.0149\n",
      "Epoch [7/20], Step [154/316], Loss: 0.0176\n",
      "Epoch [7/20], Step [155/316], Loss: 0.0083\n",
      "Epoch [7/20], Step [156/316], Loss: 0.0116\n",
      "Epoch [7/20], Step [157/316], Loss: 0.0039\n",
      "Epoch [7/20], Step [158/316], Loss: 0.0085\n",
      "Epoch [7/20], Step [159/316], Loss: 0.0114\n",
      "Epoch [7/20], Step [160/316], Loss: 0.0085\n",
      "Epoch [7/20], Step [161/316], Loss: 0.0041\n",
      "Epoch [7/20], Step [162/316], Loss: 0.0032\n",
      "Epoch [7/20], Step [163/316], Loss: 0.0695\n",
      "Epoch [7/20], Step [164/316], Loss: 0.0071\n",
      "Epoch [7/20], Step [165/316], Loss: 0.0287\n",
      "Epoch [7/20], Step [166/316], Loss: 0.0063\n",
      "Epoch [7/20], Step [167/316], Loss: 0.0028\n",
      "Epoch [7/20], Step [168/316], Loss: 0.0051\n",
      "Epoch [7/20], Step [169/316], Loss: 0.0154\n",
      "Epoch [7/20], Step [170/316], Loss: 0.0027\n",
      "Epoch [7/20], Step [171/316], Loss: 0.0655\n",
      "Epoch [7/20], Step [172/316], Loss: 0.0031\n",
      "Epoch [7/20], Step [173/316], Loss: 0.0141\n",
      "Epoch [7/20], Step [174/316], Loss: 0.0071\n",
      "Epoch [7/20], Step [175/316], Loss: 0.0094\n",
      "Epoch [7/20], Step [176/316], Loss: 0.0096\n",
      "Epoch [7/20], Step [177/316], Loss: 0.0034\n",
      "Epoch [7/20], Step [178/316], Loss: 0.0023\n",
      "Epoch [7/20], Step [179/316], Loss: 0.0010\n",
      "Epoch [7/20], Step [180/316], Loss: 0.0713\n",
      "Epoch [7/20], Step [181/316], Loss: 0.0128\n",
      "Epoch [7/20], Step [182/316], Loss: 0.0039\n",
      "Epoch [7/20], Step [183/316], Loss: 0.0042\n",
      "Epoch [7/20], Step [184/316], Loss: 0.0036\n",
      "Epoch [7/20], Step [185/316], Loss: 0.0130\n",
      "Epoch [7/20], Step [186/316], Loss: 0.0074\n",
      "Epoch [7/20], Step [187/316], Loss: 0.0022\n",
      "Epoch [7/20], Step [188/316], Loss: 0.0971\n",
      "Epoch [7/20], Step [189/316], Loss: 0.0292\n",
      "Epoch [7/20], Step [190/316], Loss: 0.0237\n",
      "Epoch [7/20], Step [191/316], Loss: 0.0015\n",
      "Epoch [7/20], Step [192/316], Loss: 0.0052\n",
      "Epoch [7/20], Step [193/316], Loss: 0.1017\n",
      "Epoch [7/20], Step [194/316], Loss: 0.0147\n",
      "Epoch [7/20], Step [195/316], Loss: 0.0011\n",
      "Epoch [7/20], Step [196/316], Loss: 0.0036\n",
      "Epoch [7/20], Step [197/316], Loss: 0.0062\n",
      "Epoch [7/20], Step [198/316], Loss: 0.0032\n",
      "Epoch [7/20], Step [199/316], Loss: 0.0108\n",
      "Epoch [7/20], Step [200/316], Loss: 0.1481\n",
      "Epoch [7/20], Step [201/316], Loss: 0.0023\n",
      "Epoch [7/20], Step [202/316], Loss: 0.0497\n",
      "Epoch [7/20], Step [203/316], Loss: 0.0008\n",
      "Epoch [7/20], Step [204/316], Loss: 0.0059\n",
      "Epoch [7/20], Step [205/316], Loss: 0.1608\n",
      "Epoch [7/20], Step [206/316], Loss: 0.0032\n",
      "Epoch [7/20], Step [207/316], Loss: 0.0039\n",
      "Epoch [7/20], Step [208/316], Loss: 0.0028\n",
      "Epoch [7/20], Step [209/316], Loss: 0.0183\n",
      "Epoch [7/20], Step [210/316], Loss: 0.0229\n",
      "Epoch [7/20], Step [211/316], Loss: 0.0022\n",
      "Epoch [7/20], Step [212/316], Loss: 0.1152\n",
      "Epoch [7/20], Step [213/316], Loss: 0.1394\n",
      "Epoch [7/20], Step [214/316], Loss: 0.0016\n",
      "Epoch [7/20], Step [215/316], Loss: 0.0016\n",
      "Epoch [7/20], Step [216/316], Loss: 0.1147\n",
      "Epoch [7/20], Step [217/316], Loss: 0.2158\n",
      "Epoch [7/20], Step [218/316], Loss: 0.0043\n",
      "Epoch [7/20], Step [219/316], Loss: 0.1092\n",
      "Epoch [7/20], Step [220/316], Loss: 0.0136\n",
      "Epoch [7/20], Step [221/316], Loss: 0.0005\n",
      "Epoch [7/20], Step [222/316], Loss: 0.0019\n",
      "Epoch [7/20], Step [223/316], Loss: 0.0184\n",
      "Epoch [7/20], Step [224/316], Loss: 0.0044\n",
      "Epoch [7/20], Step [225/316], Loss: 0.0024\n",
      "Epoch [7/20], Step [226/316], Loss: 0.0021\n",
      "Epoch [7/20], Step [227/316], Loss: 0.0045\n",
      "Epoch [7/20], Step [228/316], Loss: 0.0025\n",
      "Epoch [7/20], Step [229/316], Loss: 0.0040\n",
      "Epoch [7/20], Step [230/316], Loss: 0.0273\n",
      "Epoch [7/20], Step [231/316], Loss: 0.0048\n",
      "Epoch [7/20], Step [232/316], Loss: 0.0073\n",
      "Epoch [7/20], Step [233/316], Loss: 0.0035\n",
      "Epoch [7/20], Step [234/316], Loss: 0.0431\n",
      "Epoch [7/20], Step [235/316], Loss: 0.0020\n",
      "Epoch [7/20], Step [236/316], Loss: 0.0083\n",
      "Epoch [7/20], Step [237/316], Loss: 0.0225\n",
      "Epoch [7/20], Step [238/316], Loss: 0.0146\n",
      "Epoch [7/20], Step [239/316], Loss: 0.0101\n",
      "Epoch [7/20], Step [240/316], Loss: 0.0274\n",
      "Epoch [7/20], Step [241/316], Loss: 0.0021\n",
      "Epoch [7/20], Step [242/316], Loss: 0.0124\n",
      "Epoch [7/20], Step [243/316], Loss: 0.2091\n",
      "Epoch [7/20], Step [244/316], Loss: 0.1617\n",
      "Epoch [7/20], Step [245/316], Loss: 0.0010\n",
      "Epoch [7/20], Step [246/316], Loss: 0.0108\n",
      "Epoch [7/20], Step [247/316], Loss: 0.0020\n",
      "Epoch [7/20], Step [248/316], Loss: 0.0061\n",
      "Epoch [7/20], Step [249/316], Loss: 0.0008\n",
      "Epoch [7/20], Step [250/316], Loss: 0.1546\n",
      "Epoch [7/20], Step [251/316], Loss: 0.0937\n",
      "Epoch [7/20], Step [252/316], Loss: 0.0043\n",
      "Epoch [7/20], Step [253/316], Loss: 0.0673\n",
      "Epoch [7/20], Step [254/316], Loss: 0.0359\n",
      "Epoch [7/20], Step [255/316], Loss: 0.0053\n",
      "Epoch [7/20], Step [256/316], Loss: 0.0042\n",
      "Epoch [7/20], Step [257/316], Loss: 0.0132\n",
      "Epoch [7/20], Step [258/316], Loss: 0.0062\n",
      "Epoch [7/20], Step [259/316], Loss: 0.0060\n",
      "Epoch [7/20], Step [260/316], Loss: 0.0034\n",
      "Epoch [7/20], Step [261/316], Loss: 0.0016\n",
      "Epoch [7/20], Step [262/316], Loss: 0.1887\n",
      "Epoch [7/20], Step [263/316], Loss: 0.0090\n",
      "Epoch [7/20], Step [264/316], Loss: 0.0014\n",
      "Epoch [7/20], Step [265/316], Loss: 0.0122\n",
      "Epoch [7/20], Step [266/316], Loss: 0.0417\n",
      "Epoch [7/20], Step [267/316], Loss: 0.0220\n",
      "Epoch [7/20], Step [268/316], Loss: 0.0003\n",
      "Epoch [7/20], Step [269/316], Loss: 0.0030\n",
      "Epoch [7/20], Step [270/316], Loss: 0.0018\n",
      "Epoch [7/20], Step [271/316], Loss: 0.1632\n",
      "Epoch [7/20], Step [272/316], Loss: 0.0150\n",
      "Epoch [7/20], Step [273/316], Loss: 0.0025\n",
      "Epoch [7/20], Step [274/316], Loss: 0.0055\n",
      "Epoch [7/20], Step [275/316], Loss: 0.0045\n",
      "Epoch [7/20], Step [276/316], Loss: 0.0154\n",
      "Epoch [7/20], Step [277/316], Loss: 0.1305\n",
      "Epoch [7/20], Step [278/316], Loss: 0.0432\n",
      "Epoch [7/20], Step [279/316], Loss: 0.0038\n",
      "Epoch [7/20], Step [280/316], Loss: 0.0033\n",
      "Epoch [7/20], Step [281/316], Loss: 0.0050\n",
      "Epoch [7/20], Step [282/316], Loss: 0.0706\n",
      "Epoch [7/20], Step [283/316], Loss: 0.0083\n",
      "Epoch [7/20], Step [284/316], Loss: 0.0056\n",
      "Epoch [7/20], Step [285/316], Loss: 0.0027\n",
      "Epoch [7/20], Step [286/316], Loss: 0.0129\n",
      "Epoch [7/20], Step [287/316], Loss: 0.0045\n",
      "Epoch [7/20], Step [288/316], Loss: 0.0053\n",
      "Epoch [7/20], Step [289/316], Loss: 0.0010\n",
      "Epoch [7/20], Step [290/316], Loss: 0.0033\n",
      "Epoch [7/20], Step [291/316], Loss: 0.0128\n",
      "Epoch [7/20], Step [292/316], Loss: 0.1645\n",
      "Epoch [7/20], Step [293/316], Loss: 0.0025\n",
      "Epoch [7/20], Step [294/316], Loss: 0.0013\n",
      "Epoch [7/20], Step [295/316], Loss: 0.0422\n",
      "Epoch [7/20], Step [296/316], Loss: 0.0073\n",
      "Epoch [7/20], Step [297/316], Loss: 0.0076\n",
      "Epoch [7/20], Step [298/316], Loss: 0.0042\n",
      "Epoch [7/20], Step [299/316], Loss: 0.0033\n",
      "Epoch [7/20], Step [300/316], Loss: 0.0241\n",
      "Epoch [7/20], Step [301/316], Loss: 0.0774\n",
      "Epoch [7/20], Step [302/316], Loss: 0.0093\n",
      "Epoch [7/20], Step [303/316], Loss: 0.0050\n",
      "Epoch [7/20], Step [304/316], Loss: 0.0019\n",
      "Epoch [7/20], Step [305/316], Loss: 0.0062\n",
      "Epoch [7/20], Step [306/316], Loss: 0.1272\n",
      "Epoch [7/20], Step [307/316], Loss: 0.0113\n",
      "Epoch [7/20], Step [308/316], Loss: 0.1060\n",
      "Epoch [7/20], Step [309/316], Loss: 0.0199\n",
      "Epoch [7/20], Step [310/316], Loss: 0.0051\n",
      "Epoch [7/20], Step [311/316], Loss: 0.0113\n",
      "Epoch [7/20], Step [312/316], Loss: 0.0246\n",
      "Epoch [7/20], Step [313/316], Loss: 0.0046\n",
      "Epoch [7/20], Step [314/316], Loss: 0.0134\n",
      "Epoch [7/20], Step [315/316], Loss: 0.0076\n",
      "Epoch [7/20], Step [316/316], Loss: 0.0047\n",
      "Epoch [7/20], Train Loss: 0.0246\n",
      "Epoch [7/20], Validation Loss: 0.1657\n",
      "Epoch [7/20], Validation Accuracy: 0.8119\n",
      "Epoch [8/20], Step [1/316], Loss: 0.0187\n",
      "Epoch [8/20], Step [2/316], Loss: 0.0026\n",
      "Epoch [8/20], Step [3/316], Loss: 0.0066\n",
      "Epoch [8/20], Step [4/316], Loss: 0.0181\n",
      "Epoch [8/20], Step [5/316], Loss: 0.0152\n",
      "Epoch [8/20], Step [6/316], Loss: 0.0286\n",
      "Epoch [8/20], Step [7/316], Loss: 0.0273\n",
      "Epoch [8/20], Step [8/316], Loss: 0.1283\n",
      "Epoch [8/20], Step [9/316], Loss: 0.1532\n",
      "Epoch [8/20], Step [10/316], Loss: 0.0685\n",
      "Epoch [8/20], Step [11/316], Loss: 0.0353\n",
      "Epoch [8/20], Step [12/316], Loss: 0.0108\n",
      "Epoch [8/20], Step [13/316], Loss: 0.0001\n",
      "Epoch [8/20], Step [14/316], Loss: 0.1549\n",
      "Epoch [8/20], Step [15/316], Loss: 0.0041\n",
      "Epoch [8/20], Step [16/316], Loss: 0.0051\n",
      "Epoch [8/20], Step [17/316], Loss: 0.2515\n",
      "Epoch [8/20], Step [18/316], Loss: 0.0051\n",
      "Epoch [8/20], Step [19/316], Loss: 0.0030\n",
      "Epoch [8/20], Step [20/316], Loss: 0.0135\n",
      "Epoch [8/20], Step [21/316], Loss: 0.0040\n",
      "Epoch [8/20], Step [22/316], Loss: 0.0708\n",
      "Epoch [8/20], Step [23/316], Loss: 0.0034\n",
      "Epoch [8/20], Step [24/316], Loss: 0.0067\n",
      "Epoch [8/20], Step [25/316], Loss: 0.0031\n",
      "Epoch [8/20], Step [26/316], Loss: 0.0071\n",
      "Epoch [8/20], Step [27/316], Loss: 0.0028\n",
      "Epoch [8/20], Step [28/316], Loss: 0.0661\n",
      "Epoch [8/20], Step [29/316], Loss: 0.0238\n",
      "Epoch [8/20], Step [30/316], Loss: 0.0234\n",
      "Epoch [8/20], Step [31/316], Loss: 0.0033\n",
      "Epoch [8/20], Step [32/316], Loss: 0.0018\n",
      "Epoch [8/20], Step [33/316], Loss: 0.0017\n",
      "Epoch [8/20], Step [34/316], Loss: 0.0061\n",
      "Epoch [8/20], Step [35/316], Loss: 0.0117\n",
      "Epoch [8/20], Step [36/316], Loss: 0.1740\n",
      "Epoch [8/20], Step [37/316], Loss: 0.0019\n",
      "Epoch [8/20], Step [38/316], Loss: 0.0018\n",
      "Epoch [8/20], Step [39/316], Loss: 0.0028\n",
      "Epoch [8/20], Step [40/316], Loss: 0.0040\n",
      "Epoch [8/20], Step [41/316], Loss: 0.0565\n",
      "Epoch [8/20], Step [42/316], Loss: 0.0039\n",
      "Epoch [8/20], Step [43/316], Loss: 0.0244\n",
      "Epoch [8/20], Step [44/316], Loss: 0.0162\n",
      "Epoch [8/20], Step [45/316], Loss: 0.0761\n",
      "Epoch [8/20], Step [46/316], Loss: 0.0060\n",
      "Epoch [8/20], Step [47/316], Loss: 0.0046\n",
      "Epoch [8/20], Step [48/316], Loss: 0.0039\n",
      "Epoch [8/20], Step [49/316], Loss: 0.1504\n",
      "Epoch [8/20], Step [50/316], Loss: 0.0034\n",
      "Epoch [8/20], Step [51/316], Loss: 0.0101\n",
      "Epoch [8/20], Step [52/316], Loss: 0.0019\n",
      "Epoch [8/20], Step [53/316], Loss: 0.0065\n",
      "Epoch [8/20], Step [54/316], Loss: 0.0535\n",
      "Epoch [8/20], Step [55/316], Loss: 0.0054\n",
      "Epoch [8/20], Step [56/316], Loss: 0.0021\n",
      "Epoch [8/20], Step [57/316], Loss: 0.0244\n",
      "Epoch [8/20], Step [58/316], Loss: 0.0070\n",
      "Epoch [8/20], Step [59/316], Loss: 0.0025\n",
      "Epoch [8/20], Step [60/316], Loss: 0.1553\n",
      "Epoch [8/20], Step [61/316], Loss: 0.0046\n",
      "Epoch [8/20], Step [62/316], Loss: 0.0057\n",
      "Epoch [8/20], Step [63/316], Loss: 0.0042\n",
      "Epoch [8/20], Step [64/316], Loss: 0.0085\n",
      "Epoch [8/20], Step [65/316], Loss: 0.0069\n",
      "Epoch [8/20], Step [66/316], Loss: 0.0023\n",
      "Epoch [8/20], Step [67/316], Loss: 0.0010\n",
      "Epoch [8/20], Step [68/316], Loss: 0.0077\n",
      "Epoch [8/20], Step [69/316], Loss: 0.0092\n",
      "Epoch [8/20], Step [70/316], Loss: 0.0057\n",
      "Epoch [8/20], Step [71/316], Loss: 0.0025\n",
      "Epoch [8/20], Step [72/316], Loss: 0.0060\n",
      "Epoch [8/20], Step [73/316], Loss: 0.0006\n",
      "Epoch [8/20], Step [74/316], Loss: 0.0052\n",
      "Epoch [8/20], Step [75/316], Loss: 0.0025\n",
      "Epoch [8/20], Step [76/316], Loss: 0.0125\n",
      "Epoch [8/20], Step [77/316], Loss: 0.1345\n",
      "Epoch [8/20], Step [78/316], Loss: 0.0033\n",
      "Epoch [8/20], Step [79/316], Loss: 0.0028\n",
      "Epoch [8/20], Step [80/316], Loss: 0.0425\n",
      "Epoch [8/20], Step [81/316], Loss: 0.0097\n",
      "Epoch [8/20], Step [82/316], Loss: 0.0056\n",
      "Epoch [8/20], Step [83/316], Loss: 0.1647\n",
      "Epoch [8/20], Step [84/316], Loss: 0.0025\n",
      "Epoch [8/20], Step [85/316], Loss: 0.0028\n",
      "Epoch [8/20], Step [86/316], Loss: 0.0044\n",
      "Epoch [8/20], Step [87/316], Loss: 0.0098\n",
      "Epoch [8/20], Step [88/316], Loss: 0.0021\n",
      "Epoch [8/20], Step [89/316], Loss: 0.0005\n",
      "Epoch [8/20], Step [90/316], Loss: 0.0051\n",
      "Epoch [8/20], Step [91/316], Loss: 0.0067\n",
      "Epoch [8/20], Step [92/316], Loss: 0.0071\n",
      "Epoch [8/20], Step [93/316], Loss: 0.2078\n",
      "Epoch [8/20], Step [94/316], Loss: 0.0036\n",
      "Epoch [8/20], Step [95/316], Loss: 0.0046\n",
      "Epoch [8/20], Step [96/316], Loss: 0.0630\n",
      "Epoch [8/20], Step [97/316], Loss: 0.0083\n",
      "Epoch [8/20], Step [98/316], Loss: 0.0017\n",
      "Epoch [8/20], Step [99/316], Loss: 0.1379\n",
      "Epoch [8/20], Step [100/316], Loss: 0.0064\n",
      "Epoch [8/20], Step [101/316], Loss: 0.0043\n",
      "Epoch [8/20], Step [102/316], Loss: 0.0074\n",
      "Epoch [8/20], Step [103/316], Loss: 0.0064\n",
      "Epoch [8/20], Step [104/316], Loss: 0.0030\n",
      "Epoch [8/20], Step [105/316], Loss: 0.0023\n",
      "Epoch [8/20], Step [106/316], Loss: 0.3110\n",
      "Epoch [8/20], Step [107/316], Loss: 0.0023\n",
      "Epoch [8/20], Step [108/316], Loss: 0.0034\n",
      "Epoch [8/20], Step [109/316], Loss: 0.0014\n",
      "Epoch [8/20], Step [110/316], Loss: 0.1181\n",
      "Epoch [8/20], Step [111/316], Loss: 0.1637\n",
      "Epoch [8/20], Step [112/316], Loss: 0.0540\n",
      "Epoch [8/20], Step [113/316], Loss: 0.0151\n",
      "Epoch [8/20], Step [114/316], Loss: 0.0584\n",
      "Epoch [8/20], Step [115/316], Loss: 0.0064\n",
      "Epoch [8/20], Step [116/316], Loss: 0.0050\n",
      "Epoch [8/20], Step [117/316], Loss: 0.1101\n",
      "Epoch [8/20], Step [118/316], Loss: 0.0006\n",
      "Epoch [8/20], Step [119/316], Loss: 0.0035\n",
      "Epoch [8/20], Step [120/316], Loss: 0.0083\n",
      "Epoch [8/20], Step [121/316], Loss: 0.0730\n",
      "Epoch [8/20], Step [122/316], Loss: 0.0055\n",
      "Epoch [8/20], Step [123/316], Loss: 0.0044\n",
      "Epoch [8/20], Step [124/316], Loss: 0.0090\n",
      "Epoch [8/20], Step [125/316], Loss: 0.0029\n",
      "Epoch [8/20], Step [126/316], Loss: 0.0285\n",
      "Epoch [8/20], Step [127/316], Loss: 0.0027\n",
      "Epoch [8/20], Step [128/316], Loss: 0.1110\n",
      "Epoch [8/20], Step [129/316], Loss: 0.0033\n",
      "Epoch [8/20], Step [130/316], Loss: 0.0079\n",
      "Epoch [8/20], Step [131/316], Loss: 0.0028\n",
      "Epoch [8/20], Step [132/316], Loss: 0.0397\n",
      "Epoch [8/20], Step [133/316], Loss: 0.0049\n",
      "Epoch [8/20], Step [134/316], Loss: 0.0048\n",
      "Epoch [8/20], Step [135/316], Loss: 0.0576\n",
      "Epoch [8/20], Step [136/316], Loss: 0.0081\n",
      "Epoch [8/20], Step [137/316], Loss: 0.0037\n",
      "Epoch [8/20], Step [138/316], Loss: 0.0039\n",
      "Epoch [8/20], Step [139/316], Loss: 0.0141\n",
      "Epoch [8/20], Step [140/316], Loss: 0.0025\n",
      "Epoch [8/20], Step [141/316], Loss: 0.0937\n",
      "Epoch [8/20], Step [142/316], Loss: 0.0062\n",
      "Epoch [8/20], Step [143/316], Loss: 0.0027\n",
      "Epoch [8/20], Step [144/316], Loss: 0.0089\n",
      "Epoch [8/20], Step [145/316], Loss: 0.0080\n",
      "Epoch [8/20], Step [146/316], Loss: 0.0013\n",
      "Epoch [8/20], Step [147/316], Loss: 0.0050\n",
      "Epoch [8/20], Step [148/316], Loss: 0.0056\n",
      "Epoch [8/20], Step [149/316], Loss: 0.0265\n",
      "Epoch [8/20], Step [150/316], Loss: 0.0856\n",
      "Epoch [8/20], Step [151/316], Loss: 0.0001\n",
      "Epoch [8/20], Step [152/316], Loss: 0.0600\n",
      "Epoch [8/20], Step [153/316], Loss: 0.0042\n",
      "Epoch [8/20], Step [154/316], Loss: 0.0046\n",
      "Epoch [8/20], Step [155/316], Loss: 0.0079\n",
      "Epoch [8/20], Step [156/316], Loss: 0.0299\n",
      "Epoch [8/20], Step [157/316], Loss: 0.0034\n",
      "Epoch [8/20], Step [158/316], Loss: 0.0098\n",
      "Epoch [8/20], Step [159/316], Loss: 0.0040\n",
      "Epoch [8/20], Step [160/316], Loss: 0.0027\n",
      "Epoch [8/20], Step [161/316], Loss: 0.0353\n",
      "Epoch [8/20], Step [162/316], Loss: 0.0061\n",
      "Epoch [8/20], Step [163/316], Loss: 0.0050\n",
      "Epoch [8/20], Step [164/316], Loss: 0.0136\n",
      "Epoch [8/20], Step [165/316], Loss: 0.1133\n",
      "Epoch [8/20], Step [166/316], Loss: 0.0068\n",
      "Epoch [8/20], Step [167/316], Loss: 0.0174\n",
      "Epoch [8/20], Step [168/316], Loss: 0.0030\n",
      "Epoch [8/20], Step [169/316], Loss: 0.0040\n",
      "Epoch [8/20], Step [170/316], Loss: 0.0022\n",
      "Epoch [8/20], Step [171/316], Loss: 0.0066\n",
      "Epoch [8/20], Step [172/316], Loss: 0.0159\n",
      "Epoch [8/20], Step [173/316], Loss: 0.0037\n",
      "Epoch [8/20], Step [174/316], Loss: 0.0236\n",
      "Epoch [8/20], Step [175/316], Loss: 0.0027\n",
      "Epoch [8/20], Step [176/316], Loss: 0.2269\n",
      "Epoch [8/20], Step [177/316], Loss: 0.0066\n",
      "Epoch [8/20], Step [178/316], Loss: 0.0021\n",
      "Epoch [8/20], Step [179/316], Loss: 0.0175\n",
      "Epoch [8/20], Step [180/316], Loss: 0.0024\n",
      "Epoch [8/20], Step [181/316], Loss: 0.0199\n",
      "Epoch [8/20], Step [182/316], Loss: 0.0044\n",
      "Epoch [8/20], Step [183/316], Loss: 0.0194\n",
      "Epoch [8/20], Step [184/316], Loss: 0.1522\n",
      "Epoch [8/20], Step [185/316], Loss: 0.0024\n",
      "Epoch [8/20], Step [186/316], Loss: 0.0848\n",
      "Epoch [8/20], Step [187/316], Loss: 0.0036\n",
      "Epoch [8/20], Step [188/316], Loss: 0.0103\n",
      "Epoch [8/20], Step [189/316], Loss: 0.1176\n",
      "Epoch [8/20], Step [190/316], Loss: 0.0044\n",
      "Epoch [8/20], Step [191/316], Loss: 0.1190\n",
      "Epoch [8/20], Step [192/316], Loss: 0.0066\n",
      "Epoch [8/20], Step [193/316], Loss: 0.0038\n",
      "Epoch [8/20], Step [194/316], Loss: 0.0044\n",
      "Epoch [8/20], Step [195/316], Loss: 0.0066\n",
      "Epoch [8/20], Step [196/316], Loss: 0.0111\n",
      "Epoch [8/20], Step [197/316], Loss: 0.0022\n",
      "Epoch [8/20], Step [198/316], Loss: 0.0510\n",
      "Epoch [8/20], Step [199/316], Loss: 0.0228\n",
      "Epoch [8/20], Step [200/316], Loss: 0.0094\n",
      "Epoch [8/20], Step [201/316], Loss: 0.0234\n",
      "Epoch [8/20], Step [202/316], Loss: 0.0057\n",
      "Epoch [8/20], Step [203/316], Loss: 0.1360\n",
      "Epoch [8/20], Step [204/316], Loss: 0.0027\n",
      "Epoch [8/20], Step [205/316], Loss: 0.0060\n",
      "Epoch [8/20], Step [206/316], Loss: 0.0030\n",
      "Epoch [8/20], Step [207/316], Loss: 0.0023\n",
      "Epoch [8/20], Step [208/316], Loss: 0.0050\n",
      "Epoch [8/20], Step [209/316], Loss: 0.0137\n",
      "Epoch [8/20], Step [210/316], Loss: 0.0034\n",
      "Epoch [8/20], Step [211/316], Loss: 0.0077\n",
      "Epoch [8/20], Step [212/316], Loss: 0.0056\n",
      "Epoch [8/20], Step [213/316], Loss: 0.0868\n",
      "Epoch [8/20], Step [214/316], Loss: 0.0037\n",
      "Epoch [8/20], Step [215/316], Loss: 0.0009\n",
      "Epoch [8/20], Step [216/316], Loss: 0.1986\n",
      "Epoch [8/20], Step [217/316], Loss: 0.0024\n",
      "Epoch [8/20], Step [218/316], Loss: 0.0064\n",
      "Epoch [8/20], Step [219/316], Loss: 0.0169\n",
      "Epoch [8/20], Step [220/316], Loss: 0.0036\n",
      "Epoch [8/20], Step [221/316], Loss: 0.0576\n",
      "Epoch [8/20], Step [222/316], Loss: 0.0021\n",
      "Epoch [8/20], Step [223/316], Loss: 0.0317\n",
      "Epoch [8/20], Step [224/316], Loss: 0.0044\n",
      "Epoch [8/20], Step [225/316], Loss: 0.0164\n",
      "Epoch [8/20], Step [226/316], Loss: 0.0007\n",
      "Epoch [8/20], Step [227/316], Loss: 0.0013\n",
      "Epoch [8/20], Step [228/316], Loss: 0.0318\n",
      "Epoch [8/20], Step [229/316], Loss: 0.0032\n",
      "Epoch [8/20], Step [230/316], Loss: 0.0063\n",
      "Epoch [8/20], Step [231/316], Loss: 0.0008\n",
      "Epoch [8/20], Step [232/316], Loss: 0.0045\n",
      "Epoch [8/20], Step [233/316], Loss: 0.0026\n",
      "Epoch [8/20], Step [234/316], Loss: 0.0013\n",
      "Epoch [8/20], Step [235/316], Loss: 0.0036\n",
      "Epoch [8/20], Step [236/316], Loss: 0.0484\n",
      "Epoch [8/20], Step [237/316], Loss: 0.0338\n",
      "Epoch [8/20], Step [238/316], Loss: 0.0021\n",
      "Epoch [8/20], Step [239/316], Loss: 0.0070\n",
      "Epoch [8/20], Step [240/316], Loss: 0.0036\n",
      "Epoch [8/20], Step [241/316], Loss: 0.0150\n",
      "Epoch [8/20], Step [242/316], Loss: 0.0253\n",
      "Epoch [8/20], Step [243/316], Loss: 0.0027\n",
      "Epoch [8/20], Step [244/316], Loss: 0.0110\n",
      "Epoch [8/20], Step [245/316], Loss: 0.0014\n",
      "Epoch [8/20], Step [246/316], Loss: 0.0057\n",
      "Epoch [8/20], Step [247/316], Loss: 0.0009\n",
      "Epoch [8/20], Step [248/316], Loss: 0.0027\n",
      "Epoch [8/20], Step [249/316], Loss: 0.0021\n",
      "Epoch [8/20], Step [250/316], Loss: 0.0017\n",
      "Epoch [8/20], Step [251/316], Loss: 0.0147\n",
      "Epoch [8/20], Step [252/316], Loss: 0.1218\n",
      "Epoch [8/20], Step [253/316], Loss: 0.1288\n",
      "Epoch [8/20], Step [254/316], Loss: 0.0008\n",
      "Epoch [8/20], Step [255/316], Loss: 0.0404\n",
      "Epoch [8/20], Step [256/316], Loss: 0.0060\n",
      "Epoch [8/20], Step [257/316], Loss: 0.0019\n",
      "Epoch [8/20], Step [258/316], Loss: 0.0666\n",
      "Epoch [8/20], Step [259/316], Loss: 0.0118\n",
      "Epoch [8/20], Step [260/316], Loss: 0.0092\n",
      "Epoch [8/20], Step [261/316], Loss: 0.0638\n",
      "Epoch [8/20], Step [262/316], Loss: 0.0125\n",
      "Epoch [8/20], Step [263/316], Loss: 0.0022\n",
      "Epoch [8/20], Step [264/316], Loss: 0.0014\n",
      "Epoch [8/20], Step [265/316], Loss: 0.0992\n",
      "Epoch [8/20], Step [266/316], Loss: 0.0104\n",
      "Epoch [8/20], Step [267/316], Loss: 0.0035\n",
      "Epoch [8/20], Step [268/316], Loss: 0.0027\n",
      "Epoch [8/20], Step [269/316], Loss: 0.0195\n",
      "Epoch [8/20], Step [270/316], Loss: 0.0054\n",
      "Epoch [8/20], Step [271/316], Loss: 0.0019\n",
      "Epoch [8/20], Step [272/316], Loss: 0.0237\n",
      "Epoch [8/20], Step [273/316], Loss: 0.0047\n",
      "Epoch [8/20], Step [274/316], Loss: 0.0456\n",
      "Epoch [8/20], Step [275/316], Loss: 0.1371\n",
      "Epoch [8/20], Step [276/316], Loss: 0.0011\n",
      "Epoch [8/20], Step [277/316], Loss: 0.0221\n",
      "Epoch [8/20], Step [278/316], Loss: 0.0075\n",
      "Epoch [8/20], Step [279/316], Loss: 0.0911\n",
      "Epoch [8/20], Step [280/316], Loss: 0.0085\n",
      "Epoch [8/20], Step [281/316], Loss: 0.0041\n",
      "Epoch [8/20], Step [282/316], Loss: 0.0271\n",
      "Epoch [8/20], Step [283/316], Loss: 0.0041\n",
      "Epoch [8/20], Step [284/316], Loss: 0.0127\n",
      "Epoch [8/20], Step [285/316], Loss: 0.0167\n",
      "Epoch [8/20], Step [286/316], Loss: 0.0032\n",
      "Epoch [8/20], Step [287/316], Loss: 0.0058\n",
      "Epoch [8/20], Step [288/316], Loss: 0.0470\n",
      "Epoch [8/20], Step [289/316], Loss: 0.0033\n",
      "Epoch [8/20], Step [290/316], Loss: 0.0308\n",
      "Epoch [8/20], Step [291/316], Loss: 0.0930\n",
      "Epoch [8/20], Step [292/316], Loss: 0.0012\n",
      "Epoch [8/20], Step [293/316], Loss: 0.0041\n",
      "Epoch [8/20], Step [294/316], Loss: 0.0030\n",
      "Epoch [8/20], Step [295/316], Loss: 0.0073\n",
      "Epoch [8/20], Step [296/316], Loss: 0.0037\n",
      "Epoch [8/20], Step [297/316], Loss: 0.0475\n",
      "Epoch [8/20], Step [298/316], Loss: 0.0073\n",
      "Epoch [8/20], Step [299/316], Loss: 0.0514\n",
      "Epoch [8/20], Step [300/316], Loss: 0.0034\n",
      "Epoch [8/20], Step [301/316], Loss: 0.0071\n",
      "Epoch [8/20], Step [302/316], Loss: 0.0037\n",
      "Epoch [8/20], Step [303/316], Loss: 0.0873\n",
      "Epoch [8/20], Step [304/316], Loss: 0.1134\n",
      "Epoch [8/20], Step [305/316], Loss: 0.0025\n",
      "Epoch [8/20], Step [306/316], Loss: 0.0020\n",
      "Epoch [8/20], Step [307/316], Loss: 0.0061\n",
      "Epoch [8/20], Step [308/316], Loss: 0.0160\n",
      "Epoch [8/20], Step [309/316], Loss: 0.0334\n",
      "Epoch [8/20], Step [310/316], Loss: 0.0056\n",
      "Epoch [8/20], Step [311/316], Loss: 0.2680\n",
      "Epoch [8/20], Step [312/316], Loss: 0.0016\n",
      "Epoch [8/20], Step [313/316], Loss: 0.0041\n",
      "Epoch [8/20], Step [314/316], Loss: 0.0332\n",
      "Epoch [8/20], Step [315/316], Loss: 0.0015\n",
      "Epoch [8/20], Step [316/316], Loss: 0.0109\n",
      "Epoch [8/20], Train Loss: 0.0270\n",
      "Epoch [8/20], Validation Loss: 0.1637\n",
      "Epoch [8/20], Validation Accuracy: 0.8110\n",
      "Epoch [9/20], Step [1/316], Loss: 0.0331\n",
      "Epoch [9/20], Step [2/316], Loss: 0.0909\n",
      "Epoch [9/20], Step [3/316], Loss: 0.0065\n",
      "Epoch [9/20], Step [4/316], Loss: 0.1401\n",
      "Epoch [9/20], Step [5/316], Loss: 0.0354\n",
      "Epoch [9/20], Step [6/316], Loss: 0.1163\n",
      "Epoch [9/20], Step [7/316], Loss: 0.0376\n",
      "Epoch [9/20], Step [8/316], Loss: 0.0128\n",
      "Epoch [9/20], Step [9/316], Loss: 0.0016\n",
      "Epoch [9/20], Step [10/316], Loss: 0.0100\n",
      "Epoch [9/20], Step [11/316], Loss: 0.0025\n",
      "Epoch [9/20], Step [12/316], Loss: 0.0226\n",
      "Epoch [9/20], Step [13/316], Loss: 0.0103\n",
      "Epoch [9/20], Step [14/316], Loss: 0.0008\n",
      "Epoch [9/20], Step [15/316], Loss: 0.0067\n",
      "Epoch [9/20], Step [16/316], Loss: 0.0085\n",
      "Epoch [9/20], Step [17/316], Loss: 0.0015\n",
      "Epoch [9/20], Step [18/316], Loss: 0.1129\n",
      "Epoch [9/20], Step [19/316], Loss: 0.0045\n",
      "Epoch [9/20], Step [20/316], Loss: 0.0038\n",
      "Epoch [9/20], Step [21/316], Loss: 0.0071\n",
      "Epoch [9/20], Step [22/316], Loss: 0.0482\n",
      "Epoch [9/20], Step [23/316], Loss: 0.0097\n",
      "Epoch [9/20], Step [24/316], Loss: 0.0030\n",
      "Epoch [9/20], Step [25/316], Loss: 0.0552\n",
      "Epoch [9/20], Step [26/316], Loss: 0.0180\n",
      "Epoch [9/20], Step [27/316], Loss: 0.0068\n",
      "Epoch [9/20], Step [28/316], Loss: 0.0172\n",
      "Epoch [9/20], Step [29/316], Loss: 0.0028\n",
      "Epoch [9/20], Step [30/316], Loss: 0.0044\n",
      "Epoch [9/20], Step [31/316], Loss: 0.0050\n",
      "Epoch [9/20], Step [32/316], Loss: 0.1021\n",
      "Epoch [9/20], Step [33/316], Loss: 0.0166\n",
      "Epoch [9/20], Step [34/316], Loss: 0.0014\n",
      "Epoch [9/20], Step [35/316], Loss: 0.0051\n",
      "Epoch [9/20], Step [36/316], Loss: 0.0013\n",
      "Epoch [9/20], Step [37/316], Loss: 0.0025\n",
      "Epoch [9/20], Step [38/316], Loss: 0.0898\n",
      "Epoch [9/20], Step [39/316], Loss: 0.0022\n",
      "Epoch [9/20], Step [40/316], Loss: 0.0843\n",
      "Epoch [9/20], Step [41/316], Loss: 0.0087\n",
      "Epoch [9/20], Step [42/316], Loss: 0.0065\n",
      "Epoch [9/20], Step [43/316], Loss: 0.0064\n",
      "Epoch [9/20], Step [44/316], Loss: 0.0706\n",
      "Epoch [9/20], Step [45/316], Loss: 0.0044\n",
      "Epoch [9/20], Step [46/316], Loss: 0.0869\n",
      "Epoch [9/20], Step [47/316], Loss: 0.0058\n",
      "Epoch [9/20], Step [48/316], Loss: 0.0036\n",
      "Epoch [9/20], Step [49/316], Loss: 0.0040\n",
      "Epoch [9/20], Step [50/316], Loss: 0.0250\n",
      "Epoch [9/20], Step [51/316], Loss: 0.0031\n",
      "Epoch [9/20], Step [52/316], Loss: 0.0268\n",
      "Epoch [9/20], Step [53/316], Loss: 0.0031\n",
      "Epoch [9/20], Step [54/316], Loss: 0.0325\n",
      "Epoch [9/20], Step [55/316], Loss: 0.0025\n",
      "Epoch [9/20], Step [56/316], Loss: 0.0159\n",
      "Epoch [9/20], Step [57/316], Loss: 0.0050\n",
      "Epoch [9/20], Step [58/316], Loss: 0.0060\n",
      "Epoch [9/20], Step [59/316], Loss: 0.0014\n",
      "Epoch [9/20], Step [60/316], Loss: 0.0179\n",
      "Epoch [9/20], Step [61/316], Loss: 0.0262\n",
      "Epoch [9/20], Step [62/316], Loss: 0.0100\n",
      "Epoch [9/20], Step [63/316], Loss: 0.0022\n",
      "Epoch [9/20], Step [64/316], Loss: 0.0081\n",
      "Epoch [9/20], Step [65/316], Loss: 0.0052\n",
      "Epoch [9/20], Step [66/316], Loss: 0.0493\n",
      "Epoch [9/20], Step [67/316], Loss: 0.0035\n",
      "Epoch [9/20], Step [68/316], Loss: 0.0061\n",
      "Epoch [9/20], Step [69/316], Loss: 0.0069\n",
      "Epoch [9/20], Step [70/316], Loss: 0.0114\n",
      "Epoch [9/20], Step [71/316], Loss: 0.1198\n",
      "Epoch [9/20], Step [72/316], Loss: 0.0679\n",
      "Epoch [9/20], Step [73/316], Loss: 0.0329\n",
      "Epoch [9/20], Step [74/316], Loss: 0.0120\n",
      "Epoch [9/20], Step [75/316], Loss: 0.0044\n",
      "Epoch [9/20], Step [76/316], Loss: 0.0082\n",
      "Epoch [9/20], Step [77/316], Loss: 0.0031\n",
      "Epoch [9/20], Step [78/316], Loss: 0.1040\n",
      "Epoch [9/20], Step [79/316], Loss: 0.0022\n",
      "Epoch [9/20], Step [80/316], Loss: 0.0023\n",
      "Epoch [9/20], Step [81/316], Loss: 0.0082\n",
      "Epoch [9/20], Step [82/316], Loss: 0.0091\n",
      "Epoch [9/20], Step [83/316], Loss: 0.0723\n",
      "Epoch [9/20], Step [84/316], Loss: 0.0740\n",
      "Epoch [9/20], Step [85/316], Loss: 0.0076\n",
      "Epoch [9/20], Step [86/316], Loss: 0.0008\n",
      "Epoch [9/20], Step [87/316], Loss: 0.0070\n",
      "Epoch [9/20], Step [88/316], Loss: 0.0239\n",
      "Epoch [9/20], Step [89/316], Loss: 0.0100\n",
      "Epoch [9/20], Step [90/316], Loss: 0.0079\n",
      "Epoch [9/20], Step [91/316], Loss: 0.0018\n",
      "Epoch [9/20], Step [92/316], Loss: 0.0053\n",
      "Epoch [9/20], Step [93/316], Loss: 0.0037\n",
      "Epoch [9/20], Step [94/316], Loss: 0.0013\n",
      "Epoch [9/20], Step [95/316], Loss: 0.0059\n",
      "Epoch [9/20], Step [96/316], Loss: 0.0035\n",
      "Epoch [9/20], Step [97/316], Loss: 0.0051\n",
      "Epoch [9/20], Step [98/316], Loss: 0.0047\n",
      "Epoch [9/20], Step [99/316], Loss: 0.0072\n",
      "Epoch [9/20], Step [100/316], Loss: 0.0040\n",
      "Epoch [9/20], Step [101/316], Loss: 0.0003\n",
      "Epoch [9/20], Step [102/316], Loss: 0.0040\n",
      "Epoch [9/20], Step [103/316], Loss: 0.0036\n",
      "Epoch [9/20], Step [104/316], Loss: 0.0057\n",
      "Epoch [9/20], Step [105/316], Loss: 0.0045\n",
      "Epoch [9/20], Step [106/316], Loss: 0.0032\n",
      "Epoch [9/20], Step [107/316], Loss: 0.0132\n",
      "Epoch [9/20], Step [108/316], Loss: 0.0013\n",
      "Epoch [9/20], Step [109/316], Loss: 0.0026\n",
      "Epoch [9/20], Step [110/316], Loss: 0.0060\n",
      "Epoch [9/20], Step [111/316], Loss: 0.0089\n",
      "Epoch [9/20], Step [112/316], Loss: 0.0153\n",
      "Epoch [9/20], Step [113/316], Loss: 0.1529\n",
      "Epoch [9/20], Step [114/316], Loss: 0.0083\n",
      "Epoch [9/20], Step [115/316], Loss: 0.0028\n",
      "Epoch [9/20], Step [116/316], Loss: 0.0017\n",
      "Epoch [9/20], Step [117/316], Loss: 0.1324\n",
      "Epoch [9/20], Step [118/316], Loss: 0.0029\n",
      "Epoch [9/20], Step [119/316], Loss: 0.0033\n",
      "Epoch [9/20], Step [120/316], Loss: 0.0074\n",
      "Epoch [9/20], Step [121/316], Loss: 0.0083\n",
      "Epoch [9/20], Step [122/316], Loss: 0.0070\n",
      "Epoch [9/20], Step [123/316], Loss: 0.2048\n",
      "Epoch [9/20], Step [124/316], Loss: 0.0026\n",
      "Epoch [9/20], Step [125/316], Loss: 0.0123\n",
      "Epoch [9/20], Step [126/316], Loss: 0.0257\n",
      "Epoch [9/20], Step [127/316], Loss: 0.0012\n",
      "Epoch [9/20], Step [128/316], Loss: 0.0016\n",
      "Epoch [9/20], Step [129/316], Loss: 0.0016\n",
      "Epoch [9/20], Step [130/316], Loss: 0.0128\n",
      "Epoch [9/20], Step [131/316], Loss: 0.0144\n",
      "Epoch [9/20], Step [132/316], Loss: 0.0033\n",
      "Epoch [9/20], Step [133/316], Loss: 0.0009\n",
      "Epoch [9/20], Step [134/316], Loss: 0.0258\n",
      "Epoch [9/20], Step [135/316], Loss: 0.0416\n",
      "Epoch [9/20], Step [136/316], Loss: 0.0029\n",
      "Epoch [9/20], Step [137/316], Loss: 0.0038\n",
      "Epoch [9/20], Step [138/316], Loss: 0.0388\n",
      "Epoch [9/20], Step [139/316], Loss: 0.0022\n",
      "Epoch [9/20], Step [140/316], Loss: 0.0008\n",
      "Epoch [9/20], Step [141/316], Loss: 0.0024\n",
      "Epoch [9/20], Step [142/316], Loss: 0.0050\n",
      "Epoch [9/20], Step [143/316], Loss: 0.0024\n",
      "Epoch [9/20], Step [144/316], Loss: 0.0066\n",
      "Epoch [9/20], Step [145/316], Loss: 0.0065\n",
      "Epoch [9/20], Step [146/316], Loss: 0.0029\n",
      "Epoch [9/20], Step [147/316], Loss: 0.0021\n",
      "Epoch [9/20], Step [148/316], Loss: 0.0314\n",
      "Epoch [9/20], Step [149/316], Loss: 0.0050\n",
      "Epoch [9/20], Step [150/316], Loss: 0.0038\n",
      "Epoch [9/20], Step [151/316], Loss: 0.0089\n",
      "Epoch [9/20], Step [152/316], Loss: 0.0025\n",
      "Epoch [9/20], Step [153/316], Loss: 0.0529\n",
      "Epoch [9/20], Step [154/316], Loss: 0.1552\n",
      "Epoch [9/20], Step [155/316], Loss: 0.0019\n",
      "Epoch [9/20], Step [156/316], Loss: 0.0129\n",
      "Epoch [9/20], Step [157/316], Loss: 0.0018\n",
      "Epoch [9/20], Step [158/316], Loss: 0.1617\n",
      "Epoch [9/20], Step [159/316], Loss: 0.0063\n",
      "Epoch [9/20], Step [160/316], Loss: 0.2082\n",
      "Epoch [9/20], Step [161/316], Loss: 0.0267\n",
      "Epoch [9/20], Step [162/316], Loss: 0.0264\n",
      "Epoch [9/20], Step [163/316], Loss: 0.0106\n",
      "Epoch [9/20], Step [164/316], Loss: 0.0096\n",
      "Epoch [9/20], Step [165/316], Loss: 0.0021\n",
      "Epoch [9/20], Step [166/316], Loss: 0.0019\n",
      "Epoch [9/20], Step [167/316], Loss: 0.0093\n",
      "Epoch [9/20], Step [168/316], Loss: 0.0031\n",
      "Epoch [9/20], Step [169/316], Loss: 0.0026\n",
      "Epoch [9/20], Step [170/316], Loss: 0.0051\n",
      "Epoch [9/20], Step [171/316], Loss: 0.0109\n",
      "Epoch [9/20], Step [172/316], Loss: 0.0077\n",
      "Epoch [9/20], Step [173/316], Loss: 0.0079\n",
      "Epoch [9/20], Step [174/316], Loss: 0.0127\n",
      "Epoch [9/20], Step [175/316], Loss: 0.0013\n",
      "Epoch [9/20], Step [176/316], Loss: 0.0197\n",
      "Epoch [9/20], Step [177/316], Loss: 0.0002\n",
      "Epoch [9/20], Step [178/316], Loss: 0.0015\n",
      "Epoch [9/20], Step [179/316], Loss: 0.2453\n",
      "Epoch [9/20], Step [180/316], Loss: 0.1001\n",
      "Epoch [9/20], Step [181/316], Loss: 0.0016\n",
      "Epoch [9/20], Step [182/316], Loss: 0.0031\n",
      "Epoch [9/20], Step [183/316], Loss: 0.0417\n",
      "Epoch [9/20], Step [184/316], Loss: 0.0217\n",
      "Epoch [9/20], Step [185/316], Loss: 0.0300\n",
      "Epoch [9/20], Step [186/316], Loss: 0.0087\n",
      "Epoch [9/20], Step [187/316], Loss: 0.0037\n",
      "Epoch [9/20], Step [188/316], Loss: 0.0082\n",
      "Epoch [9/20], Step [189/316], Loss: 0.0384\n",
      "Epoch [9/20], Step [190/316], Loss: 0.0034\n",
      "Epoch [9/20], Step [191/316], Loss: 0.0074\n",
      "Epoch [9/20], Step [192/316], Loss: 0.0060\n",
      "Epoch [9/20], Step [193/316], Loss: 0.0120\n",
      "Epoch [9/20], Step [194/316], Loss: 0.0053\n",
      "Epoch [9/20], Step [195/316], Loss: 0.0227\n",
      "Epoch [9/20], Step [196/316], Loss: 0.1816\n",
      "Epoch [9/20], Step [197/316], Loss: 0.0336\n",
      "Epoch [9/20], Step [198/316], Loss: 0.1536\n",
      "Epoch [9/20], Step [199/316], Loss: 0.0048\n",
      "Epoch [9/20], Step [200/316], Loss: 0.0067\n",
      "Epoch [9/20], Step [201/316], Loss: 0.0093\n",
      "Epoch [9/20], Step [202/316], Loss: 0.0044\n",
      "Epoch [9/20], Step [203/316], Loss: 0.0061\n",
      "Epoch [9/20], Step [204/316], Loss: 0.0040\n",
      "Epoch [9/20], Step [205/316], Loss: 0.0069\n",
      "Epoch [9/20], Step [206/316], Loss: 0.0610\n",
      "Epoch [9/20], Step [207/316], Loss: 0.0077\n",
      "Epoch [9/20], Step [208/316], Loss: 0.0812\n",
      "Epoch [9/20], Step [209/316], Loss: 0.0014\n",
      "Epoch [9/20], Step [210/316], Loss: 0.0016\n",
      "Epoch [9/20], Step [211/316], Loss: 0.0117\n",
      "Epoch [9/20], Step [212/316], Loss: 0.0433\n",
      "Epoch [9/20], Step [213/316], Loss: 0.2202\n",
      "Epoch [9/20], Step [214/316], Loss: 0.0058\n",
      "Epoch [9/20], Step [215/316], Loss: 0.0043\n",
      "Epoch [9/20], Step [216/316], Loss: 0.0027\n",
      "Epoch [9/20], Step [217/316], Loss: 0.0107\n",
      "Epoch [9/20], Step [218/316], Loss: 0.1377\n",
      "Epoch [9/20], Step [219/316], Loss: 0.0014\n",
      "Epoch [9/20], Step [220/316], Loss: 0.0059\n",
      "Epoch [9/20], Step [221/316], Loss: 0.0264\n",
      "Epoch [9/20], Step [222/316], Loss: 0.0021\n",
      "Epoch [9/20], Step [223/316], Loss: 0.0042\n",
      "Epoch [9/20], Step [224/316], Loss: 0.0043\n",
      "Epoch [9/20], Step [225/316], Loss: 0.0018\n",
      "Epoch [9/20], Step [226/316], Loss: 0.1194\n",
      "Epoch [9/20], Step [227/316], Loss: 0.0076\n",
      "Epoch [9/20], Step [228/316], Loss: 0.0014\n",
      "Epoch [9/20], Step [229/316], Loss: 0.1333\n",
      "Epoch [9/20], Step [230/316], Loss: 0.0724\n",
      "Epoch [9/20], Step [231/316], Loss: 0.0011\n",
      "Epoch [9/20], Step [232/316], Loss: 0.0019\n",
      "Epoch [9/20], Step [233/316], Loss: 0.1258\n",
      "Epoch [9/20], Step [234/316], Loss: 0.0008\n",
      "Epoch [9/20], Step [235/316], Loss: 0.0097\n",
      "Epoch [9/20], Step [236/316], Loss: 0.0110\n",
      "Epoch [9/20], Step [237/316], Loss: 0.0027\n",
      "Epoch [9/20], Step [238/316], Loss: 0.0029\n",
      "Epoch [9/20], Step [239/316], Loss: 0.0246\n",
      "Epoch [9/20], Step [240/316], Loss: 0.0985\n",
      "Epoch [9/20], Step [241/316], Loss: 0.1709\n",
      "Epoch [9/20], Step [242/316], Loss: 0.0037\n",
      "Epoch [9/20], Step [243/316], Loss: 0.0036\n",
      "Epoch [9/20], Step [244/316], Loss: 0.0016\n",
      "Epoch [9/20], Step [245/316], Loss: 0.0082\n",
      "Epoch [9/20], Step [246/316], Loss: 0.1564\n",
      "Epoch [9/20], Step [247/316], Loss: 0.1167\n",
      "Epoch [9/20], Step [248/316], Loss: 0.0085\n",
      "Epoch [9/20], Step [249/316], Loss: 0.0052\n",
      "Epoch [9/20], Step [250/316], Loss: 0.0014\n",
      "Epoch [9/20], Step [251/316], Loss: 0.0046\n",
      "Epoch [9/20], Step [252/316], Loss: 0.0265\n",
      "Epoch [9/20], Step [253/316], Loss: 0.0262\n",
      "Epoch [9/20], Step [254/316], Loss: 0.0731\n",
      "Epoch [9/20], Step [255/316], Loss: 0.0038\n",
      "Epoch [9/20], Step [256/316], Loss: 0.0030\n",
      "Epoch [9/20], Step [257/316], Loss: 0.0024\n",
      "Epoch [9/20], Step [258/316], Loss: 0.1047\n",
      "Epoch [9/20], Step [259/316], Loss: 0.0045\n",
      "Epoch [9/20], Step [260/316], Loss: 0.0057\n",
      "Epoch [9/20], Step [261/316], Loss: 0.0355\n",
      "Epoch [9/20], Step [262/316], Loss: 0.0023\n",
      "Epoch [9/20], Step [263/316], Loss: 0.0043\n",
      "Epoch [9/20], Step [264/316], Loss: 0.0150\n",
      "Epoch [9/20], Step [265/316], Loss: 0.0050\n",
      "Epoch [9/20], Step [266/316], Loss: 0.0010\n",
      "Epoch [9/20], Step [267/316], Loss: 0.0062\n",
      "Epoch [9/20], Step [268/316], Loss: 0.0035\n",
      "Epoch [9/20], Step [269/316], Loss: 0.1014\n",
      "Epoch [9/20], Step [270/316], Loss: 0.0009\n",
      "Epoch [9/20], Step [271/316], Loss: 0.0195\n",
      "Epoch [9/20], Step [272/316], Loss: 0.0036\n",
      "Epoch [9/20], Step [273/316], Loss: 0.0056\n",
      "Epoch [9/20], Step [274/316], Loss: 0.0053\n",
      "Epoch [9/20], Step [275/316], Loss: 0.0027\n",
      "Epoch [9/20], Step [276/316], Loss: 0.0058\n",
      "Epoch [9/20], Step [277/316], Loss: 0.0005\n",
      "Epoch [9/20], Step [278/316], Loss: 0.0045\n",
      "Epoch [9/20], Step [279/316], Loss: 0.0241\n",
      "Epoch [9/20], Step [280/316], Loss: 0.0019\n",
      "Epoch [9/20], Step [281/316], Loss: 0.1247\n",
      "Epoch [9/20], Step [282/316], Loss: 0.0022\n",
      "Epoch [9/20], Step [283/316], Loss: 0.0086\n",
      "Epoch [9/20], Step [284/316], Loss: 0.0047\n",
      "Epoch [9/20], Step [285/316], Loss: 0.0062\n",
      "Epoch [9/20], Step [286/316], Loss: 0.0461\n",
      "Epoch [9/20], Step [287/316], Loss: 0.0039\n",
      "Epoch [9/20], Step [288/316], Loss: 0.0013\n",
      "Epoch [9/20], Step [289/316], Loss: 0.1150\n",
      "Epoch [9/20], Step [290/316], Loss: 0.0049\n",
      "Epoch [9/20], Step [291/316], Loss: 0.0018\n",
      "Epoch [9/20], Step [292/316], Loss: 0.0052\n",
      "Epoch [9/20], Step [293/316], Loss: 0.0025\n",
      "Epoch [9/20], Step [294/316], Loss: 0.0202\n",
      "Epoch [9/20], Step [295/316], Loss: 0.0050\n",
      "Epoch [9/20], Step [296/316], Loss: 0.0131\n",
      "Epoch [9/20], Step [297/316], Loss: 0.1156\n",
      "Epoch [9/20], Step [298/316], Loss: 0.0024\n",
      "Epoch [9/20], Step [299/316], Loss: 0.0017\n",
      "Epoch [9/20], Step [300/316], Loss: 0.0021\n",
      "Epoch [9/20], Step [301/316], Loss: 0.0907\n",
      "Epoch [9/20], Step [302/316], Loss: 0.0017\n",
      "Epoch [9/20], Step [303/316], Loss: 0.0886\n",
      "Epoch [9/20], Step [304/316], Loss: 0.0048\n",
      "Epoch [9/20], Step [305/316], Loss: 0.1695\n",
      "Epoch [9/20], Step [306/316], Loss: 0.0232\n",
      "Epoch [9/20], Step [307/316], Loss: 0.0100\n",
      "Epoch [9/20], Step [308/316], Loss: 0.0165\n",
      "Epoch [9/20], Step [309/316], Loss: 0.0014\n",
      "Epoch [9/20], Step [310/316], Loss: 0.0009\n",
      "Epoch [9/20], Step [311/316], Loss: 0.0911\n",
      "Epoch [9/20], Step [312/316], Loss: 0.1328\n",
      "Epoch [9/20], Step [313/316], Loss: 0.0039\n",
      "Epoch [9/20], Step [314/316], Loss: 0.0489\n",
      "Epoch [9/20], Step [315/316], Loss: 0.0074\n",
      "Epoch [9/20], Step [316/316], Loss: 0.0015\n",
      "Epoch [9/20], Train Loss: 0.0261\n",
      "Epoch [9/20], Validation Loss: 0.1610\n",
      "Epoch [9/20], Validation Accuracy: 0.8116\n",
      "Epoch [10/20], Step [1/316], Loss: 0.0107\n",
      "Epoch [10/20], Step [2/316], Loss: 0.0011\n",
      "Epoch [10/20], Step [3/316], Loss: 0.0033\n",
      "Epoch [10/20], Step [4/316], Loss: 0.0048\n",
      "Epoch [10/20], Step [5/316], Loss: 0.0042\n",
      "Epoch [10/20], Step [6/316], Loss: 0.0030\n",
      "Epoch [10/20], Step [7/316], Loss: 0.0021\n",
      "Epoch [10/20], Step [8/316], Loss: 0.0039\n",
      "Epoch [10/20], Step [9/316], Loss: 0.0013\n",
      "Epoch [10/20], Step [10/316], Loss: 0.0024\n",
      "Epoch [10/20], Step [11/316], Loss: 0.0046\n",
      "Epoch [10/20], Step [12/316], Loss: 0.1664\n",
      "Epoch [10/20], Step [13/316], Loss: 0.0056\n",
      "Epoch [10/20], Step [14/316], Loss: 0.0051\n",
      "Epoch [10/20], Step [15/316], Loss: 0.0054\n",
      "Epoch [10/20], Step [16/316], Loss: 0.1315\n",
      "Epoch [10/20], Step [17/316], Loss: 0.1458\n",
      "Epoch [10/20], Step [18/316], Loss: 0.0030\n",
      "Epoch [10/20], Step [19/316], Loss: 0.0066\n",
      "Epoch [10/20], Step [20/316], Loss: 0.0025\n",
      "Epoch [10/20], Step [21/316], Loss: 0.0011\n",
      "Epoch [10/20], Step [22/316], Loss: 0.0162\n",
      "Epoch [10/20], Step [23/316], Loss: 0.0047\n",
      "Epoch [10/20], Step [24/316], Loss: 0.0056\n",
      "Epoch [10/20], Step [25/316], Loss: 0.0026\n",
      "Epoch [10/20], Step [26/316], Loss: 0.0071\n",
      "Epoch [10/20], Step [27/316], Loss: 0.0047\n",
      "Epoch [10/20], Step [28/316], Loss: 0.0263\n",
      "Epoch [10/20], Step [29/316], Loss: 0.1028\n",
      "Epoch [10/20], Step [30/316], Loss: 0.0319\n",
      "Epoch [10/20], Step [31/316], Loss: 0.0061\n",
      "Epoch [10/20], Step [32/316], Loss: 0.0775\n",
      "Epoch [10/20], Step [33/316], Loss: 0.0081\n",
      "Epoch [10/20], Step [34/316], Loss: 0.0007\n",
      "Epoch [10/20], Step [35/316], Loss: 0.0040\n",
      "Epoch [10/20], Step [36/316], Loss: 0.1223\n",
      "Epoch [10/20], Step [37/316], Loss: 0.0023\n",
      "Epoch [10/20], Step [38/316], Loss: 0.0547\n",
      "Epoch [10/20], Step [39/316], Loss: 0.0087\n",
      "Epoch [10/20], Step [40/316], Loss: 0.0729\n",
      "Epoch [10/20], Step [41/316], Loss: 0.0180\n",
      "Epoch [10/20], Step [42/316], Loss: 0.1836\n",
      "Epoch [10/20], Step [43/316], Loss: 0.0868\n",
      "Epoch [10/20], Step [44/316], Loss: 0.0063\n",
      "Epoch [10/20], Step [45/316], Loss: 0.3361\n",
      "Epoch [10/20], Step [46/316], Loss: 0.0076\n",
      "Epoch [10/20], Step [47/316], Loss: 0.0031\n",
      "Epoch [10/20], Step [48/316], Loss: 0.0091\n",
      "Epoch [10/20], Step [49/316], Loss: 0.0119\n",
      "Epoch [10/20], Step [50/316], Loss: 0.0032\n",
      "Epoch [10/20], Step [51/316], Loss: 0.0056\n",
      "Epoch [10/20], Step [52/316], Loss: 0.0015\n",
      "Epoch [10/20], Step [53/316], Loss: 0.0015\n",
      "Epoch [10/20], Step [54/316], Loss: 0.0103\n",
      "Epoch [10/20], Step [55/316], Loss: 0.0066\n",
      "Epoch [10/20], Step [56/316], Loss: 0.0114\n",
      "Epoch [10/20], Step [57/316], Loss: 0.0025\n",
      "Epoch [10/20], Step [58/316], Loss: 0.0067\n",
      "Epoch [10/20], Step [59/316], Loss: 0.0224\n",
      "Epoch [10/20], Step [60/316], Loss: 0.0041\n",
      "Epoch [10/20], Step [61/316], Loss: 0.0338\n",
      "Epoch [10/20], Step [62/316], Loss: 0.0026\n",
      "Epoch [10/20], Step [63/316], Loss: 0.0102\n",
      "Epoch [10/20], Step [64/316], Loss: 0.2953\n",
      "Epoch [10/20], Step [65/316], Loss: 0.0309\n",
      "Epoch [10/20], Step [66/316], Loss: 0.0066\n",
      "Epoch [10/20], Step [67/316], Loss: 0.0786\n",
      "Epoch [10/20], Step [68/316], Loss: 0.0050\n",
      "Epoch [10/20], Step [69/316], Loss: 0.0099\n",
      "Epoch [10/20], Step [70/316], Loss: 0.0228\n",
      "Epoch [10/20], Step [71/316], Loss: 0.0101\n",
      "Epoch [10/20], Step [72/316], Loss: 0.0045\n",
      "Epoch [10/20], Step [73/316], Loss: 0.0173\n",
      "Epoch [10/20], Step [74/316], Loss: 0.0029\n",
      "Epoch [10/20], Step [75/316], Loss: 0.0158\n",
      "Epoch [10/20], Step [76/316], Loss: 0.0050\n",
      "Epoch [10/20], Step [77/316], Loss: 0.0279\n",
      "Epoch [10/20], Step [78/316], Loss: 0.0183\n",
      "Epoch [10/20], Step [79/316], Loss: 0.0082\n",
      "Epoch [10/20], Step [80/316], Loss: 0.0055\n",
      "Epoch [10/20], Step [81/316], Loss: 0.0013\n",
      "Epoch [10/20], Step [82/316], Loss: 0.0065\n",
      "Epoch [10/20], Step [83/316], Loss: 0.0066\n",
      "Epoch [10/20], Step [84/316], Loss: 0.1042\n",
      "Epoch [10/20], Step [85/316], Loss: 0.0138\n",
      "Epoch [10/20], Step [86/316], Loss: 0.0051\n",
      "Epoch [10/20], Step [87/316], Loss: 0.0072\n",
      "Epoch [10/20], Step [88/316], Loss: 0.1147\n",
      "Epoch [10/20], Step [89/316], Loss: 0.1328\n",
      "Epoch [10/20], Step [90/316], Loss: 0.0247\n",
      "Epoch [10/20], Step [91/316], Loss: 0.0071\n",
      "Epoch [10/20], Step [92/316], Loss: 0.0074\n",
      "Epoch [10/20], Step [93/316], Loss: 0.0239\n",
      "Epoch [10/20], Step [94/316], Loss: 0.0039\n",
      "Epoch [10/20], Step [95/316], Loss: 0.0053\n",
      "Epoch [10/20], Step [96/316], Loss: 0.0098\n",
      "Epoch [10/20], Step [97/316], Loss: 0.0695\n",
      "Epoch [10/20], Step [98/316], Loss: 0.0024\n",
      "Epoch [10/20], Step [99/316], Loss: 0.0101\n",
      "Epoch [10/20], Step [100/316], Loss: 0.0019\n",
      "Epoch [10/20], Step [101/316], Loss: 0.0131\n",
      "Epoch [10/20], Step [102/316], Loss: 0.0033\n",
      "Epoch [10/20], Step [103/316], Loss: 0.0080\n",
      "Epoch [10/20], Step [104/316], Loss: 0.0174\n",
      "Epoch [10/20], Step [105/316], Loss: 0.0047\n",
      "Epoch [10/20], Step [106/316], Loss: 0.0018\n",
      "Epoch [10/20], Step [107/316], Loss: 0.0022\n",
      "Epoch [10/20], Step [108/316], Loss: 0.0028\n",
      "Epoch [10/20], Step [109/316], Loss: 0.0017\n",
      "Epoch [10/20], Step [110/316], Loss: 0.1392\n",
      "Epoch [10/20], Step [111/316], Loss: 0.0026\n",
      "Epoch [10/20], Step [112/316], Loss: 0.0518\n",
      "Epoch [10/20], Step [113/316], Loss: 0.0017\n",
      "Epoch [10/20], Step [114/316], Loss: 0.0025\n",
      "Epoch [10/20], Step [115/316], Loss: 0.0323\n",
      "Epoch [10/20], Step [116/316], Loss: 0.0032\n",
      "Epoch [10/20], Step [117/316], Loss: 0.1695\n",
      "Epoch [10/20], Step [118/316], Loss: 0.1359\n",
      "Epoch [10/20], Step [119/316], Loss: 0.0039\n",
      "Epoch [10/20], Step [120/316], Loss: 0.0001\n",
      "Epoch [10/20], Step [121/316], Loss: 0.0135\n",
      "Epoch [10/20], Step [122/316], Loss: 0.0093\n",
      "Epoch [10/20], Step [123/316], Loss: 0.0053\n",
      "Epoch [10/20], Step [124/316], Loss: 0.0031\n",
      "Epoch [10/20], Step [125/316], Loss: 0.0040\n",
      "Epoch [10/20], Step [126/316], Loss: 0.0140\n",
      "Epoch [10/20], Step [127/316], Loss: 0.0098\n",
      "Epoch [10/20], Step [128/316], Loss: 0.0094\n",
      "Epoch [10/20], Step [129/316], Loss: 0.1699\n",
      "Epoch [10/20], Step [130/316], Loss: 0.0064\n",
      "Epoch [10/20], Step [131/316], Loss: 0.0033\n",
      "Epoch [10/20], Step [132/316], Loss: 0.1082\n",
      "Epoch [10/20], Step [133/316], Loss: 0.1285\n",
      "Epoch [10/20], Step [134/316], Loss: 0.0099\n",
      "Epoch [10/20], Step [135/316], Loss: 0.0043\n",
      "Epoch [10/20], Step [136/316], Loss: 0.2596\n",
      "Epoch [10/20], Step [137/316], Loss: 0.0044\n",
      "Epoch [10/20], Step [138/316], Loss: 0.0083\n",
      "Epoch [10/20], Step [139/316], Loss: 0.0570\n",
      "Epoch [10/20], Step [140/316], Loss: 0.0094\n",
      "Epoch [10/20], Step [141/316], Loss: 0.0018\n",
      "Epoch [10/20], Step [142/316], Loss: 0.0024\n",
      "Epoch [10/20], Step [143/316], Loss: 0.0104\n",
      "Epoch [10/20], Step [144/316], Loss: 0.0043\n",
      "Epoch [10/20], Step [145/316], Loss: 0.0054\n",
      "Epoch [10/20], Step [146/316], Loss: 0.0069\n",
      "Epoch [10/20], Step [147/316], Loss: 0.0527\n",
      "Epoch [10/20], Step [148/316], Loss: 0.0166\n",
      "Epoch [10/20], Step [149/316], Loss: 0.1149\n",
      "Epoch [10/20], Step [150/316], Loss: 0.0075\n",
      "Epoch [10/20], Step [151/316], Loss: 0.0033\n",
      "Epoch [10/20], Step [152/316], Loss: 0.1010\n",
      "Epoch [10/20], Step [153/316], Loss: 0.0021\n",
      "Epoch [10/20], Step [154/316], Loss: 0.1173\n",
      "Epoch [10/20], Step [155/316], Loss: 0.0126\n",
      "Epoch [10/20], Step [156/316], Loss: 0.0019\n",
      "Epoch [10/20], Step [157/316], Loss: 0.0633\n",
      "Epoch [10/20], Step [158/316], Loss: 0.0134\n",
      "Epoch [10/20], Step [159/316], Loss: 0.0019\n",
      "Epoch [10/20], Step [160/316], Loss: 0.0513\n",
      "Epoch [10/20], Step [161/316], Loss: 0.0157\n",
      "Epoch [10/20], Step [162/316], Loss: 0.0595\n",
      "Epoch [10/20], Step [163/316], Loss: 0.0088\n",
      "Epoch [10/20], Step [164/316], Loss: 0.0017\n",
      "Epoch [10/20], Step [165/316], Loss: 0.0058\n",
      "Epoch [10/20], Step [166/316], Loss: 0.0090\n",
      "Epoch [10/20], Step [167/316], Loss: 0.1287\n",
      "Epoch [10/20], Step [168/316], Loss: 0.0359\n",
      "Epoch [10/20], Step [169/316], Loss: 0.0010\n",
      "Epoch [10/20], Step [170/316], Loss: 0.0622\n",
      "Epoch [10/20], Step [171/316], Loss: 0.0891\n",
      "Epoch [10/20], Step [172/316], Loss: 0.0033\n",
      "Epoch [10/20], Step [173/316], Loss: 0.0070\n",
      "Epoch [10/20], Step [174/316], Loss: 0.0189\n",
      "Epoch [10/20], Step [175/316], Loss: 0.0914\n",
      "Epoch [10/20], Step [176/316], Loss: 0.0201\n",
      "Epoch [10/20], Step [177/316], Loss: 0.0130\n",
      "Epoch [10/20], Step [178/316], Loss: 0.0022\n",
      "Epoch [10/20], Step [179/316], Loss: 0.0335\n",
      "Epoch [10/20], Step [180/316], Loss: 0.0034\n",
      "Epoch [10/20], Step [181/316], Loss: 0.0047\n",
      "Epoch [10/20], Step [182/316], Loss: 0.0142\n",
      "Epoch [10/20], Step [183/316], Loss: 0.0091\n",
      "Epoch [10/20], Step [184/316], Loss: 0.0035\n",
      "Epoch [10/20], Step [185/316], Loss: 0.0087\n",
      "Epoch [10/20], Step [186/316], Loss: 0.0884\n",
      "Epoch [10/20], Step [187/316], Loss: 0.0220\n",
      "Epoch [10/20], Step [188/316], Loss: 0.1057\n",
      "Epoch [10/20], Step [189/316], Loss: 0.0855\n",
      "Epoch [10/20], Step [190/316], Loss: 0.1164\n",
      "Epoch [10/20], Step [191/316], Loss: 0.0024\n",
      "Epoch [10/20], Step [192/316], Loss: 0.2159\n",
      "Epoch [10/20], Step [193/316], Loss: 0.0442\n",
      "Epoch [10/20], Step [194/316], Loss: 0.0044\n",
      "Epoch [10/20], Step [195/316], Loss: 0.0018\n",
      "Epoch [10/20], Step [196/316], Loss: 0.0146\n",
      "Epoch [10/20], Step [197/316], Loss: 0.1153\n",
      "Epoch [10/20], Step [198/316], Loss: 0.0067\n",
      "Epoch [10/20], Step [199/316], Loss: 0.0126\n",
      "Epoch [10/20], Step [200/316], Loss: 0.0124\n",
      "Epoch [10/20], Step [201/316], Loss: 0.0107\n",
      "Epoch [10/20], Step [202/316], Loss: 0.0760\n",
      "Epoch [10/20], Step [203/316], Loss: 0.0082\n",
      "Epoch [10/20], Step [204/316], Loss: 0.0027\n",
      "Epoch [10/20], Step [205/316], Loss: 0.0261\n",
      "Epoch [10/20], Step [206/316], Loss: 0.0081\n",
      "Epoch [10/20], Step [207/316], Loss: 0.0126\n",
      "Epoch [10/20], Step [208/316], Loss: 0.0028\n",
      "Epoch [10/20], Step [209/316], Loss: 0.0454\n",
      "Epoch [10/20], Step [210/316], Loss: 0.0075\n",
      "Epoch [10/20], Step [211/316], Loss: 0.0032\n",
      "Epoch [10/20], Step [212/316], Loss: 0.0481\n",
      "Epoch [10/20], Step [213/316], Loss: 0.0072\n",
      "Epoch [10/20], Step [214/316], Loss: 0.0071\n",
      "Epoch [10/20], Step [215/316], Loss: 0.0083\n",
      "Epoch [10/20], Step [216/316], Loss: 0.0020\n",
      "Epoch [10/20], Step [217/316], Loss: 0.0096\n",
      "Epoch [10/20], Step [218/316], Loss: 0.0201\n",
      "Epoch [10/20], Step [219/316], Loss: 0.1611\n",
      "Epoch [10/20], Step [220/316], Loss: 0.0360\n",
      "Epoch [10/20], Step [221/316], Loss: 0.0054\n",
      "Epoch [10/20], Step [222/316], Loss: 0.0036\n",
      "Epoch [10/20], Step [223/316], Loss: 0.0879\n",
      "Epoch [10/20], Step [224/316], Loss: 0.0078\n",
      "Epoch [10/20], Step [225/316], Loss: 0.0043\n",
      "Epoch [10/20], Step [226/316], Loss: 0.0150\n",
      "Epoch [10/20], Step [227/316], Loss: 0.0050\n",
      "Epoch [10/20], Step [228/316], Loss: 0.0065\n",
      "Epoch [10/20], Step [229/316], Loss: 0.0043\n",
      "Epoch [10/20], Step [230/316], Loss: 0.0106\n",
      "Epoch [10/20], Step [231/316], Loss: 0.0059\n",
      "Epoch [10/20], Step [232/316], Loss: 0.0032\n",
      "Epoch [10/20], Step [233/316], Loss: 0.0858\n",
      "Epoch [10/20], Step [234/316], Loss: 0.1321\n",
      "Epoch [10/20], Step [235/316], Loss: 0.0159\n",
      "Epoch [10/20], Step [236/316], Loss: 0.0074\n",
      "Epoch [10/20], Step [237/316], Loss: 0.0400\n",
      "Epoch [10/20], Step [238/316], Loss: 0.0060\n",
      "Epoch [10/20], Step [239/316], Loss: 0.0689\n",
      "Epoch [10/20], Step [240/316], Loss: 0.0478\n",
      "Epoch [10/20], Step [241/316], Loss: 0.0074\n",
      "Epoch [10/20], Step [242/316], Loss: 0.0951\n",
      "Epoch [10/20], Step [243/316], Loss: 0.0051\n",
      "Epoch [10/20], Step [244/316], Loss: 0.1872\n",
      "Epoch [10/20], Step [245/316], Loss: 0.0039\n",
      "Epoch [10/20], Step [246/316], Loss: 0.0895\n",
      "Epoch [10/20], Step [247/316], Loss: 0.0305\n",
      "Epoch [10/20], Step [248/316], Loss: 0.0018\n",
      "Epoch [10/20], Step [249/316], Loss: 0.0067\n",
      "Epoch [10/20], Step [250/316], Loss: 0.0032\n",
      "Epoch [10/20], Step [251/316], Loss: 0.0075\n",
      "Epoch [10/20], Step [252/316], Loss: 0.0065\n",
      "Epoch [10/20], Step [253/316], Loss: 0.0993\n",
      "Epoch [10/20], Step [254/316], Loss: 0.1283\n",
      "Epoch [10/20], Step [255/316], Loss: 0.0040\n",
      "Epoch [10/20], Step [256/316], Loss: 0.0054\n",
      "Epoch [10/20], Step [257/316], Loss: 0.0031\n",
      "Epoch [10/20], Step [258/316], Loss: 0.0056\n",
      "Epoch [10/20], Step [259/316], Loss: 0.0323\n",
      "Epoch [10/20], Step [260/316], Loss: 0.0070\n",
      "Epoch [10/20], Step [261/316], Loss: 0.0284\n",
      "Epoch [10/20], Step [262/316], Loss: 0.0221\n",
      "Epoch [10/20], Step [263/316], Loss: 0.0103\n",
      "Epoch [10/20], Step [264/316], Loss: 0.0057\n",
      "Epoch [10/20], Step [265/316], Loss: 0.0052\n",
      "Epoch [10/20], Step [266/316], Loss: 0.0865\n",
      "Epoch [10/20], Step [267/316], Loss: 0.0341\n",
      "Epoch [10/20], Step [268/316], Loss: 0.0130\n",
      "Epoch [10/20], Step [269/316], Loss: 0.0316\n",
      "Epoch [10/20], Step [270/316], Loss: 0.0046\n",
      "Epoch [10/20], Step [271/316], Loss: 0.0052\n",
      "Epoch [10/20], Step [272/316], Loss: 0.0671\n",
      "Epoch [10/20], Step [273/316], Loss: 0.0302\n",
      "Epoch [10/20], Step [274/316], Loss: 0.0051\n",
      "Epoch [10/20], Step [275/316], Loss: 0.0043\n",
      "Epoch [10/20], Step [276/316], Loss: 0.0042\n",
      "Epoch [10/20], Step [277/316], Loss: 0.0059\n",
      "Epoch [10/20], Step [278/316], Loss: 0.0075\n",
      "Epoch [10/20], Step [279/316], Loss: 0.1198\n",
      "Epoch [10/20], Step [280/316], Loss: 0.0505\n",
      "Epoch [10/20], Step [281/316], Loss: 0.0060\n",
      "Epoch [10/20], Step [282/316], Loss: 0.0045\n",
      "Epoch [10/20], Step [283/316], Loss: 0.0027\n",
      "Epoch [10/20], Step [284/316], Loss: 0.0095\n",
      "Epoch [10/20], Step [285/316], Loss: 0.0019\n",
      "Epoch [10/20], Step [286/316], Loss: 0.0358\n",
      "Epoch [10/20], Step [287/316], Loss: 0.0056\n",
      "Epoch [10/20], Step [288/316], Loss: 0.0081\n",
      "Epoch [10/20], Step [289/316], Loss: 0.0178\n",
      "Epoch [10/20], Step [290/316], Loss: 0.0074\n",
      "Epoch [10/20], Step [291/316], Loss: 0.0022\n",
      "Epoch [10/20], Step [292/316], Loss: 0.0291\n",
      "Epoch [10/20], Step [293/316], Loss: 0.0416\n",
      "Epoch [10/20], Step [294/316], Loss: 0.0122\n",
      "Epoch [10/20], Step [295/316], Loss: 0.0015\n",
      "Epoch [10/20], Step [296/316], Loss: 0.0538\n",
      "Epoch [10/20], Step [297/316], Loss: 0.0326\n",
      "Epoch [10/20], Step [298/316], Loss: 0.0088\n",
      "Epoch [10/20], Step [299/316], Loss: 0.1394\n",
      "Epoch [10/20], Step [300/316], Loss: 0.0133\n",
      "Epoch [10/20], Step [301/316], Loss: 0.0055\n",
      "Epoch [10/20], Step [302/316], Loss: 0.0380\n",
      "Epoch [10/20], Step [303/316], Loss: 0.0081\n",
      "Epoch [10/20], Step [304/316], Loss: 0.0016\n",
      "Epoch [10/20], Step [305/316], Loss: 0.1455\n",
      "Epoch [10/20], Step [306/316], Loss: 0.0058\n",
      "Epoch [10/20], Step [307/316], Loss: 0.0082\n",
      "Epoch [10/20], Step [308/316], Loss: 0.0038\n",
      "Epoch [10/20], Step [309/316], Loss: 0.0506\n",
      "Epoch [10/20], Step [310/316], Loss: 0.0050\n",
      "Epoch [10/20], Step [311/316], Loss: 0.0021\n",
      "Epoch [10/20], Step [312/316], Loss: 0.0035\n",
      "Epoch [10/20], Step [313/316], Loss: 0.0008\n",
      "Epoch [10/20], Step [314/316], Loss: 0.0082\n",
      "Epoch [10/20], Step [315/316], Loss: 0.0044\n",
      "Epoch [10/20], Step [316/316], Loss: 0.0054\n",
      "Epoch [10/20], Train Loss: 0.0303\n",
      "Epoch [10/20], Validation Loss: 0.1631\n",
      "Epoch [10/20], Validation Accuracy: 0.8113\n",
      "Epoch [11/20], Step [1/316], Loss: 0.0091\n",
      "Epoch [11/20], Step [2/316], Loss: 0.0037\n",
      "Epoch [11/20], Step [3/316], Loss: 0.0966\n",
      "Epoch [11/20], Step [4/316], Loss: 0.0012\n",
      "Epoch [11/20], Step [5/316], Loss: 0.0010\n",
      "Epoch [11/20], Step [6/316], Loss: 0.0041\n",
      "Epoch [11/20], Step [7/316], Loss: 0.0448\n",
      "Epoch [11/20], Step [8/316], Loss: 0.0041\n",
      "Epoch [11/20], Step [9/316], Loss: 0.0163\n",
      "Epoch [11/20], Step [10/316], Loss: 0.0024\n",
      "Epoch [11/20], Step [11/316], Loss: 0.0507\n",
      "Epoch [11/20], Step [12/316], Loss: 0.0036\n",
      "Epoch [11/20], Step [13/316], Loss: 0.0107\n",
      "Epoch [11/20], Step [14/316], Loss: 0.0094\n",
      "Epoch [11/20], Step [15/316], Loss: 0.0244\n",
      "Epoch [11/20], Step [16/316], Loss: 0.0034\n",
      "Epoch [11/20], Step [17/316], Loss: 0.0152\n",
      "Epoch [11/20], Step [18/316], Loss: 0.0020\n",
      "Epoch [11/20], Step [19/316], Loss: 0.0008\n",
      "Epoch [11/20], Step [20/316], Loss: 0.0027\n",
      "Epoch [11/20], Step [21/316], Loss: 0.0080\n",
      "Epoch [11/20], Step [22/316], Loss: 0.0004\n",
      "Epoch [11/20], Step [23/316], Loss: 0.0053\n",
      "Epoch [11/20], Step [24/316], Loss: 0.0038\n",
      "Epoch [11/20], Step [25/316], Loss: 0.1111\n",
      "Epoch [11/20], Step [26/316], Loss: 0.0157\n",
      "Epoch [11/20], Step [27/316], Loss: 0.0048\n",
      "Epoch [11/20], Step [28/316], Loss: 0.0139\n",
      "Epoch [11/20], Step [29/316], Loss: 0.0012\n",
      "Epoch [11/20], Step [30/316], Loss: 0.1091\n",
      "Epoch [11/20], Step [31/316], Loss: 0.0066\n",
      "Epoch [11/20], Step [32/316], Loss: 0.0844\n",
      "Epoch [11/20], Step [33/316], Loss: 0.0746\n",
      "Epoch [11/20], Step [34/316], Loss: 0.0053\n",
      "Epoch [11/20], Step [35/316], Loss: 0.0034\n",
      "Epoch [11/20], Step [36/316], Loss: 0.0041\n",
      "Epoch [11/20], Step [37/316], Loss: 0.0016\n",
      "Epoch [11/20], Step [38/316], Loss: 0.0124\n",
      "Epoch [11/20], Step [39/316], Loss: 0.0699\n",
      "Epoch [11/20], Step [40/316], Loss: 0.0085\n",
      "Epoch [11/20], Step [41/316], Loss: 0.0092\n",
      "Epoch [11/20], Step [42/316], Loss: 0.0560\n",
      "Epoch [11/20], Step [43/316], Loss: 0.0017\n",
      "Epoch [11/20], Step [44/316], Loss: 0.0096\n",
      "Epoch [11/20], Step [45/316], Loss: 0.0027\n",
      "Epoch [11/20], Step [46/316], Loss: 0.0140\n",
      "Epoch [11/20], Step [47/316], Loss: 0.0104\n",
      "Epoch [11/20], Step [48/316], Loss: 0.0090\n",
      "Epoch [11/20], Step [49/316], Loss: 0.0036\n",
      "Epoch [11/20], Step [50/316], Loss: 0.0031\n",
      "Epoch [11/20], Step [51/316], Loss: 0.0024\n",
      "Epoch [11/20], Step [52/316], Loss: 0.0521\n",
      "Epoch [11/20], Step [53/316], Loss: 0.0032\n",
      "Epoch [11/20], Step [54/316], Loss: 0.0030\n",
      "Epoch [11/20], Step [55/316], Loss: 0.0030\n",
      "Epoch [11/20], Step [56/316], Loss: 0.0250\n",
      "Epoch [11/20], Step [57/316], Loss: 0.0069\n",
      "Epoch [11/20], Step [58/316], Loss: 0.0034\n",
      "Epoch [11/20], Step [59/316], Loss: 0.0005\n",
      "Epoch [11/20], Step [60/316], Loss: 0.0058\n",
      "Epoch [11/20], Step [61/316], Loss: 0.0036\n",
      "Epoch [11/20], Step [62/316], Loss: 0.0021\n",
      "Epoch [11/20], Step [63/316], Loss: 0.0074\n",
      "Epoch [11/20], Step [64/316], Loss: 0.0046\n",
      "Epoch [11/20], Step [65/316], Loss: 0.0013\n",
      "Epoch [11/20], Step [66/316], Loss: 0.0063\n",
      "Epoch [11/20], Step [67/316], Loss: 0.1125\n",
      "Epoch [11/20], Step [68/316], Loss: 0.0015\n",
      "Epoch [11/20], Step [69/316], Loss: 0.0350\n",
      "Epoch [11/20], Step [70/316], Loss: 0.0553\n",
      "Epoch [11/20], Step [71/316], Loss: 0.0070\n",
      "Epoch [11/20], Step [72/316], Loss: 0.0061\n",
      "Epoch [11/20], Step [73/316], Loss: 0.0027\n",
      "Epoch [11/20], Step [74/316], Loss: 0.0077\n",
      "Epoch [11/20], Step [75/316], Loss: 0.0069\n",
      "Epoch [11/20], Step [76/316], Loss: 0.0056\n",
      "Epoch [11/20], Step [77/316], Loss: 0.0057\n",
      "Epoch [11/20], Step [78/316], Loss: 0.0029\n",
      "Epoch [11/20], Step [79/316], Loss: 0.0056\n",
      "Epoch [11/20], Step [80/316], Loss: 0.0044\n",
      "Epoch [11/20], Step [81/316], Loss: 0.0148\n",
      "Epoch [11/20], Step [82/316], Loss: 0.0046\n",
      "Epoch [11/20], Step [83/316], Loss: 0.0039\n",
      "Epoch [11/20], Step [84/316], Loss: 0.0198\n",
      "Epoch [11/20], Step [85/316], Loss: 0.0034\n",
      "Epoch [11/20], Step [86/316], Loss: 0.0049\n",
      "Epoch [11/20], Step [87/316], Loss: 0.0043\n",
      "Epoch [11/20], Step [88/316], Loss: 0.0019\n",
      "Epoch [11/20], Step [89/316], Loss: 0.0023\n",
      "Epoch [11/20], Step [90/316], Loss: 0.0576\n",
      "Epoch [11/20], Step [91/316], Loss: 0.1366\n",
      "Epoch [11/20], Step [92/316], Loss: 0.0654\n",
      "Epoch [11/20], Step [93/316], Loss: 0.0028\n",
      "Epoch [11/20], Step [94/316], Loss: 0.0019\n",
      "Epoch [11/20], Step [95/316], Loss: 0.0009\n",
      "Epoch [11/20], Step [96/316], Loss: 0.0089\n",
      "Epoch [11/20], Step [97/316], Loss: 0.0011\n",
      "Epoch [11/20], Step [98/316], Loss: 0.0009\n",
      "Epoch [11/20], Step [99/316], Loss: 0.0030\n",
      "Epoch [11/20], Step [100/316], Loss: 0.0011\n",
      "Epoch [11/20], Step [101/316], Loss: 0.0023\n",
      "Epoch [11/20], Step [102/316], Loss: 0.0058\n",
      "Epoch [11/20], Step [103/316], Loss: 0.0117\n",
      "Epoch [11/20], Step [104/316], Loss: 0.0122\n",
      "Epoch [11/20], Step [105/316], Loss: 0.0958\n",
      "Epoch [11/20], Step [106/316], Loss: 0.0129\n",
      "Epoch [11/20], Step [107/316], Loss: 0.0023\n",
      "Epoch [11/20], Step [108/316], Loss: 0.0015\n",
      "Epoch [11/20], Step [109/316], Loss: 0.0066\n",
      "Epoch [11/20], Step [110/316], Loss: 0.0028\n",
      "Epoch [11/20], Step [111/316], Loss: 0.0081\n",
      "Epoch [11/20], Step [112/316], Loss: 0.0017\n",
      "Epoch [11/20], Step [113/316], Loss: 0.0028\n",
      "Epoch [11/20], Step [114/316], Loss: 0.0029\n",
      "Epoch [11/20], Step [115/316], Loss: 0.1488\n",
      "Epoch [11/20], Step [116/316], Loss: 0.0061\n",
      "Epoch [11/20], Step [117/316], Loss: 0.0035\n",
      "Epoch [11/20], Step [118/316], Loss: 0.0730\n",
      "Epoch [11/20], Step [119/316], Loss: 0.1254\n",
      "Epoch [11/20], Step [120/316], Loss: 0.0081\n",
      "Epoch [11/20], Step [121/316], Loss: 0.0039\n",
      "Epoch [11/20], Step [122/316], Loss: 0.0044\n",
      "Epoch [11/20], Step [123/316], Loss: 0.0997\n",
      "Epoch [11/20], Step [124/316], Loss: 0.0086\n",
      "Epoch [11/20], Step [125/316], Loss: 0.0056\n",
      "Epoch [11/20], Step [126/316], Loss: 0.0032\n",
      "Epoch [11/20], Step [127/316], Loss: 0.1410\n",
      "Epoch [11/20], Step [128/316], Loss: 0.0242\n",
      "Epoch [11/20], Step [129/316], Loss: 0.0033\n",
      "Epoch [11/20], Step [130/316], Loss: 0.0013\n",
      "Epoch [11/20], Step [131/316], Loss: 0.1229\n",
      "Epoch [11/20], Step [132/316], Loss: 0.0074\n",
      "Epoch [11/20], Step [133/316], Loss: 0.0177\n",
      "Epoch [11/20], Step [134/316], Loss: 0.0044\n",
      "Epoch [11/20], Step [135/316], Loss: 0.0039\n",
      "Epoch [11/20], Step [136/316], Loss: 0.0020\n",
      "Epoch [11/20], Step [137/316], Loss: 0.0018\n",
      "Epoch [11/20], Step [138/316], Loss: 0.0001\n",
      "Epoch [11/20], Step [139/316], Loss: 0.0324\n",
      "Epoch [11/20], Step [140/316], Loss: 0.0031\n",
      "Epoch [11/20], Step [141/316], Loss: 0.0385\n",
      "Epoch [11/20], Step [142/316], Loss: 0.0057\n",
      "Epoch [11/20], Step [143/316], Loss: 0.0129\n",
      "Epoch [11/20], Step [144/316], Loss: 0.0058\n",
      "Epoch [11/20], Step [145/316], Loss: 0.0011\n",
      "Epoch [11/20], Step [146/316], Loss: 0.0035\n",
      "Epoch [11/20], Step [147/316], Loss: 0.0041\n",
      "Epoch [11/20], Step [148/316], Loss: 0.0031\n",
      "Epoch [11/20], Step [149/316], Loss: 0.0334\n",
      "Epoch [11/20], Step [150/316], Loss: 0.1029\n",
      "Epoch [11/20], Step [151/316], Loss: 0.0010\n",
      "Epoch [11/20], Step [152/316], Loss: 0.0030\n",
      "Epoch [11/20], Step [153/316], Loss: 0.0107\n",
      "Epoch [11/20], Step [154/316], Loss: 0.0669\n",
      "Epoch [11/20], Step [155/316], Loss: 0.0009\n",
      "Epoch [11/20], Step [156/316], Loss: 0.0038\n",
      "Epoch [11/20], Step [157/316], Loss: 0.1670\n",
      "Epoch [11/20], Step [158/316], Loss: 0.0018\n",
      "Epoch [11/20], Step [159/316], Loss: 0.0024\n",
      "Epoch [11/20], Step [160/316], Loss: 0.1678\n",
      "Epoch [11/20], Step [161/316], Loss: 0.0068\n",
      "Epoch [11/20], Step [162/316], Loss: 0.0019\n",
      "Epoch [11/20], Step [163/316], Loss: 0.0095\n",
      "Epoch [11/20], Step [164/316], Loss: 0.0027\n",
      "Epoch [11/20], Step [165/316], Loss: 0.0181\n",
      "Epoch [11/20], Step [166/316], Loss: 0.0508\n",
      "Epoch [11/20], Step [167/316], Loss: 0.0029\n",
      "Epoch [11/20], Step [168/316], Loss: 0.0045\n",
      "Epoch [11/20], Step [169/316], Loss: 0.0050\n",
      "Epoch [11/20], Step [170/316], Loss: 0.0126\n",
      "Epoch [11/20], Step [171/316], Loss: 0.0850\n",
      "Epoch [11/20], Step [172/316], Loss: 0.0070\n",
      "Epoch [11/20], Step [173/316], Loss: 0.0024\n",
      "Epoch [11/20], Step [174/316], Loss: 0.0036\n",
      "Epoch [11/20], Step [175/316], Loss: 0.0062\n",
      "Epoch [11/20], Step [176/316], Loss: 0.0068\n",
      "Epoch [11/20], Step [177/316], Loss: 0.0135\n",
      "Epoch [11/20], Step [178/316], Loss: 0.0045\n",
      "Epoch [11/20], Step [179/316], Loss: 0.0028\n",
      "Epoch [11/20], Step [180/316], Loss: 0.0080\n",
      "Epoch [11/20], Step [181/316], Loss: 0.1551\n",
      "Epoch [11/20], Step [182/316], Loss: 0.0047\n",
      "Epoch [11/20], Step [183/316], Loss: 0.0027\n",
      "Epoch [11/20], Step [184/316], Loss: 0.0018\n",
      "Epoch [11/20], Step [185/316], Loss: 0.0052\n",
      "Epoch [11/20], Step [186/316], Loss: 0.0066\n",
      "Epoch [11/20], Step [187/316], Loss: 0.0028\n",
      "Epoch [11/20], Step [188/316], Loss: 0.1375\n",
      "Epoch [11/20], Step [189/316], Loss: 0.0019\n",
      "Epoch [11/20], Step [190/316], Loss: 0.0037\n",
      "Epoch [11/20], Step [191/316], Loss: 0.1633\n",
      "Epoch [11/20], Step [192/316], Loss: 0.0132\n",
      "Epoch [11/20], Step [193/316], Loss: 0.0461\n",
      "Epoch [11/20], Step [194/316], Loss: 0.0014\n",
      "Epoch [11/20], Step [195/316], Loss: 0.0944\n",
      "Epoch [11/20], Step [196/316], Loss: 0.0041\n",
      "Epoch [11/20], Step [197/316], Loss: 0.0026\n",
      "Epoch [11/20], Step [198/316], Loss: 0.0336\n",
      "Epoch [11/20], Step [199/316], Loss: 0.0022\n",
      "Epoch [11/20], Step [200/316], Loss: 0.0028\n",
      "Epoch [11/20], Step [201/316], Loss: 0.0031\n",
      "Epoch [11/20], Step [202/316], Loss: 0.0036\n",
      "Epoch [11/20], Step [203/316], Loss: 0.0034\n",
      "Epoch [11/20], Step [204/316], Loss: 0.0057\n",
      "Epoch [11/20], Step [205/316], Loss: 0.0021\n",
      "Epoch [11/20], Step [206/316], Loss: 0.0913\n",
      "Epoch [11/20], Step [207/316], Loss: 0.0095\n",
      "Epoch [11/20], Step [208/316], Loss: 0.0055\n",
      "Epoch [11/20], Step [209/316], Loss: 0.0317\n",
      "Epoch [11/20], Step [210/316], Loss: 0.0154\n",
      "Epoch [11/20], Step [211/316], Loss: 0.0056\n",
      "Epoch [11/20], Step [212/316], Loss: 0.0033\n",
      "Epoch [11/20], Step [213/316], Loss: 0.2567\n",
      "Epoch [11/20], Step [214/316], Loss: 0.0058\n",
      "Epoch [11/20], Step [215/316], Loss: 0.0170\n",
      "Epoch [11/20], Step [216/316], Loss: 0.0064\n",
      "Epoch [11/20], Step [217/316], Loss: 0.0704\n",
      "Epoch [11/20], Step [218/316], Loss: 0.1516\n",
      "Epoch [11/20], Step [219/316], Loss: 0.0033\n",
      "Epoch [11/20], Step [220/316], Loss: 0.0205\n",
      "Epoch [11/20], Step [221/316], Loss: 0.0974\n",
      "Epoch [11/20], Step [222/316], Loss: 0.0041\n",
      "Epoch [11/20], Step [223/316], Loss: 0.0042\n",
      "Epoch [11/20], Step [224/316], Loss: 0.0132\n",
      "Epoch [11/20], Step [225/316], Loss: 0.0018\n",
      "Epoch [11/20], Step [226/316], Loss: 0.0021\n",
      "Epoch [11/20], Step [227/316], Loss: 0.0096\n",
      "Epoch [11/20], Step [228/316], Loss: 0.0187\n",
      "Epoch [11/20], Step [229/316], Loss: 0.1275\n",
      "Epoch [11/20], Step [230/316], Loss: 0.0036\n",
      "Epoch [11/20], Step [231/316], Loss: 0.0061\n",
      "Epoch [11/20], Step [232/316], Loss: 0.0044\n",
      "Epoch [11/20], Step [233/316], Loss: 0.0449\n",
      "Epoch [11/20], Step [234/316], Loss: 0.0214\n",
      "Epoch [11/20], Step [235/316], Loss: 0.0239\n",
      "Epoch [11/20], Step [236/316], Loss: 0.0040\n",
      "Epoch [11/20], Step [237/316], Loss: 0.2089\n",
      "Epoch [11/20], Step [238/316], Loss: 0.0044\n",
      "Epoch [11/20], Step [239/316], Loss: 0.0078\n",
      "Epoch [11/20], Step [240/316], Loss: 0.0436\n",
      "Epoch [11/20], Step [241/316], Loss: 0.0109\n",
      "Epoch [11/20], Step [242/316], Loss: 0.0048\n",
      "Epoch [11/20], Step [243/316], Loss: 0.0188\n",
      "Epoch [11/20], Step [244/316], Loss: 0.1950\n",
      "Epoch [11/20], Step [245/316], Loss: 0.1420\n",
      "Epoch [11/20], Step [246/316], Loss: 0.0023\n",
      "Epoch [11/20], Step [247/316], Loss: 0.0073\n",
      "Epoch [11/20], Step [248/316], Loss: 0.0072\n",
      "Epoch [11/20], Step [249/316], Loss: 0.0133\n",
      "Epoch [11/20], Step [250/316], Loss: 0.0075\n",
      "Epoch [11/20], Step [251/316], Loss: 0.0041\n",
      "Epoch [11/20], Step [252/316], Loss: 0.0192\n",
      "Epoch [11/20], Step [253/316], Loss: 0.0174\n",
      "Epoch [11/20], Step [254/316], Loss: 0.0088\n",
      "Epoch [11/20], Step [255/316], Loss: 0.0050\n",
      "Epoch [11/20], Step [256/316], Loss: 0.0930\n",
      "Epoch [11/20], Step [257/316], Loss: 0.0937\n",
      "Epoch [11/20], Step [258/316], Loss: 0.0285\n",
      "Epoch [11/20], Step [259/316], Loss: 0.0039\n",
      "Epoch [11/20], Step [260/316], Loss: 0.0069\n",
      "Epoch [11/20], Step [261/316], Loss: 0.0032\n",
      "Epoch [11/20], Step [262/316], Loss: 0.0013\n",
      "Epoch [11/20], Step [263/316], Loss: 0.0910\n",
      "Epoch [11/20], Step [264/316], Loss: 0.0768\n",
      "Epoch [11/20], Step [265/316], Loss: 0.0043\n",
      "Epoch [11/20], Step [266/316], Loss: 0.0027\n",
      "Epoch [11/20], Step [267/316], Loss: 0.0033\n",
      "Epoch [11/20], Step [268/316], Loss: 0.0101\n",
      "Epoch [11/20], Step [269/316], Loss: 0.0117\n",
      "Epoch [11/20], Step [270/316], Loss: 0.0017\n",
      "Epoch [11/20], Step [271/316], Loss: 0.0140\n",
      "Epoch [11/20], Step [272/316], Loss: 0.1393\n",
      "Epoch [11/20], Step [273/316], Loss: 0.0123\n",
      "Epoch [11/20], Step [274/316], Loss: 0.0073\n",
      "Epoch [11/20], Step [275/316], Loss: 0.0022\n",
      "Epoch [11/20], Step [276/316], Loss: 0.0104\n",
      "Epoch [11/20], Step [277/316], Loss: 0.0030\n",
      "Epoch [11/20], Step [278/316], Loss: 0.0022\n",
      "Epoch [11/20], Step [279/316], Loss: 0.0032\n",
      "Epoch [11/20], Step [280/316], Loss: 0.0932\n",
      "Epoch [11/20], Step [281/316], Loss: 0.0012\n",
      "Epoch [11/20], Step [282/316], Loss: 0.0061\n",
      "Epoch [11/20], Step [283/316], Loss: 0.0032\n",
      "Epoch [11/20], Step [284/316], Loss: 0.0022\n",
      "Epoch [11/20], Step [285/316], Loss: 0.0064\n",
      "Epoch [11/20], Step [286/316], Loss: 0.0047\n",
      "Epoch [11/20], Step [287/316], Loss: 0.0062\n",
      "Epoch [11/20], Step [288/316], Loss: 0.0128\n",
      "Epoch [11/20], Step [289/316], Loss: 0.0972\n",
      "Epoch [11/20], Step [290/316], Loss: 0.0049\n",
      "Epoch [11/20], Step [291/316], Loss: 0.0349\n",
      "Epoch [11/20], Step [292/316], Loss: 0.0021\n",
      "Epoch [11/20], Step [293/316], Loss: 0.0018\n",
      "Epoch [11/20], Step [294/316], Loss: 0.0373\n",
      "Epoch [11/20], Step [295/316], Loss: 0.0052\n",
      "Epoch [11/20], Step [296/316], Loss: 0.0047\n",
      "Epoch [11/20], Step [297/316], Loss: 0.0023\n",
      "Epoch [11/20], Step [298/316], Loss: 0.0373\n",
      "Epoch [11/20], Step [299/316], Loss: 0.0018\n",
      "Epoch [11/20], Step [300/316], Loss: 0.1297\n",
      "Epoch [11/20], Step [301/316], Loss: 0.0015\n",
      "Epoch [11/20], Step [302/316], Loss: 0.0031\n",
      "Epoch [11/20], Step [303/316], Loss: 0.0027\n",
      "Epoch [11/20], Step [304/316], Loss: 0.2011\n",
      "Epoch [11/20], Step [305/316], Loss: 0.1932\n",
      "Epoch [11/20], Step [306/316], Loss: 0.0027\n",
      "Epoch [11/20], Step [307/316], Loss: 0.0063\n",
      "Epoch [11/20], Step [308/316], Loss: 0.0012\n",
      "Epoch [11/20], Step [309/316], Loss: 0.0267\n",
      "Epoch [11/20], Step [310/316], Loss: 0.0004\n",
      "Epoch [11/20], Step [311/316], Loss: 0.0174\n",
      "Epoch [11/20], Step [312/316], Loss: 0.0086\n",
      "Epoch [11/20], Step [313/316], Loss: 0.0041\n",
      "Epoch [11/20], Step [314/316], Loss: 0.0264\n",
      "Epoch [11/20], Step [315/316], Loss: 0.0044\n",
      "Epoch [11/20], Step [316/316], Loss: 0.0286\n",
      "Epoch [11/20], Train Loss: 0.0246\n",
      "Epoch [11/20], Validation Loss: 0.1653\n",
      "Epoch [11/20], Validation Accuracy: 0.8139\n",
      "Epoch [12/20], Step [1/316], Loss: 0.0071\n",
      "Epoch [12/20], Step [2/316], Loss: 0.0011\n",
      "Epoch [12/20], Step [3/316], Loss: 0.0024\n",
      "Epoch [12/20], Step [4/316], Loss: 0.0070\n",
      "Epoch [12/20], Step [5/316], Loss: 0.0155\n",
      "Epoch [12/20], Step [6/316], Loss: 0.0013\n",
      "Epoch [12/20], Step [7/316], Loss: 0.0049\n",
      "Epoch [12/20], Step [8/316], Loss: 0.0007\n",
      "Epoch [12/20], Step [9/316], Loss: 0.0020\n",
      "Epoch [12/20], Step [10/316], Loss: 0.0084\n",
      "Epoch [12/20], Step [11/316], Loss: 0.1190\n",
      "Epoch [12/20], Step [12/316], Loss: 0.1879\n",
      "Epoch [12/20], Step [13/316], Loss: 0.0033\n",
      "Epoch [12/20], Step [14/316], Loss: 0.0039\n",
      "Epoch [12/20], Step [15/316], Loss: 0.0588\n",
      "Epoch [12/20], Step [16/316], Loss: 0.0160\n",
      "Epoch [12/20], Step [17/316], Loss: 0.0510\n",
      "Epoch [12/20], Step [18/316], Loss: 0.0045\n",
      "Epoch [12/20], Step [19/316], Loss: 0.1118\n",
      "Epoch [12/20], Step [20/316], Loss: 0.0012\n",
      "Epoch [12/20], Step [21/316], Loss: 0.0826\n",
      "Epoch [12/20], Step [22/316], Loss: 0.0818\n",
      "Epoch [12/20], Step [23/316], Loss: 0.0041\n",
      "Epoch [12/20], Step [24/316], Loss: 0.0039\n",
      "Epoch [12/20], Step [25/316], Loss: 0.0077\n",
      "Epoch [12/20], Step [26/316], Loss: 0.0134\n",
      "Epoch [12/20], Step [27/316], Loss: 0.0631\n",
      "Epoch [12/20], Step [28/316], Loss: 0.0634\n",
      "Epoch [12/20], Step [29/316], Loss: 0.0065\n",
      "Epoch [12/20], Step [30/316], Loss: 0.0084\n",
      "Epoch [12/20], Step [31/316], Loss: 0.0125\n",
      "Epoch [12/20], Step [32/316], Loss: 0.0044\n",
      "Epoch [12/20], Step [33/316], Loss: 0.1462\n",
      "Epoch [12/20], Step [34/316], Loss: 0.0299\n",
      "Epoch [12/20], Step [35/316], Loss: 0.0838\n",
      "Epoch [12/20], Step [36/316], Loss: 0.0059\n",
      "Epoch [12/20], Step [37/316], Loss: 0.0113\n",
      "Epoch [12/20], Step [38/316], Loss: 0.0089\n",
      "Epoch [12/20], Step [39/316], Loss: 0.0084\n",
      "Epoch [12/20], Step [40/316], Loss: 0.0691\n",
      "Epoch [12/20], Step [41/316], Loss: 0.0275\n",
      "Epoch [12/20], Step [42/316], Loss: 0.0011\n",
      "Epoch [12/20], Step [43/316], Loss: 0.0052\n",
      "Epoch [12/20], Step [44/316], Loss: 0.0073\n",
      "Epoch [12/20], Step [45/316], Loss: 0.0030\n",
      "Epoch [12/20], Step [46/316], Loss: 0.0081\n",
      "Epoch [12/20], Step [47/316], Loss: 0.0127\n",
      "Epoch [12/20], Step [48/316], Loss: 0.0051\n",
      "Epoch [12/20], Step [49/316], Loss: 0.0082\n",
      "Epoch [12/20], Step [50/316], Loss: 0.0051\n",
      "Epoch [12/20], Step [51/316], Loss: 0.0048\n",
      "Epoch [12/20], Step [52/316], Loss: 0.0052\n",
      "Epoch [12/20], Step [53/316], Loss: 0.0073\n",
      "Epoch [12/20], Step [54/316], Loss: 0.0095\n",
      "Epoch [12/20], Step [55/316], Loss: 0.0077\n",
      "Epoch [12/20], Step [56/316], Loss: 0.0184\n",
      "Epoch [12/20], Step [57/316], Loss: 0.0676\n",
      "Epoch [12/20], Step [58/316], Loss: 0.2594\n",
      "Epoch [12/20], Step [59/316], Loss: 0.0095\n",
      "Epoch [12/20], Step [60/316], Loss: 0.0165\n",
      "Epoch [12/20], Step [61/316], Loss: 0.1548\n",
      "Epoch [12/20], Step [62/316], Loss: 0.0943\n",
      "Epoch [12/20], Step [63/316], Loss: 0.0031\n",
      "Epoch [12/20], Step [64/316], Loss: 0.0074\n",
      "Epoch [12/20], Step [65/316], Loss: 0.0080\n",
      "Epoch [12/20], Step [66/316], Loss: 0.0513\n",
      "Epoch [12/20], Step [67/316], Loss: 0.0901\n",
      "Epoch [12/20], Step [68/316], Loss: 0.0040\n",
      "Epoch [12/20], Step [69/316], Loss: 0.0126\n",
      "Epoch [12/20], Step [70/316], Loss: 0.0253\n",
      "Epoch [12/20], Step [71/316], Loss: 0.0152\n",
      "Epoch [12/20], Step [72/316], Loss: 0.0076\n",
      "Epoch [12/20], Step [73/316], Loss: 0.2560\n",
      "Epoch [12/20], Step [74/316], Loss: 0.0051\n",
      "Epoch [12/20], Step [75/316], Loss: 0.0134\n",
      "Epoch [12/20], Step [76/316], Loss: 0.0035\n",
      "Epoch [12/20], Step [77/316], Loss: 0.0120\n",
      "Epoch [12/20], Step [78/316], Loss: 0.0017\n",
      "Epoch [12/20], Step [79/316], Loss: 0.0679\n",
      "Epoch [12/20], Step [80/316], Loss: 0.0014\n",
      "Epoch [12/20], Step [81/316], Loss: 0.0036\n",
      "Epoch [12/20], Step [82/316], Loss: 0.0041\n",
      "Epoch [12/20], Step [83/316], Loss: 0.0034\n",
      "Epoch [12/20], Step [84/316], Loss: 0.0139\n",
      "Epoch [12/20], Step [85/316], Loss: 0.0617\n",
      "Epoch [12/20], Step [86/316], Loss: 0.0042\n",
      "Epoch [12/20], Step [87/316], Loss: 0.0925\n",
      "Epoch [12/20], Step [88/316], Loss: 0.1208\n",
      "Epoch [12/20], Step [89/316], Loss: 0.0030\n",
      "Epoch [12/20], Step [90/316], Loss: 0.1250\n",
      "Epoch [12/20], Step [91/316], Loss: 0.0147\n",
      "Epoch [12/20], Step [92/316], Loss: 0.1190\n",
      "Epoch [12/20], Step [93/316], Loss: 0.0056\n",
      "Epoch [12/20], Step [94/316], Loss: 0.0222\n",
      "Epoch [12/20], Step [95/316], Loss: 0.0066\n",
      "Epoch [12/20], Step [96/316], Loss: 0.0069\n",
      "Epoch [12/20], Step [97/316], Loss: 0.0069\n",
      "Epoch [12/20], Step [98/316], Loss: 0.0078\n",
      "Epoch [12/20], Step [99/316], Loss: 0.0042\n",
      "Epoch [12/20], Step [100/316], Loss: 0.0019\n",
      "Epoch [12/20], Step [101/316], Loss: 0.0145\n",
      "Epoch [12/20], Step [102/316], Loss: 0.0041\n",
      "Epoch [12/20], Step [103/316], Loss: 0.0069\n",
      "Epoch [12/20], Step [104/316], Loss: 0.0105\n",
      "Epoch [12/20], Step [105/316], Loss: 0.1372\n",
      "Epoch [12/20], Step [106/316], Loss: 0.0084\n",
      "Epoch [12/20], Step [107/316], Loss: 0.0134\n",
      "Epoch [12/20], Step [108/316], Loss: 0.0987\n",
      "Epoch [12/20], Step [109/316], Loss: 0.0125\n",
      "Epoch [12/20], Step [110/316], Loss: 0.0096\n",
      "Epoch [12/20], Step [111/316], Loss: 0.0706\n",
      "Epoch [12/20], Step [112/316], Loss: 0.0059\n",
      "Epoch [12/20], Step [113/316], Loss: 0.0572\n",
      "Epoch [12/20], Step [114/316], Loss: 0.0071\n",
      "Epoch [12/20], Step [115/316], Loss: 0.0200\n",
      "Epoch [12/20], Step [116/316], Loss: 0.0084\n",
      "Epoch [12/20], Step [117/316], Loss: 0.0288\n",
      "Epoch [12/20], Step [118/316], Loss: 0.0928\n",
      "Epoch [12/20], Step [119/316], Loss: 0.0041\n",
      "Epoch [12/20], Step [120/316], Loss: 0.0096\n",
      "Epoch [12/20], Step [121/316], Loss: 0.0181\n",
      "Epoch [12/20], Step [122/316], Loss: 0.0079\n",
      "Epoch [12/20], Step [123/316], Loss: 0.0060\n",
      "Epoch [12/20], Step [124/316], Loss: 0.0039\n",
      "Epoch [12/20], Step [125/316], Loss: 0.0040\n",
      "Epoch [12/20], Step [126/316], Loss: 0.0069\n",
      "Epoch [12/20], Step [127/316], Loss: 0.0022\n",
      "Epoch [12/20], Step [128/316], Loss: 0.0939\n",
      "Epoch [12/20], Step [129/316], Loss: 0.0112\n",
      "Epoch [12/20], Step [130/316], Loss: 0.0018\n",
      "Epoch [12/20], Step [131/316], Loss: 0.0054\n",
      "Epoch [12/20], Step [132/316], Loss: 0.0092\n",
      "Epoch [12/20], Step [133/316], Loss: 0.0097\n",
      "Epoch [12/20], Step [134/316], Loss: 0.0070\n",
      "Epoch [12/20], Step [135/316], Loss: 0.0777\n",
      "Epoch [12/20], Step [136/316], Loss: 0.0102\n",
      "Epoch [12/20], Step [137/316], Loss: 0.0045\n",
      "Epoch [12/20], Step [138/316], Loss: 0.0084\n",
      "Epoch [12/20], Step [139/316], Loss: 0.0103\n",
      "Epoch [12/20], Step [140/316], Loss: 0.0019\n",
      "Epoch [12/20], Step [141/316], Loss: 0.0556\n",
      "Epoch [12/20], Step [142/316], Loss: 0.1473\n",
      "Epoch [12/20], Step [143/316], Loss: 0.0038\n",
      "Epoch [12/20], Step [144/316], Loss: 0.0868\n",
      "Epoch [12/20], Step [145/316], Loss: 0.0072\n",
      "Epoch [12/20], Step [146/316], Loss: 0.0054\n",
      "Epoch [12/20], Step [147/316], Loss: 0.0840\n",
      "Epoch [12/20], Step [148/316], Loss: 0.0075\n",
      "Epoch [12/20], Step [149/316], Loss: 0.0022\n",
      "Epoch [12/20], Step [150/316], Loss: 0.0014\n",
      "Epoch [12/20], Step [151/316], Loss: 0.0142\n",
      "Epoch [12/20], Step [152/316], Loss: 0.0820\n",
      "Epoch [12/20], Step [153/316], Loss: 0.0007\n",
      "Epoch [12/20], Step [154/316], Loss: 0.0120\n",
      "Epoch [12/20], Step [155/316], Loss: 0.0181\n",
      "Epoch [12/20], Step [156/316], Loss: 0.0079\n",
      "Epoch [12/20], Step [157/316], Loss: 0.0841\n",
      "Epoch [12/20], Step [158/316], Loss: 0.0014\n",
      "Epoch [12/20], Step [159/316], Loss: 0.0132\n",
      "Epoch [12/20], Step [160/316], Loss: 0.0972\n",
      "Epoch [12/20], Step [161/316], Loss: 0.0238\n",
      "Epoch [12/20], Step [162/316], Loss: 0.0449\n",
      "Epoch [12/20], Step [163/316], Loss: 0.0057\n",
      "Epoch [12/20], Step [164/316], Loss: 0.0893\n",
      "Epoch [12/20], Step [165/316], Loss: 0.0008\n",
      "Epoch [12/20], Step [166/316], Loss: 0.0011\n",
      "Epoch [12/20], Step [167/316], Loss: 0.0344\n",
      "Epoch [12/20], Step [168/316], Loss: 0.0098\n",
      "Epoch [12/20], Step [169/316], Loss: 0.0083\n",
      "Epoch [12/20], Step [170/316], Loss: 0.0125\n",
      "Epoch [12/20], Step [171/316], Loss: 0.0040\n",
      "Epoch [12/20], Step [172/316], Loss: 0.0046\n",
      "Epoch [12/20], Step [173/316], Loss: 0.0082\n",
      "Epoch [12/20], Step [174/316], Loss: 0.0042\n",
      "Epoch [12/20], Step [175/316], Loss: 0.1265\n",
      "Epoch [12/20], Step [176/316], Loss: 0.0098\n",
      "Epoch [12/20], Step [177/316], Loss: 0.0061\n",
      "Epoch [12/20], Step [178/316], Loss: 0.0060\n",
      "Epoch [12/20], Step [179/316], Loss: 0.1825\n",
      "Epoch [12/20], Step [180/316], Loss: 0.0037\n",
      "Epoch [12/20], Step [181/316], Loss: 0.0034\n",
      "Epoch [12/20], Step [182/316], Loss: 0.0041\n",
      "Epoch [12/20], Step [183/316], Loss: 0.0050\n",
      "Epoch [12/20], Step [184/316], Loss: 0.0236\n",
      "Epoch [12/20], Step [185/316], Loss: 0.0166\n",
      "Epoch [12/20], Step [186/316], Loss: 0.0007\n",
      "Epoch [12/20], Step [187/316], Loss: 0.0493\n",
      "Epoch [12/20], Step [188/316], Loss: 0.1391\n",
      "Epoch [12/20], Step [189/316], Loss: 0.0089\n",
      "Epoch [12/20], Step [190/316], Loss: 0.0067\n",
      "Epoch [12/20], Step [191/316], Loss: 0.0047\n",
      "Epoch [12/20], Step [192/316], Loss: 0.0100\n",
      "Epoch [12/20], Step [193/316], Loss: 0.0074\n",
      "Epoch [12/20], Step [194/316], Loss: 0.0089\n",
      "Epoch [12/20], Step [195/316], Loss: 0.0301\n",
      "Epoch [12/20], Step [196/316], Loss: 0.0121\n",
      "Epoch [12/20], Step [197/316], Loss: 0.0045\n",
      "Epoch [12/20], Step [198/316], Loss: 0.0016\n",
      "Epoch [12/20], Step [199/316], Loss: 0.0001\n",
      "Epoch [12/20], Step [200/316], Loss: 0.0047\n",
      "Epoch [12/20], Step [201/316], Loss: 0.0317\n",
      "Epoch [12/20], Step [202/316], Loss: 0.0074\n",
      "Epoch [12/20], Step [203/316], Loss: 0.0408\n",
      "Epoch [12/20], Step [204/316], Loss: 0.0031\n",
      "Epoch [12/20], Step [205/316], Loss: 0.0025\n",
      "Epoch [12/20], Step [206/316], Loss: 0.0049\n",
      "Epoch [12/20], Step [207/316], Loss: 0.0015\n",
      "Epoch [12/20], Step [208/316], Loss: 0.1188\n",
      "Epoch [12/20], Step [209/316], Loss: 0.0567\n",
      "Epoch [12/20], Step [210/316], Loss: 0.0564\n",
      "Epoch [12/20], Step [211/316], Loss: 0.0034\n",
      "Epoch [12/20], Step [212/316], Loss: 0.1118\n",
      "Epoch [12/20], Step [213/316], Loss: 0.1210\n",
      "Epoch [12/20], Step [214/316], Loss: 0.0802\n",
      "Epoch [12/20], Step [215/316], Loss: 0.0423\n",
      "Epoch [12/20], Step [216/316], Loss: 0.0722\n",
      "Epoch [12/20], Step [217/316], Loss: 0.0043\n",
      "Epoch [12/20], Step [218/316], Loss: 0.0080\n",
      "Epoch [12/20], Step [219/316], Loss: 0.0074\n",
      "Epoch [12/20], Step [220/316], Loss: 0.0047\n",
      "Epoch [12/20], Step [221/316], Loss: 0.0041\n",
      "Epoch [12/20], Step [222/316], Loss: 0.0051\n",
      "Epoch [12/20], Step [223/316], Loss: 0.0075\n",
      "Epoch [12/20], Step [224/316], Loss: 0.1572\n",
      "Epoch [12/20], Step [225/316], Loss: 0.0016\n",
      "Epoch [12/20], Step [226/316], Loss: 0.0248\n",
      "Epoch [12/20], Step [227/316], Loss: 0.0016\n",
      "Epoch [12/20], Step [228/316], Loss: 0.0146\n",
      "Epoch [12/20], Step [229/316], Loss: 0.0044\n",
      "Epoch [12/20], Step [230/316], Loss: 0.0125\n",
      "Epoch [12/20], Step [231/316], Loss: 0.0124\n",
      "Epoch [12/20], Step [232/316], Loss: 0.0012\n",
      "Epoch [12/20], Step [233/316], Loss: 0.1338\n",
      "Epoch [12/20], Step [234/316], Loss: 0.1147\n",
      "Epoch [12/20], Step [235/316], Loss: 0.0350\n",
      "Epoch [12/20], Step [236/316], Loss: 0.0029\n",
      "Epoch [12/20], Step [237/316], Loss: 0.0039\n",
      "Epoch [12/20], Step [238/316], Loss: 0.0115\n",
      "Epoch [12/20], Step [239/316], Loss: 0.0032\n",
      "Epoch [12/20], Step [240/316], Loss: 0.0032\n",
      "Epoch [12/20], Step [241/316], Loss: 0.0990\n",
      "Epoch [12/20], Step [242/316], Loss: 0.0075\n",
      "Epoch [12/20], Step [243/316], Loss: 0.0019\n",
      "Epoch [12/20], Step [244/316], Loss: 0.0732\n",
      "Epoch [12/20], Step [245/316], Loss: 0.0465\n",
      "Epoch [12/20], Step [246/316], Loss: 0.0020\n",
      "Epoch [12/20], Step [247/316], Loss: 0.0096\n",
      "Epoch [12/20], Step [248/316], Loss: 0.0025\n",
      "Epoch [12/20], Step [249/316], Loss: 0.0027\n",
      "Epoch [12/20], Step [250/316], Loss: 0.1212\n",
      "Epoch [12/20], Step [251/316], Loss: 0.1082\n",
      "Epoch [12/20], Step [252/316], Loss: 0.0054\n",
      "Epoch [12/20], Step [253/316], Loss: 0.0055\n",
      "Epoch [12/20], Step [254/316], Loss: 0.0255\n",
      "Epoch [12/20], Step [255/316], Loss: 0.0064\n",
      "Epoch [12/20], Step [256/316], Loss: 0.2594\n",
      "Epoch [12/20], Step [257/316], Loss: 0.0060\n",
      "Epoch [12/20], Step [258/316], Loss: 0.0186\n",
      "Epoch [12/20], Step [259/316], Loss: 0.0060\n",
      "Epoch [12/20], Step [260/316], Loss: 0.0093\n",
      "Epoch [12/20], Step [261/316], Loss: 0.0126\n",
      "Epoch [12/20], Step [262/316], Loss: 0.0023\n",
      "Epoch [12/20], Step [263/316], Loss: 0.0052\n",
      "Epoch [12/20], Step [264/316], Loss: 0.1366\n",
      "Epoch [12/20], Step [265/316], Loss: 0.0054\n",
      "Epoch [12/20], Step [266/316], Loss: 0.0726\n",
      "Epoch [12/20], Step [267/316], Loss: 0.0902\n",
      "Epoch [12/20], Step [268/316], Loss: 0.0002\n",
      "Epoch [12/20], Step [269/316], Loss: 0.0851\n",
      "Epoch [12/20], Step [270/316], Loss: 0.1048\n",
      "Epoch [12/20], Step [271/316], Loss: 0.0101\n",
      "Epoch [12/20], Step [272/316], Loss: 0.0056\n",
      "Epoch [12/20], Step [273/316], Loss: 0.0059\n",
      "Epoch [12/20], Step [274/316], Loss: 0.0121\n",
      "Epoch [12/20], Step [275/316], Loss: 0.0085\n",
      "Epoch [12/20], Step [276/316], Loss: 0.0048\n",
      "Epoch [12/20], Step [277/316], Loss: 0.0072\n",
      "Epoch [12/20], Step [278/316], Loss: 0.0075\n",
      "Epoch [12/20], Step [279/316], Loss: 0.3132\n",
      "Epoch [12/20], Step [280/316], Loss: 0.0062\n",
      "Epoch [12/20], Step [281/316], Loss: 0.1061\n",
      "Epoch [12/20], Step [282/316], Loss: 0.0064\n",
      "Epoch [12/20], Step [283/316], Loss: 0.0030\n",
      "Epoch [12/20], Step [284/316], Loss: 0.2102\n",
      "Epoch [12/20], Step [285/316], Loss: 0.0064\n",
      "Epoch [12/20], Step [286/316], Loss: 0.0017\n",
      "Epoch [12/20], Step [287/316], Loss: 0.0034\n",
      "Epoch [12/20], Step [288/316], Loss: 0.0065\n",
      "Epoch [12/20], Step [289/316], Loss: 0.0169\n",
      "Epoch [12/20], Step [290/316], Loss: 0.0059\n",
      "Epoch [12/20], Step [291/316], Loss: 0.0077\n",
      "Epoch [12/20], Step [292/316], Loss: 0.0028\n",
      "Epoch [12/20], Step [293/316], Loss: 0.0804\n",
      "Epoch [12/20], Step [294/316], Loss: 0.0058\n",
      "Epoch [12/20], Step [295/316], Loss: 0.0055\n",
      "Epoch [12/20], Step [296/316], Loss: 0.1000\n",
      "Epoch [12/20], Step [297/316], Loss: 0.0052\n",
      "Epoch [12/20], Step [298/316], Loss: 0.0044\n",
      "Epoch [12/20], Step [299/316], Loss: 0.0058\n",
      "Epoch [12/20], Step [300/316], Loss: 0.0033\n",
      "Epoch [12/20], Step [301/316], Loss: 0.0024\n",
      "Epoch [12/20], Step [302/316], Loss: 0.0074\n",
      "Epoch [12/20], Step [303/316], Loss: 0.0407\n",
      "Epoch [12/20], Step [304/316], Loss: 0.0030\n",
      "Epoch [12/20], Step [305/316], Loss: 0.0017\n",
      "Epoch [12/20], Step [306/316], Loss: 0.0070\n",
      "Epoch [12/20], Step [307/316], Loss: 0.0050\n",
      "Epoch [12/20], Step [308/316], Loss: 0.0050\n",
      "Epoch [12/20], Step [309/316], Loss: 0.0517\n",
      "Epoch [12/20], Step [310/316], Loss: 0.0241\n",
      "Epoch [12/20], Step [311/316], Loss: 0.0041\n",
      "Epoch [12/20], Step [312/316], Loss: 0.0070\n",
      "Epoch [12/20], Step [313/316], Loss: 0.0059\n",
      "Epoch [12/20], Step [314/316], Loss: 0.0105\n",
      "Epoch [12/20], Step [315/316], Loss: 0.0072\n",
      "Epoch [12/20], Step [316/316], Loss: 0.0179\n",
      "Epoch [12/20], Train Loss: 0.0307\n",
      "Epoch [12/20], Validation Loss: 0.1591\n",
      "Epoch [12/20], Validation Accuracy: 0.8119\n",
      "Epoch [13/20], Step [1/316], Loss: 0.0889\n",
      "Epoch [13/20], Step [2/316], Loss: 0.0033\n",
      "Epoch [13/20], Step [3/316], Loss: 0.0062\n",
      "Epoch [13/20], Step [4/316], Loss: 0.0052\n",
      "Epoch [13/20], Step [5/316], Loss: 0.0711\n",
      "Epoch [13/20], Step [6/316], Loss: 0.0017\n",
      "Epoch [13/20], Step [7/316], Loss: 0.0071\n",
      "Epoch [13/20], Step [8/316], Loss: 0.0019\n",
      "Epoch [13/20], Step [9/316], Loss: 0.1144\n",
      "Epoch [13/20], Step [10/316], Loss: 0.0045\n",
      "Epoch [13/20], Step [11/316], Loss: 0.0170\n",
      "Epoch [13/20], Step [12/316], Loss: 0.0027\n",
      "Epoch [13/20], Step [13/316], Loss: 0.0034\n",
      "Epoch [13/20], Step [14/316], Loss: 0.0017\n",
      "Epoch [13/20], Step [15/316], Loss: 0.0985\n",
      "Epoch [13/20], Step [16/316], Loss: 0.0138\n",
      "Epoch [13/20], Step [17/316], Loss: 0.0035\n",
      "Epoch [13/20], Step [18/316], Loss: 0.0020\n",
      "Epoch [13/20], Step [19/316], Loss: 0.0036\n",
      "Epoch [13/20], Step [20/316], Loss: 0.1022\n",
      "Epoch [13/20], Step [21/316], Loss: 0.0170\n",
      "Epoch [13/20], Step [22/316], Loss: 0.0032\n",
      "Epoch [13/20], Step [23/316], Loss: 0.0053\n",
      "Epoch [13/20], Step [24/316], Loss: 0.0047\n",
      "Epoch [13/20], Step [25/316], Loss: 0.0269\n",
      "Epoch [13/20], Step [26/316], Loss: 0.0498\n",
      "Epoch [13/20], Step [27/316], Loss: 0.0040\n",
      "Epoch [13/20], Step [28/316], Loss: 0.0029\n",
      "Epoch [13/20], Step [29/316], Loss: 0.0034\n",
      "Epoch [13/20], Step [30/316], Loss: 0.0053\n",
      "Epoch [13/20], Step [31/316], Loss: 0.0131\n",
      "Epoch [13/20], Step [32/316], Loss: 0.0067\n",
      "Epoch [13/20], Step [33/316], Loss: 0.0046\n",
      "Epoch [13/20], Step [34/316], Loss: 0.1111\n",
      "Epoch [13/20], Step [35/316], Loss: 0.0040\n",
      "Epoch [13/20], Step [36/316], Loss: 0.0017\n",
      "Epoch [13/20], Step [37/316], Loss: 0.0104\n",
      "Epoch [13/20], Step [38/316], Loss: 0.0187\n",
      "Epoch [13/20], Step [39/316], Loss: 0.1140\n",
      "Epoch [13/20], Step [40/316], Loss: 0.0077\n",
      "Epoch [13/20], Step [41/316], Loss: 0.0056\n",
      "Epoch [13/20], Step [42/316], Loss: 0.0064\n",
      "Epoch [13/20], Step [43/316], Loss: 0.1153\n",
      "Epoch [13/20], Step [44/316], Loss: 0.0039\n",
      "Epoch [13/20], Step [45/316], Loss: 0.0089\n",
      "Epoch [13/20], Step [46/316], Loss: 0.0022\n",
      "Epoch [13/20], Step [47/316], Loss: 0.0052\n",
      "Epoch [13/20], Step [48/316], Loss: 0.0977\n",
      "Epoch [13/20], Step [49/316], Loss: 0.0082\n",
      "Epoch [13/20], Step [50/316], Loss: 0.0401\n",
      "Epoch [13/20], Step [51/316], Loss: 0.1114\n",
      "Epoch [13/20], Step [52/316], Loss: 0.0035\n",
      "Epoch [13/20], Step [53/316], Loss: 0.0118\n",
      "Epoch [13/20], Step [54/316], Loss: 0.2399\n",
      "Epoch [13/20], Step [55/316], Loss: 0.0278\n",
      "Epoch [13/20], Step [56/316], Loss: 0.0371\n",
      "Epoch [13/20], Step [57/316], Loss: 0.0465\n",
      "Epoch [13/20], Step [58/316], Loss: 0.0466\n",
      "Epoch [13/20], Step [59/316], Loss: 0.0385\n",
      "Epoch [13/20], Step [60/316], Loss: 0.0023\n",
      "Epoch [13/20], Step [61/316], Loss: 0.0021\n",
      "Epoch [13/20], Step [62/316], Loss: 0.0416\n",
      "Epoch [13/20], Step [63/316], Loss: 0.1279\n",
      "Epoch [13/20], Step [64/316], Loss: 0.1268\n",
      "Epoch [13/20], Step [65/316], Loss: 0.0361\n",
      "Epoch [13/20], Step [66/316], Loss: 0.0480\n",
      "Epoch [13/20], Step [67/316], Loss: 0.1289\n",
      "Epoch [13/20], Step [68/316], Loss: 0.0026\n",
      "Epoch [13/20], Step [69/316], Loss: 0.0030\n",
      "Epoch [13/20], Step [70/316], Loss: 0.0311\n",
      "Epoch [13/20], Step [71/316], Loss: 0.0812\n",
      "Epoch [13/20], Step [72/316], Loss: 0.0069\n",
      "Epoch [13/20], Step [73/316], Loss: 0.0129\n",
      "Epoch [13/20], Step [74/316], Loss: 0.0023\n",
      "Epoch [13/20], Step [75/316], Loss: 0.0109\n",
      "Epoch [13/20], Step [76/316], Loss: 0.0050\n",
      "Epoch [13/20], Step [77/316], Loss: 0.0047\n",
      "Epoch [13/20], Step [78/316], Loss: 0.0054\n",
      "Epoch [13/20], Step [79/316], Loss: 0.0114\n",
      "Epoch [13/20], Step [80/316], Loss: 0.0352\n",
      "Epoch [13/20], Step [81/316], Loss: 0.0227\n",
      "Epoch [13/20], Step [82/316], Loss: 0.0167\n",
      "Epoch [13/20], Step [83/316], Loss: 0.3168\n",
      "Epoch [13/20], Step [84/316], Loss: 0.0103\n",
      "Epoch [13/20], Step [85/316], Loss: 0.0066\n",
      "Epoch [13/20], Step [86/316], Loss: 0.0573\n",
      "Epoch [13/20], Step [87/316], Loss: 0.0388\n",
      "Epoch [13/20], Step [88/316], Loss: 0.0113\n",
      "Epoch [13/20], Step [89/316], Loss: 0.0175\n",
      "Epoch [13/20], Step [90/316], Loss: 0.0041\n",
      "Epoch [13/20], Step [91/316], Loss: 0.0054\n",
      "Epoch [13/20], Step [92/316], Loss: 0.0124\n",
      "Epoch [13/20], Step [93/316], Loss: 0.0100\n",
      "Epoch [13/20], Step [94/316], Loss: 0.0060\n",
      "Epoch [13/20], Step [95/316], Loss: 0.0043\n",
      "Epoch [13/20], Step [96/316], Loss: 0.0161\n",
      "Epoch [13/20], Step [97/316], Loss: 0.0108\n",
      "Epoch [13/20], Step [98/316], Loss: 0.0997\n",
      "Epoch [13/20], Step [99/316], Loss: 0.0050\n",
      "Epoch [13/20], Step [100/316], Loss: 0.2412\n",
      "Epoch [13/20], Step [101/316], Loss: 0.0660\n",
      "Epoch [13/20], Step [102/316], Loss: 0.0087\n",
      "Epoch [13/20], Step [103/316], Loss: 0.0177\n",
      "Epoch [13/20], Step [104/316], Loss: 0.0090\n",
      "Epoch [13/20], Step [105/316], Loss: 0.0096\n",
      "Epoch [13/20], Step [106/316], Loss: 0.0038\n",
      "Epoch [13/20], Step [107/316], Loss: 0.0094\n",
      "Epoch [13/20], Step [108/316], Loss: 0.0036\n",
      "Epoch [13/20], Step [109/316], Loss: 0.0309\n",
      "Epoch [13/20], Step [110/316], Loss: 0.0846\n",
      "Epoch [13/20], Step [111/316], Loss: 0.1502\n",
      "Epoch [13/20], Step [112/316], Loss: 0.0025\n",
      "Epoch [13/20], Step [113/316], Loss: 0.0043\n",
      "Epoch [13/20], Step [114/316], Loss: 0.0014\n",
      "Epoch [13/20], Step [115/316], Loss: 0.0068\n",
      "Epoch [13/20], Step [116/316], Loss: 0.0230\n",
      "Epoch [13/20], Step [117/316], Loss: 0.0046\n",
      "Epoch [13/20], Step [118/316], Loss: 0.0059\n",
      "Epoch [13/20], Step [119/316], Loss: 0.0037\n",
      "Epoch [13/20], Step [120/316], Loss: 0.0043\n",
      "Epoch [13/20], Step [121/316], Loss: 0.1984\n",
      "Epoch [13/20], Step [122/316], Loss: 0.0945\n",
      "Epoch [13/20], Step [123/316], Loss: 0.0153\n",
      "Epoch [13/20], Step [124/316], Loss: 0.0035\n",
      "Epoch [13/20], Step [125/316], Loss: 0.0097\n",
      "Epoch [13/20], Step [126/316], Loss: 0.0335\n",
      "Epoch [13/20], Step [127/316], Loss: 0.0070\n",
      "Epoch [13/20], Step [128/316], Loss: 0.0063\n",
      "Epoch [13/20], Step [129/316], Loss: 0.0116\n",
      "Epoch [13/20], Step [130/316], Loss: 0.0556\n",
      "Epoch [13/20], Step [131/316], Loss: 0.1329\n",
      "Epoch [13/20], Step [132/316], Loss: 0.0019\n",
      "Epoch [13/20], Step [133/316], Loss: 0.0121\n",
      "Epoch [13/20], Step [134/316], Loss: 0.0029\n",
      "Epoch [13/20], Step [135/316], Loss: 0.1511\n",
      "Epoch [13/20], Step [136/316], Loss: 0.0962\n",
      "Epoch [13/20], Step [137/316], Loss: 0.0032\n",
      "Epoch [13/20], Step [138/316], Loss: 0.0062\n",
      "Epoch [13/20], Step [139/316], Loss: 0.0774\n",
      "Epoch [13/20], Step [140/316], Loss: 0.0083\n",
      "Epoch [13/20], Step [141/316], Loss: 0.0027\n",
      "Epoch [13/20], Step [142/316], Loss: 0.0478\n",
      "Epoch [13/20], Step [143/316], Loss: 0.0020\n",
      "Epoch [13/20], Step [144/316], Loss: 0.0212\n",
      "Epoch [13/20], Step [145/316], Loss: 0.0036\n",
      "Epoch [13/20], Step [146/316], Loss: 0.0192\n",
      "Epoch [13/20], Step [147/316], Loss: 0.0018\n",
      "Epoch [13/20], Step [148/316], Loss: 0.0031\n",
      "Epoch [13/20], Step [149/316], Loss: 0.4614\n",
      "Epoch [13/20], Step [150/316], Loss: 0.0020\n",
      "Epoch [13/20], Step [151/316], Loss: 0.0119\n",
      "Epoch [13/20], Step [152/316], Loss: 0.0032\n",
      "Epoch [13/20], Step [153/316], Loss: 0.0046\n",
      "Epoch [13/20], Step [154/316], Loss: 0.0064\n",
      "Epoch [13/20], Step [155/316], Loss: 0.0072\n",
      "Epoch [13/20], Step [156/316], Loss: 0.0142\n",
      "Epoch [13/20], Step [157/316], Loss: 0.0070\n",
      "Epoch [13/20], Step [158/316], Loss: 0.0148\n",
      "Epoch [13/20], Step [159/316], Loss: 0.0063\n",
      "Epoch [13/20], Step [160/316], Loss: 0.0030\n",
      "Epoch [13/20], Step [161/316], Loss: 0.0037\n",
      "Epoch [13/20], Step [162/316], Loss: 0.0024\n",
      "Epoch [13/20], Step [163/316], Loss: 0.0488\n",
      "Epoch [13/20], Step [164/316], Loss: 0.0900\n",
      "Epoch [13/20], Step [165/316], Loss: 0.0063\n",
      "Epoch [13/20], Step [166/316], Loss: 0.0415\n",
      "Epoch [13/20], Step [167/316], Loss: 0.0565\n",
      "Epoch [13/20], Step [168/316], Loss: 0.0877\n",
      "Epoch [13/20], Step [169/316], Loss: 0.0169\n",
      "Epoch [13/20], Step [170/316], Loss: 0.0093\n",
      "Epoch [13/20], Step [171/316], Loss: 0.0042\n",
      "Epoch [13/20], Step [172/316], Loss: 0.0071\n",
      "Epoch [13/20], Step [173/316], Loss: 0.0076\n",
      "Epoch [13/20], Step [174/316], Loss: 0.0047\n",
      "Epoch [13/20], Step [175/316], Loss: 0.0146\n",
      "Epoch [13/20], Step [176/316], Loss: 0.0062\n",
      "Epoch [13/20], Step [177/316], Loss: 0.0067\n",
      "Epoch [13/20], Step [178/316], Loss: 0.0104\n",
      "Epoch [13/20], Step [179/316], Loss: 0.0087\n",
      "Epoch [13/20], Step [180/316], Loss: 0.0082\n",
      "Epoch [13/20], Step [181/316], Loss: 0.0014\n",
      "Epoch [13/20], Step [182/316], Loss: 0.0132\n",
      "Epoch [13/20], Step [183/316], Loss: 0.0076\n",
      "Epoch [13/20], Step [184/316], Loss: 0.0595\n",
      "Epoch [13/20], Step [185/316], Loss: 0.0035\n",
      "Epoch [13/20], Step [186/316], Loss: 0.0085\n",
      "Epoch [13/20], Step [187/316], Loss: 0.0249\n",
      "Epoch [13/20], Step [188/316], Loss: 0.0008\n",
      "Epoch [13/20], Step [189/316], Loss: 0.0241\n",
      "Epoch [13/20], Step [190/316], Loss: 0.1028\n",
      "Epoch [13/20], Step [191/316], Loss: 0.0089\n",
      "Epoch [13/20], Step [192/316], Loss: 0.0143\n",
      "Epoch [13/20], Step [193/316], Loss: 0.0034\n",
      "Epoch [13/20], Step [194/316], Loss: 0.0077\n",
      "Epoch [13/20], Step [195/316], Loss: 0.0028\n",
      "Epoch [13/20], Step [196/316], Loss: 0.0028\n",
      "Epoch [13/20], Step [197/316], Loss: 0.0021\n",
      "Epoch [13/20], Step [198/316], Loss: 0.0035\n",
      "Epoch [13/20], Step [199/316], Loss: 0.0650\n",
      "Epoch [13/20], Step [200/316], Loss: 0.1170\n",
      "Epoch [13/20], Step [201/316], Loss: 0.0052\n",
      "Epoch [13/20], Step [202/316], Loss: 0.0053\n",
      "Epoch [13/20], Step [203/316], Loss: 0.2949\n",
      "Epoch [13/20], Step [204/316], Loss: 0.0746\n",
      "Epoch [13/20], Step [205/316], Loss: 0.1382\n",
      "Epoch [13/20], Step [206/316], Loss: 0.0099\n",
      "Epoch [13/20], Step [207/316], Loss: 0.0055\n",
      "Epoch [13/20], Step [208/316], Loss: 0.0099\n",
      "Epoch [13/20], Step [209/316], Loss: 0.0040\n",
      "Epoch [13/20], Step [210/316], Loss: 0.1236\n",
      "Epoch [13/20], Step [211/316], Loss: 0.0056\n",
      "Epoch [13/20], Step [212/316], Loss: 0.0038\n",
      "Epoch [13/20], Step [213/316], Loss: 0.0180\n",
      "Epoch [13/20], Step [214/316], Loss: 0.0151\n",
      "Epoch [13/20], Step [215/316], Loss: 0.0028\n",
      "Epoch [13/20], Step [216/316], Loss: 0.0080\n",
      "Epoch [13/20], Step [217/316], Loss: 0.0056\n",
      "Epoch [13/20], Step [218/316], Loss: 0.0072\n",
      "Epoch [13/20], Step [219/316], Loss: 0.0968\n",
      "Epoch [13/20], Step [220/316], Loss: 0.0072\n",
      "Epoch [13/20], Step [221/316], Loss: 0.0094\n",
      "Epoch [13/20], Step [222/316], Loss: 0.0020\n",
      "Epoch [13/20], Step [223/316], Loss: 0.0035\n",
      "Epoch [13/20], Step [224/316], Loss: 0.0053\n",
      "Epoch [13/20], Step [225/316], Loss: 0.0644\n",
      "Epoch [13/20], Step [226/316], Loss: 0.0091\n",
      "Epoch [13/20], Step [227/316], Loss: 0.0144\n",
      "Epoch [13/20], Step [228/316], Loss: 0.0055\n",
      "Epoch [13/20], Step [229/316], Loss: 0.0042\n",
      "Epoch [13/20], Step [230/316], Loss: 0.0673\n",
      "Epoch [13/20], Step [231/316], Loss: 0.0024\n",
      "Epoch [13/20], Step [232/316], Loss: 0.0038\n",
      "Epoch [13/20], Step [233/316], Loss: 0.0046\n",
      "Epoch [13/20], Step [234/316], Loss: 0.0464\n",
      "Epoch [13/20], Step [235/316], Loss: 0.0040\n",
      "Epoch [13/20], Step [236/316], Loss: 0.0039\n",
      "Epoch [13/20], Step [237/316], Loss: 0.0053\n",
      "Epoch [13/20], Step [238/316], Loss: 0.0126\n",
      "Epoch [13/20], Step [239/316], Loss: 0.0015\n",
      "Epoch [13/20], Step [240/316], Loss: 0.0040\n",
      "Epoch [13/20], Step [241/316], Loss: 0.0424\n",
      "Epoch [13/20], Step [242/316], Loss: 0.0012\n",
      "Epoch [13/20], Step [243/316], Loss: 0.0454\n",
      "Epoch [13/20], Step [244/316], Loss: 0.0091\n",
      "Epoch [13/20], Step [245/316], Loss: 0.0029\n",
      "Epoch [13/20], Step [246/316], Loss: 0.0045\n",
      "Epoch [13/20], Step [247/316], Loss: 0.1155\n",
      "Epoch [13/20], Step [248/316], Loss: 0.1271\n",
      "Epoch [13/20], Step [249/316], Loss: 0.0347\n",
      "Epoch [13/20], Step [250/316], Loss: 0.1781\n",
      "Epoch [13/20], Step [251/316], Loss: 0.0829\n",
      "Epoch [13/20], Step [252/316], Loss: 0.1122\n",
      "Epoch [13/20], Step [253/316], Loss: 0.0029\n",
      "Epoch [13/20], Step [254/316], Loss: 0.0033\n",
      "Epoch [13/20], Step [255/316], Loss: 0.0056\n",
      "Epoch [13/20], Step [256/316], Loss: 0.0124\n",
      "Epoch [13/20], Step [257/316], Loss: 0.0044\n",
      "Epoch [13/20], Step [258/316], Loss: 0.0330\n",
      "Epoch [13/20], Step [259/316], Loss: 0.0061\n",
      "Epoch [13/20], Step [260/316], Loss: 0.0102\n",
      "Epoch [13/20], Step [261/316], Loss: 0.0011\n",
      "Epoch [13/20], Step [262/316], Loss: 0.0031\n",
      "Epoch [13/20], Step [263/316], Loss: 0.0019\n",
      "Epoch [13/20], Step [264/316], Loss: 0.0048\n",
      "Epoch [13/20], Step [265/316], Loss: 0.0060\n",
      "Epoch [13/20], Step [266/316], Loss: 0.0025\n",
      "Epoch [13/20], Step [267/316], Loss: 0.0037\n",
      "Epoch [13/20], Step [268/316], Loss: 0.0220\n",
      "Epoch [13/20], Step [269/316], Loss: 0.0012\n",
      "Epoch [13/20], Step [270/316], Loss: 0.0587\n",
      "Epoch [13/20], Step [271/316], Loss: 0.1291\n",
      "Epoch [13/20], Step [272/316], Loss: 0.0188\n",
      "Epoch [13/20], Step [273/316], Loss: 0.0039\n",
      "Epoch [13/20], Step [274/316], Loss: 0.0056\n",
      "Epoch [13/20], Step [275/316], Loss: 0.0100\n",
      "Epoch [13/20], Step [276/316], Loss: 0.0128\n",
      "Epoch [13/20], Step [277/316], Loss: 0.0086\n",
      "Epoch [13/20], Step [278/316], Loss: 0.0048\n",
      "Epoch [13/20], Step [279/316], Loss: 0.2336\n",
      "Epoch [13/20], Step [280/316], Loss: 0.0068\n",
      "Epoch [13/20], Step [281/316], Loss: 0.0041\n",
      "Epoch [13/20], Step [282/316], Loss: 0.1473\n",
      "Epoch [13/20], Step [283/316], Loss: 0.0037\n",
      "Epoch [13/20], Step [284/316], Loss: 0.0097\n",
      "Epoch [13/20], Step [285/316], Loss: 0.1744\n",
      "Epoch [13/20], Step [286/316], Loss: 0.0326\n",
      "Epoch [13/20], Step [287/316], Loss: 0.0066\n",
      "Epoch [13/20], Step [288/316], Loss: 0.0563\n",
      "Epoch [13/20], Step [289/316], Loss: 0.0358\n",
      "Epoch [13/20], Step [290/316], Loss: 0.0031\n",
      "Epoch [13/20], Step [291/316], Loss: 0.0092\n",
      "Epoch [13/20], Step [292/316], Loss: 0.0092\n",
      "Epoch [13/20], Step [293/316], Loss: 0.0059\n",
      "Epoch [13/20], Step [294/316], Loss: 0.0173\n",
      "Epoch [13/20], Step [295/316], Loss: 0.0024\n",
      "Epoch [13/20], Step [296/316], Loss: 0.0049\n",
      "Epoch [13/20], Step [297/316], Loss: 0.0066\n",
      "Epoch [13/20], Step [298/316], Loss: 0.0100\n",
      "Epoch [13/20], Step [299/316], Loss: 0.0057\n",
      "Epoch [13/20], Step [300/316], Loss: 0.0163\n",
      "Epoch [13/20], Step [301/316], Loss: 0.2951\n",
      "Epoch [13/20], Step [302/316], Loss: 0.0929\n",
      "Epoch [13/20], Step [303/316], Loss: 0.0054\n",
      "Epoch [13/20], Step [304/316], Loss: 0.0009\n",
      "Epoch [13/20], Step [305/316], Loss: 0.0011\n",
      "Epoch [13/20], Step [306/316], Loss: 0.0174\n",
      "Epoch [13/20], Step [307/316], Loss: 0.0134\n",
      "Epoch [13/20], Step [308/316], Loss: 0.0052\n",
      "Epoch [13/20], Step [309/316], Loss: 0.2196\n",
      "Epoch [13/20], Step [310/316], Loss: 0.0039\n",
      "Epoch [13/20], Step [311/316], Loss: 0.1316\n",
      "Epoch [13/20], Step [312/316], Loss: 0.0017\n",
      "Epoch [13/20], Step [313/316], Loss: 0.0039\n",
      "Epoch [13/20], Step [314/316], Loss: 0.0023\n",
      "Epoch [13/20], Step [315/316], Loss: 0.0119\n",
      "Epoch [13/20], Step [316/316], Loss: 0.0065\n",
      "Epoch [13/20], Train Loss: 0.0325\n",
      "Epoch [13/20], Validation Loss: 0.1665\n",
      "Epoch [13/20], Validation Accuracy: 0.8095\n",
      "Epoch [14/20], Step [1/316], Loss: 0.0078\n",
      "Epoch [14/20], Step [2/316], Loss: 0.0065\n",
      "Epoch [14/20], Step [3/316], Loss: 0.0028\n",
      "Epoch [14/20], Step [4/316], Loss: 0.0018\n",
      "Epoch [14/20], Step [5/316], Loss: 0.0175\n",
      "Epoch [14/20], Step [6/316], Loss: 0.0045\n",
      "Epoch [14/20], Step [7/316], Loss: 0.0051\n",
      "Epoch [14/20], Step [8/316], Loss: 0.0063\n",
      "Epoch [14/20], Step [9/316], Loss: 0.0075\n",
      "Epoch [14/20], Step [10/316], Loss: 0.0295\n",
      "Epoch [14/20], Step [11/316], Loss: 0.0244\n",
      "Epoch [14/20], Step [12/316], Loss: 0.0096\n",
      "Epoch [14/20], Step [13/316], Loss: 0.0143\n",
      "Epoch [14/20], Step [14/316], Loss: 0.0023\n",
      "Epoch [14/20], Step [15/316], Loss: 0.0035\n",
      "Epoch [14/20], Step [16/316], Loss: 0.0748\n",
      "Epoch [14/20], Step [17/316], Loss: 0.0064\n",
      "Epoch [14/20], Step [18/316], Loss: 0.1269\n",
      "Epoch [14/20], Step [19/316], Loss: 0.0019\n",
      "Epoch [14/20], Step [20/316], Loss: 0.0285\n",
      "Epoch [14/20], Step [21/316], Loss: 0.0060\n",
      "Epoch [14/20], Step [22/316], Loss: 0.0026\n",
      "Epoch [14/20], Step [23/316], Loss: 0.0104\n",
      "Epoch [14/20], Step [24/316], Loss: 0.1338\n",
      "Epoch [14/20], Step [25/316], Loss: 0.0046\n",
      "Epoch [14/20], Step [26/316], Loss: 0.0412\n",
      "Epoch [14/20], Step [27/316], Loss: 0.1105\n",
      "Epoch [14/20], Step [28/316], Loss: 0.0189\n",
      "Epoch [14/20], Step [29/316], Loss: 0.0811\n",
      "Epoch [14/20], Step [30/316], Loss: 0.0330\n",
      "Epoch [14/20], Step [31/316], Loss: 0.1618\n",
      "Epoch [14/20], Step [32/316], Loss: 0.0113\n",
      "Epoch [14/20], Step [33/316], Loss: 0.0023\n",
      "Epoch [14/20], Step [34/316], Loss: 0.0040\n",
      "Epoch [14/20], Step [35/316], Loss: 0.0017\n",
      "Epoch [14/20], Step [36/316], Loss: 0.0021\n",
      "Epoch [14/20], Step [37/316], Loss: 0.0043\n",
      "Epoch [14/20], Step [38/316], Loss: 0.2102\n",
      "Epoch [14/20], Step [39/316], Loss: 0.0015\n",
      "Epoch [14/20], Step [40/316], Loss: 0.0022\n",
      "Epoch [14/20], Step [41/316], Loss: 0.1114\n",
      "Epoch [14/20], Step [42/316], Loss: 0.0169\n",
      "Epoch [14/20], Step [43/316], Loss: 0.0031\n",
      "Epoch [14/20], Step [44/316], Loss: 0.0023\n",
      "Epoch [14/20], Step [45/316], Loss: 0.0067\n",
      "Epoch [14/20], Step [46/316], Loss: 0.0037\n",
      "Epoch [14/20], Step [47/316], Loss: 0.0060\n",
      "Epoch [14/20], Step [48/316], Loss: 0.0037\n",
      "Epoch [14/20], Step [49/316], Loss: 0.0033\n",
      "Epoch [14/20], Step [50/316], Loss: 0.0005\n",
      "Epoch [14/20], Step [51/316], Loss: 0.0899\n",
      "Epoch [14/20], Step [52/316], Loss: 0.0025\n",
      "Epoch [14/20], Step [53/316], Loss: 0.0043\n",
      "Epoch [14/20], Step [54/316], Loss: 0.0028\n",
      "Epoch [14/20], Step [55/316], Loss: 0.0069\n",
      "Epoch [14/20], Step [56/316], Loss: 0.0030\n",
      "Epoch [14/20], Step [57/316], Loss: 0.0077\n",
      "Epoch [14/20], Step [58/316], Loss: 0.0065\n",
      "Epoch [14/20], Step [59/316], Loss: 0.0057\n",
      "Epoch [14/20], Step [60/316], Loss: 0.0014\n",
      "Epoch [14/20], Step [61/316], Loss: 0.0327\n",
      "Epoch [14/20], Step [62/316], Loss: 0.0043\n",
      "Epoch [14/20], Step [63/316], Loss: 0.0009\n",
      "Epoch [14/20], Step [64/316], Loss: 0.0143\n",
      "Epoch [14/20], Step [65/316], Loss: 0.0473\n",
      "Epoch [14/20], Step [66/316], Loss: 0.0064\n",
      "Epoch [14/20], Step [67/316], Loss: 0.0019\n",
      "Epoch [14/20], Step [68/316], Loss: 0.0034\n",
      "Epoch [14/20], Step [69/316], Loss: 0.0030\n",
      "Epoch [14/20], Step [70/316], Loss: 0.0246\n",
      "Epoch [14/20], Step [71/316], Loss: 0.0032\n",
      "Epoch [14/20], Step [72/316], Loss: 0.0398\n",
      "Epoch [14/20], Step [73/316], Loss: 0.1235\n",
      "Epoch [14/20], Step [74/316], Loss: 0.0041\n",
      "Epoch [14/20], Step [75/316], Loss: 0.0083\n",
      "Epoch [14/20], Step [76/316], Loss: 0.0011\n",
      "Epoch [14/20], Step [77/316], Loss: 0.0039\n",
      "Epoch [14/20], Step [78/316], Loss: 0.0038\n",
      "Epoch [14/20], Step [79/316], Loss: 0.0029\n",
      "Epoch [14/20], Step [80/316], Loss: 0.0070\n",
      "Epoch [14/20], Step [81/316], Loss: 0.0033\n",
      "Epoch [14/20], Step [82/316], Loss: 0.0930\n",
      "Epoch [14/20], Step [83/316], Loss: 0.0029\n",
      "Epoch [14/20], Step [84/316], Loss: 0.0030\n",
      "Epoch [14/20], Step [85/316], Loss: 0.1132\n",
      "Epoch [14/20], Step [86/316], Loss: 0.0709\n",
      "Epoch [14/20], Step [87/316], Loss: 0.1885\n",
      "Epoch [14/20], Step [88/316], Loss: 0.0103\n",
      "Epoch [14/20], Step [89/316], Loss: 0.0169\n",
      "Epoch [14/20], Step [90/316], Loss: 0.0172\n",
      "Epoch [14/20], Step [91/316], Loss: 0.0010\n",
      "Epoch [14/20], Step [92/316], Loss: 0.0013\n",
      "Epoch [14/20], Step [93/316], Loss: 0.0115\n",
      "Epoch [14/20], Step [94/316], Loss: 0.0023\n",
      "Epoch [14/20], Step [95/316], Loss: 0.0027\n",
      "Epoch [14/20], Step [96/316], Loss: 0.0592\n",
      "Epoch [14/20], Step [97/316], Loss: 0.0029\n",
      "Epoch [14/20], Step [98/316], Loss: 0.1128\n",
      "Epoch [14/20], Step [99/316], Loss: 0.0105\n",
      "Epoch [14/20], Step [100/316], Loss: 0.0060\n",
      "Epoch [14/20], Step [101/316], Loss: 0.0970\n",
      "Epoch [14/20], Step [102/316], Loss: 0.0186\n",
      "Epoch [14/20], Step [103/316], Loss: 0.0013\n",
      "Epoch [14/20], Step [104/316], Loss: 0.1332\n",
      "Epoch [14/20], Step [105/316], Loss: 0.0040\n",
      "Epoch [14/20], Step [106/316], Loss: 0.0037\n",
      "Epoch [14/20], Step [107/316], Loss: 0.0012\n",
      "Epoch [14/20], Step [108/316], Loss: 0.0059\n",
      "Epoch [14/20], Step [109/316], Loss: 0.1569\n",
      "Epoch [14/20], Step [110/316], Loss: 0.0030\n",
      "Epoch [14/20], Step [111/316], Loss: 0.0053\n",
      "Epoch [14/20], Step [112/316], Loss: 0.1142\n",
      "Epoch [14/20], Step [113/316], Loss: 0.1064\n",
      "Epoch [14/20], Step [114/316], Loss: 0.0035\n",
      "Epoch [14/20], Step [115/316], Loss: 0.1047\n",
      "Epoch [14/20], Step [116/316], Loss: 0.0056\n",
      "Epoch [14/20], Step [117/316], Loss: 0.2148\n",
      "Epoch [14/20], Step [118/316], Loss: 0.0196\n",
      "Epoch [14/20], Step [119/316], Loss: 0.0108\n",
      "Epoch [14/20], Step [120/316], Loss: 0.0123\n",
      "Epoch [14/20], Step [121/316], Loss: 0.0036\n",
      "Epoch [14/20], Step [122/316], Loss: 0.0023\n",
      "Epoch [14/20], Step [123/316], Loss: 0.0027\n",
      "Epoch [14/20], Step [124/316], Loss: 0.0021\n",
      "Epoch [14/20], Step [125/316], Loss: 0.0040\n",
      "Epoch [14/20], Step [126/316], Loss: 0.0070\n",
      "Epoch [14/20], Step [127/316], Loss: 0.0051\n",
      "Epoch [14/20], Step [128/316], Loss: 0.0061\n",
      "Epoch [14/20], Step [129/316], Loss: 0.0062\n",
      "Epoch [14/20], Step [130/316], Loss: 0.0046\n",
      "Epoch [14/20], Step [131/316], Loss: 0.0047\n",
      "Epoch [14/20], Step [132/316], Loss: 0.0114\n",
      "Epoch [14/20], Step [133/316], Loss: 0.4065\n",
      "Epoch [14/20], Step [134/316], Loss: 0.0028\n",
      "Epoch [14/20], Step [135/316], Loss: 0.0511\n",
      "Epoch [14/20], Step [136/316], Loss: 0.0856\n",
      "Epoch [14/20], Step [137/316], Loss: 0.0007\n",
      "Epoch [14/20], Step [138/316], Loss: 0.1832\n",
      "Epoch [14/20], Step [139/316], Loss: 0.0071\n",
      "Epoch [14/20], Step [140/316], Loss: 0.0040\n",
      "Epoch [14/20], Step [141/316], Loss: 0.0048\n",
      "Epoch [14/20], Step [142/316], Loss: 0.0063\n",
      "Epoch [14/20], Step [143/316], Loss: 0.0184\n",
      "Epoch [14/20], Step [144/316], Loss: 0.0116\n",
      "Epoch [14/20], Step [145/316], Loss: 0.0061\n",
      "Epoch [14/20], Step [146/316], Loss: 0.0084\n",
      "Epoch [14/20], Step [147/316], Loss: 0.0055\n",
      "Epoch [14/20], Step [148/316], Loss: 0.1012\n",
      "Epoch [14/20], Step [149/316], Loss: 0.0341\n",
      "Epoch [14/20], Step [150/316], Loss: 0.0021\n",
      "Epoch [14/20], Step [151/316], Loss: 0.0033\n",
      "Epoch [14/20], Step [152/316], Loss: 0.0116\n",
      "Epoch [14/20], Step [153/316], Loss: 0.1053\n",
      "Epoch [14/20], Step [154/316], Loss: 0.0044\n",
      "Epoch [14/20], Step [155/316], Loss: 0.0014\n",
      "Epoch [14/20], Step [156/316], Loss: 0.0041\n",
      "Epoch [14/20], Step [157/316], Loss: 0.0035\n",
      "Epoch [14/20], Step [158/316], Loss: 0.0118\n",
      "Epoch [14/20], Step [159/316], Loss: 0.0053\n",
      "Epoch [14/20], Step [160/316], Loss: 0.0042\n",
      "Epoch [14/20], Step [161/316], Loss: 0.0023\n",
      "Epoch [14/20], Step [162/316], Loss: 0.0019\n",
      "Epoch [14/20], Step [163/316], Loss: 0.0059\n",
      "Epoch [14/20], Step [164/316], Loss: 0.1140\n",
      "Epoch [14/20], Step [165/316], Loss: 0.0152\n",
      "Epoch [14/20], Step [166/316], Loss: 0.0082\n",
      "Epoch [14/20], Step [167/316], Loss: 0.0209\n",
      "Epoch [14/20], Step [168/316], Loss: 0.0138\n",
      "Epoch [14/20], Step [169/316], Loss: 0.1943\n",
      "Epoch [14/20], Step [170/316], Loss: 0.0047\n",
      "Epoch [14/20], Step [171/316], Loss: 0.0043\n",
      "Epoch [14/20], Step [172/316], Loss: 0.1001\n",
      "Epoch [14/20], Step [173/316], Loss: 0.0078\n",
      "Epoch [14/20], Step [174/316], Loss: 0.1097\n",
      "Epoch [14/20], Step [175/316], Loss: 0.0042\n",
      "Epoch [14/20], Step [176/316], Loss: 0.0035\n",
      "Epoch [14/20], Step [177/316], Loss: 0.0104\n",
      "Epoch [14/20], Step [178/316], Loss: 0.1843\n",
      "Epoch [14/20], Step [179/316], Loss: 0.0048\n",
      "Epoch [14/20], Step [180/316], Loss: 0.0101\n",
      "Epoch [14/20], Step [181/316], Loss: 0.1159\n",
      "Epoch [14/20], Step [182/316], Loss: 0.0093\n",
      "Epoch [14/20], Step [183/316], Loss: 0.0117\n",
      "Epoch [14/20], Step [184/316], Loss: 0.0044\n",
      "Epoch [14/20], Step [185/316], Loss: 0.0500\n",
      "Epoch [14/20], Step [186/316], Loss: 0.0186\n",
      "Epoch [14/20], Step [187/316], Loss: 0.0046\n",
      "Epoch [14/20], Step [188/316], Loss: 0.0646\n",
      "Epoch [14/20], Step [189/316], Loss: 0.0021\n",
      "Epoch [14/20], Step [190/316], Loss: 0.0823\n",
      "Epoch [14/20], Step [191/316], Loss: 0.0096\n",
      "Epoch [14/20], Step [192/316], Loss: 0.0123\n",
      "Epoch [14/20], Step [193/316], Loss: 0.0057\n",
      "Epoch [14/20], Step [194/316], Loss: 0.0034\n",
      "Epoch [14/20], Step [195/316], Loss: 0.0038\n",
      "Epoch [14/20], Step [196/316], Loss: 0.0101\n",
      "Epoch [14/20], Step [197/316], Loss: 0.0030\n",
      "Epoch [14/20], Step [198/316], Loss: 0.0416\n",
      "Epoch [14/20], Step [199/316], Loss: 0.0173\n",
      "Epoch [14/20], Step [200/316], Loss: 0.0030\n",
      "Epoch [14/20], Step [201/316], Loss: 0.0083\n",
      "Epoch [14/20], Step [202/316], Loss: 0.0029\n",
      "Epoch [14/20], Step [203/316], Loss: 0.0628\n",
      "Epoch [14/20], Step [204/316], Loss: 0.0010\n",
      "Epoch [14/20], Step [205/316], Loss: 0.0143\n",
      "Epoch [14/20], Step [206/316], Loss: 0.0073\n",
      "Epoch [14/20], Step [207/316], Loss: 0.0027\n",
      "Epoch [14/20], Step [208/316], Loss: 0.0041\n",
      "Epoch [14/20], Step [209/316], Loss: 0.0055\n",
      "Epoch [14/20], Step [210/316], Loss: 0.0021\n",
      "Epoch [14/20], Step [211/316], Loss: 0.0041\n",
      "Epoch [14/20], Step [212/316], Loss: 0.0058\n",
      "Epoch [14/20], Step [213/316], Loss: 0.0063\n",
      "Epoch [14/20], Step [214/316], Loss: 0.1012\n",
      "Epoch [14/20], Step [215/316], Loss: 0.0052\n",
      "Epoch [14/20], Step [216/316], Loss: 0.0077\n",
      "Epoch [14/20], Step [217/316], Loss: 0.0108\n",
      "Epoch [14/20], Step [218/316], Loss: 0.0018\n",
      "Epoch [14/20], Step [219/316], Loss: 0.0923\n",
      "Epoch [14/20], Step [220/316], Loss: 0.0067\n",
      "Epoch [14/20], Step [221/316], Loss: 0.1159\n",
      "Epoch [14/20], Step [222/316], Loss: 0.0128\n",
      "Epoch [14/20], Step [223/316], Loss: 0.1658\n",
      "Epoch [14/20], Step [224/316], Loss: 0.0039\n",
      "Epoch [14/20], Step [225/316], Loss: 0.0031\n",
      "Epoch [14/20], Step [226/316], Loss: 0.0030\n",
      "Epoch [14/20], Step [227/316], Loss: 0.0041\n",
      "Epoch [14/20], Step [228/316], Loss: 0.0554\n",
      "Epoch [14/20], Step [229/316], Loss: 0.0054\n",
      "Epoch [14/20], Step [230/316], Loss: 0.0288\n",
      "Epoch [14/20], Step [231/316], Loss: 0.0790\n",
      "Epoch [14/20], Step [232/316], Loss: 0.0056\n",
      "Epoch [14/20], Step [233/316], Loss: 0.0096\n",
      "Epoch [14/20], Step [234/316], Loss: 0.0038\n",
      "Epoch [14/20], Step [235/316], Loss: 0.0060\n",
      "Epoch [14/20], Step [236/316], Loss: 0.0542\n",
      "Epoch [14/20], Step [237/316], Loss: 0.0073\n",
      "Epoch [14/20], Step [238/316], Loss: 0.0033\n",
      "Epoch [14/20], Step [239/316], Loss: 0.0121\n",
      "Epoch [14/20], Step [240/316], Loss: 0.0172\n",
      "Epoch [14/20], Step [241/316], Loss: 0.0149\n",
      "Epoch [14/20], Step [242/316], Loss: 0.0060\n",
      "Epoch [14/20], Step [243/316], Loss: 0.0139\n",
      "Epoch [14/20], Step [244/316], Loss: 0.0499\n",
      "Epoch [14/20], Step [245/316], Loss: 0.0060\n",
      "Epoch [14/20], Step [246/316], Loss: 0.0210\n",
      "Epoch [14/20], Step [247/316], Loss: 0.0138\n",
      "Epoch [14/20], Step [248/316], Loss: 0.0123\n",
      "Epoch [14/20], Step [249/316], Loss: 0.0068\n",
      "Epoch [14/20], Step [250/316], Loss: 0.0067\n",
      "Epoch [14/20], Step [251/316], Loss: 0.0175\n",
      "Epoch [14/20], Step [252/316], Loss: 0.0487\n",
      "Epoch [14/20], Step [253/316], Loss: 0.0949\n",
      "Epoch [14/20], Step [254/316], Loss: 0.0142\n",
      "Epoch [14/20], Step [255/316], Loss: 0.0043\n",
      "Epoch [14/20], Step [256/316], Loss: 0.0051\n",
      "Epoch [14/20], Step [257/316], Loss: 0.0706\n",
      "Epoch [14/20], Step [258/316], Loss: 0.0190\n",
      "Epoch [14/20], Step [259/316], Loss: 0.1236\n",
      "Epoch [14/20], Step [260/316], Loss: 0.0063\n",
      "Epoch [14/20], Step [261/316], Loss: 0.0093\n",
      "Epoch [14/20], Step [262/316], Loss: 0.0044\n",
      "Epoch [14/20], Step [263/316], Loss: 0.0185\n",
      "Epoch [14/20], Step [264/316], Loss: 0.0004\n",
      "Epoch [14/20], Step [265/316], Loss: 0.0046\n",
      "Epoch [14/20], Step [266/316], Loss: 0.0943\n",
      "Epoch [14/20], Step [267/316], Loss: 0.0071\n",
      "Epoch [14/20], Step [268/316], Loss: 0.0634\n",
      "Epoch [14/20], Step [269/316], Loss: 0.0026\n",
      "Epoch [14/20], Step [270/316], Loss: 0.1083\n",
      "Epoch [14/20], Step [271/316], Loss: 0.0152\n",
      "Epoch [14/20], Step [272/316], Loss: 0.0054\n",
      "Epoch [14/20], Step [273/316], Loss: 0.0372\n",
      "Epoch [14/20], Step [274/316], Loss: 0.1179\n",
      "Epoch [14/20], Step [275/316], Loss: 0.0068\n",
      "Epoch [14/20], Step [276/316], Loss: 0.0600\n",
      "Epoch [14/20], Step [277/316], Loss: 0.0054\n",
      "Epoch [14/20], Step [278/316], Loss: 0.0102\n",
      "Epoch [14/20], Step [279/316], Loss: 0.0192\n",
      "Epoch [14/20], Step [280/316], Loss: 0.0095\n",
      "Epoch [14/20], Step [281/316], Loss: 0.1512\n",
      "Epoch [14/20], Step [282/316], Loss: 0.0367\n",
      "Epoch [14/20], Step [283/316], Loss: 0.0095\n",
      "Epoch [14/20], Step [284/316], Loss: 0.0118\n",
      "Epoch [14/20], Step [285/316], Loss: 0.0271\n",
      "Epoch [14/20], Step [286/316], Loss: 0.0046\n",
      "Epoch [14/20], Step [287/316], Loss: 0.0115\n",
      "Epoch [14/20], Step [288/316], Loss: 0.0088\n",
      "Epoch [14/20], Step [289/316], Loss: 0.0045\n",
      "Epoch [14/20], Step [290/316], Loss: 0.0078\n",
      "Epoch [14/20], Step [291/316], Loss: 0.0120\n",
      "Epoch [14/20], Step [292/316], Loss: 0.0213\n",
      "Epoch [14/20], Step [293/316], Loss: 0.0505\n",
      "Epoch [14/20], Step [294/316], Loss: 0.0641\n",
      "Epoch [14/20], Step [295/316], Loss: 0.0030\n",
      "Epoch [14/20], Step [296/316], Loss: 0.0032\n",
      "Epoch [14/20], Step [297/316], Loss: 0.2257\n",
      "Epoch [14/20], Step [298/316], Loss: 0.0035\n",
      "Epoch [14/20], Step [299/316], Loss: 0.0058\n",
      "Epoch [14/20], Step [300/316], Loss: 0.0081\n",
      "Epoch [14/20], Step [301/316], Loss: 0.0045\n",
      "Epoch [14/20], Step [302/316], Loss: 0.0130\n",
      "Epoch [14/20], Step [303/316], Loss: 0.1254\n",
      "Epoch [14/20], Step [304/316], Loss: 0.0061\n",
      "Epoch [14/20], Step [305/316], Loss: 0.0720\n",
      "Epoch [14/20], Step [306/316], Loss: 0.0022\n",
      "Epoch [14/20], Step [307/316], Loss: 0.0732\n",
      "Epoch [14/20], Step [308/316], Loss: 0.0280\n",
      "Epoch [14/20], Step [309/316], Loss: 0.0343\n",
      "Epoch [14/20], Step [310/316], Loss: 0.0663\n",
      "Epoch [14/20], Step [311/316], Loss: 0.0096\n",
      "Epoch [14/20], Step [312/316], Loss: 0.0139\n",
      "Epoch [14/20], Step [313/316], Loss: 0.0061\n",
      "Epoch [14/20], Step [314/316], Loss: 0.0011\n",
      "Epoch [14/20], Step [315/316], Loss: 0.0292\n",
      "Epoch [14/20], Step [316/316], Loss: 0.0031\n",
      "Epoch [14/20], Train Loss: 0.0295\n",
      "Epoch [14/20], Validation Loss: 0.1689\n",
      "Epoch [14/20], Validation Accuracy: 0.8107\n",
      "Epoch [15/20], Step [1/316], Loss: 0.0010\n",
      "Epoch [15/20], Step [2/316], Loss: 0.0046\n",
      "Epoch [15/20], Step [3/316], Loss: 0.0088\n",
      "Epoch [15/20], Step [4/316], Loss: 0.1429\n",
      "Epoch [15/20], Step [5/316], Loss: 0.0062\n",
      "Epoch [15/20], Step [6/316], Loss: 0.0067\n",
      "Epoch [15/20], Step [7/316], Loss: 0.0119\n",
      "Epoch [15/20], Step [8/316], Loss: 0.0075\n",
      "Epoch [15/20], Step [9/316], Loss: 0.0949\n",
      "Epoch [15/20], Step [10/316], Loss: 0.1005\n",
      "Epoch [15/20], Step [11/316], Loss: 0.0151\n",
      "Epoch [15/20], Step [12/316], Loss: 0.2651\n",
      "Epoch [15/20], Step [13/316], Loss: 0.1332\n",
      "Epoch [15/20], Step [14/316], Loss: 0.0048\n",
      "Epoch [15/20], Step [15/316], Loss: 0.0586\n",
      "Epoch [15/20], Step [16/316], Loss: 0.0776\n",
      "Epoch [15/20], Step [17/316], Loss: 0.0027\n",
      "Epoch [15/20], Step [18/316], Loss: 0.0048\n",
      "Epoch [15/20], Step [19/316], Loss: 0.0276\n",
      "Epoch [15/20], Step [20/316], Loss: 0.0015\n",
      "Epoch [15/20], Step [21/316], Loss: 0.1802\n",
      "Epoch [15/20], Step [22/316], Loss: 0.0289\n",
      "Epoch [15/20], Step [23/316], Loss: 0.0010\n",
      "Epoch [15/20], Step [24/316], Loss: 0.0039\n",
      "Epoch [15/20], Step [25/316], Loss: 0.0005\n",
      "Epoch [15/20], Step [26/316], Loss: 0.0365\n",
      "Epoch [15/20], Step [27/316], Loss: 0.0133\n",
      "Epoch [15/20], Step [28/316], Loss: 0.0777\n",
      "Epoch [15/20], Step [29/316], Loss: 0.0244\n",
      "Epoch [15/20], Step [30/316], Loss: 0.0901\n",
      "Epoch [15/20], Step [31/316], Loss: 0.0088\n",
      "Epoch [15/20], Step [32/316], Loss: 0.0072\n",
      "Epoch [15/20], Step [33/316], Loss: 0.0406\n",
      "Epoch [15/20], Step [34/316], Loss: 0.0038\n",
      "Epoch [15/20], Step [35/316], Loss: 0.0842\n",
      "Epoch [15/20], Step [36/316], Loss: 0.0056\n",
      "Epoch [15/20], Step [37/316], Loss: 0.1084\n",
      "Epoch [15/20], Step [38/316], Loss: 0.0023\n",
      "Epoch [15/20], Step [39/316], Loss: 0.0026\n",
      "Epoch [15/20], Step [40/316], Loss: 0.0107\n",
      "Epoch [15/20], Step [41/316], Loss: 0.0029\n",
      "Epoch [15/20], Step [42/316], Loss: 0.0064\n",
      "Epoch [15/20], Step [43/316], Loss: 0.0746\n",
      "Epoch [15/20], Step [44/316], Loss: 0.0053\n",
      "Epoch [15/20], Step [45/316], Loss: 0.0096\n",
      "Epoch [15/20], Step [46/316], Loss: 0.0037\n",
      "Epoch [15/20], Step [47/316], Loss: 0.0057\n",
      "Epoch [15/20], Step [48/316], Loss: 0.0026\n",
      "Epoch [15/20], Step [49/316], Loss: 0.0047\n",
      "Epoch [15/20], Step [50/316], Loss: 0.1020\n",
      "Epoch [15/20], Step [51/316], Loss: 0.0227\n",
      "Epoch [15/20], Step [52/316], Loss: 0.0053\n",
      "Epoch [15/20], Step [53/316], Loss: 0.0103\n",
      "Epoch [15/20], Step [54/316], Loss: 0.0061\n",
      "Epoch [15/20], Step [55/316], Loss: 0.0074\n",
      "Epoch [15/20], Step [56/316], Loss: 0.0951\n",
      "Epoch [15/20], Step [57/316], Loss: 0.1110\n",
      "Epoch [15/20], Step [58/316], Loss: 0.0835\n",
      "Epoch [15/20], Step [59/316], Loss: 0.0005\n",
      "Epoch [15/20], Step [60/316], Loss: 0.0029\n",
      "Epoch [15/20], Step [61/316], Loss: 0.0028\n",
      "Epoch [15/20], Step [62/316], Loss: 0.0038\n",
      "Epoch [15/20], Step [63/316], Loss: 0.0023\n",
      "Epoch [15/20], Step [64/316], Loss: 0.0003\n",
      "Epoch [15/20], Step [65/316], Loss: 0.0598\n",
      "Epoch [15/20], Step [66/316], Loss: 0.0222\n",
      "Epoch [15/20], Step [67/316], Loss: 0.0043\n",
      "Epoch [15/20], Step [68/316], Loss: 0.1301\n",
      "Epoch [15/20], Step [69/316], Loss: 0.0019\n",
      "Epoch [15/20], Step [70/316], Loss: 0.0039\n",
      "Epoch [15/20], Step [71/316], Loss: 0.0513\n",
      "Epoch [15/20], Step [72/316], Loss: 0.0028\n",
      "Epoch [15/20], Step [73/316], Loss: 0.0068\n",
      "Epoch [15/20], Step [74/316], Loss: 0.0042\n",
      "Epoch [15/20], Step [75/316], Loss: 0.0177\n",
      "Epoch [15/20], Step [76/316], Loss: 0.1839\n",
      "Epoch [15/20], Step [77/316], Loss: 0.0037\n",
      "Epoch [15/20], Step [78/316], Loss: 0.0349\n",
      "Epoch [15/20], Step [79/316], Loss: 0.0025\n",
      "Epoch [15/20], Step [80/316], Loss: 0.0601\n",
      "Epoch [15/20], Step [81/316], Loss: 0.0083\n",
      "Epoch [15/20], Step [82/316], Loss: 0.0018\n",
      "Epoch [15/20], Step [83/316], Loss: 0.0057\n",
      "Epoch [15/20], Step [84/316], Loss: 0.0060\n",
      "Epoch [15/20], Step [85/316], Loss: 0.0034\n",
      "Epoch [15/20], Step [86/316], Loss: 0.0200\n",
      "Epoch [15/20], Step [87/316], Loss: 0.0018\n",
      "Epoch [15/20], Step [88/316], Loss: 0.0053\n",
      "Epoch [15/20], Step [89/316], Loss: 0.1226\n",
      "Epoch [15/20], Step [90/316], Loss: 0.0062\n",
      "Epoch [15/20], Step [91/316], Loss: 0.0045\n",
      "Epoch [15/20], Step [92/316], Loss: 0.0086\n",
      "Epoch [15/20], Step [93/316], Loss: 0.0058\n",
      "Epoch [15/20], Step [94/316], Loss: 0.0026\n",
      "Epoch [15/20], Step [95/316], Loss: 0.0031\n",
      "Epoch [15/20], Step [96/316], Loss: 0.0851\n",
      "Epoch [15/20], Step [97/316], Loss: 0.0053\n",
      "Epoch [15/20], Step [98/316], Loss: 0.1121\n",
      "Epoch [15/20], Step [99/316], Loss: 0.0031\n",
      "Epoch [15/20], Step [100/316], Loss: 0.0029\n",
      "Epoch [15/20], Step [101/316], Loss: 0.0026\n",
      "Epoch [15/20], Step [102/316], Loss: 0.1196\n",
      "Epoch [15/20], Step [103/316], Loss: 0.0034\n",
      "Epoch [15/20], Step [104/316], Loss: 0.0057\n",
      "Epoch [15/20], Step [105/316], Loss: 0.0023\n",
      "Epoch [15/20], Step [106/316], Loss: 0.0066\n",
      "Epoch [15/20], Step [107/316], Loss: 0.0048\n",
      "Epoch [15/20], Step [108/316], Loss: 0.0029\n",
      "Epoch [15/20], Step [109/316], Loss: 0.0067\n",
      "Epoch [15/20], Step [110/316], Loss: 0.0020\n",
      "Epoch [15/20], Step [111/316], Loss: 0.0054\n",
      "Epoch [15/20], Step [112/316], Loss: 0.0040\n",
      "Epoch [15/20], Step [113/316], Loss: 0.0082\n",
      "Epoch [15/20], Step [114/316], Loss: 0.0038\n",
      "Epoch [15/20], Step [115/316], Loss: 0.0202\n",
      "Epoch [15/20], Step [116/316], Loss: 0.0024\n",
      "Epoch [15/20], Step [117/316], Loss: 0.0055\n",
      "Epoch [15/20], Step [118/316], Loss: 0.0059\n",
      "Epoch [15/20], Step [119/316], Loss: 0.0035\n",
      "Epoch [15/20], Step [120/316], Loss: 0.0522\n",
      "Epoch [15/20], Step [121/316], Loss: 0.0091\n",
      "Epoch [15/20], Step [122/316], Loss: 0.0027\n",
      "Epoch [15/20], Step [123/316], Loss: 0.0444\n",
      "Epoch [15/20], Step [124/316], Loss: 0.0341\n",
      "Epoch [15/20], Step [125/316], Loss: 0.0033\n",
      "Epoch [15/20], Step [126/316], Loss: 0.0039\n",
      "Epoch [15/20], Step [127/316], Loss: 0.0000\n",
      "Epoch [15/20], Step [128/316], Loss: 0.0054\n",
      "Epoch [15/20], Step [129/316], Loss: 0.0083\n",
      "Epoch [15/20], Step [130/316], Loss: 0.0017\n",
      "Epoch [15/20], Step [131/316], Loss: 0.1515\n",
      "Epoch [15/20], Step [132/316], Loss: 0.0059\n",
      "Epoch [15/20], Step [133/316], Loss: 0.0117\n",
      "Epoch [15/20], Step [134/316], Loss: 0.0038\n",
      "Epoch [15/20], Step [135/316], Loss: 0.0024\n",
      "Epoch [15/20], Step [136/316], Loss: 0.0073\n",
      "Epoch [15/20], Step [137/316], Loss: 0.0455\n",
      "Epoch [15/20], Step [138/316], Loss: 0.0566\n",
      "Epoch [15/20], Step [139/316], Loss: 0.0126\n",
      "Epoch [15/20], Step [140/316], Loss: 0.1837\n",
      "Epoch [15/20], Step [141/316], Loss: 0.0044\n",
      "Epoch [15/20], Step [142/316], Loss: 0.0044\n",
      "Epoch [15/20], Step [143/316], Loss: 0.0244\n",
      "Epoch [15/20], Step [144/316], Loss: 0.0130\n",
      "Epoch [15/20], Step [145/316], Loss: 0.0009\n",
      "Epoch [15/20], Step [146/316], Loss: 0.0035\n",
      "Epoch [15/20], Step [147/316], Loss: 0.0052\n",
      "Epoch [15/20], Step [148/316], Loss: 0.0119\n",
      "Epoch [15/20], Step [149/316], Loss: 0.0066\n",
      "Epoch [15/20], Step [150/316], Loss: 0.0049\n",
      "Epoch [15/20], Step [151/316], Loss: 0.0028\n",
      "Epoch [15/20], Step [152/316], Loss: 0.0103\n",
      "Epoch [15/20], Step [153/316], Loss: 0.0470\n",
      "Epoch [15/20], Step [154/316], Loss: 0.0069\n",
      "Epoch [15/20], Step [155/316], Loss: 0.0038\n",
      "Epoch [15/20], Step [156/316], Loss: 0.0954\n",
      "Epoch [15/20], Step [157/316], Loss: 0.0030\n",
      "Epoch [15/20], Step [158/316], Loss: 0.0028\n",
      "Epoch [15/20], Step [159/316], Loss: 0.0062\n",
      "Epoch [15/20], Step [160/316], Loss: 0.0206\n",
      "Epoch [15/20], Step [161/316], Loss: 0.0242\n",
      "Epoch [15/20], Step [162/316], Loss: 0.0072\n",
      "Epoch [15/20], Step [163/316], Loss: 0.0025\n",
      "Epoch [15/20], Step [164/316], Loss: 0.0425\n",
      "Epoch [15/20], Step [165/316], Loss: 0.1819\n",
      "Epoch [15/20], Step [166/316], Loss: 0.0021\n",
      "Epoch [15/20], Step [167/316], Loss: 0.0014\n",
      "Epoch [15/20], Step [168/316], Loss: 0.0061\n",
      "Epoch [15/20], Step [169/316], Loss: 0.0125\n",
      "Epoch [15/20], Step [170/316], Loss: 0.0748\n",
      "Epoch [15/20], Step [171/316], Loss: 0.0056\n",
      "Epoch [15/20], Step [172/316], Loss: 0.0125\n",
      "Epoch [15/20], Step [173/316], Loss: 0.0413\n",
      "Epoch [15/20], Step [174/316], Loss: 0.1421\n",
      "Epoch [15/20], Step [175/316], Loss: 0.0069\n",
      "Epoch [15/20], Step [176/316], Loss: 0.0046\n",
      "Epoch [15/20], Step [177/316], Loss: 0.0082\n",
      "Epoch [15/20], Step [178/316], Loss: 0.0061\n",
      "Epoch [15/20], Step [179/316], Loss: 0.0101\n",
      "Epoch [15/20], Step [180/316], Loss: 0.0100\n",
      "Epoch [15/20], Step [181/316], Loss: 0.0008\n",
      "Epoch [15/20], Step [182/316], Loss: 0.0046\n",
      "Epoch [15/20], Step [183/316], Loss: 0.0748\n",
      "Epoch [15/20], Step [184/316], Loss: 0.0025\n",
      "Epoch [15/20], Step [185/316], Loss: 0.1378\n",
      "Epoch [15/20], Step [186/316], Loss: 0.0072\n",
      "Epoch [15/20], Step [187/316], Loss: 0.0064\n",
      "Epoch [15/20], Step [188/316], Loss: 0.0072\n",
      "Epoch [15/20], Step [189/316], Loss: 0.1204\n",
      "Epoch [15/20], Step [190/316], Loss: 0.0034\n",
      "Epoch [15/20], Step [191/316], Loss: 0.0089\n",
      "Epoch [15/20], Step [192/316], Loss: 0.0027\n",
      "Epoch [15/20], Step [193/316], Loss: 0.0314\n",
      "Epoch [15/20], Step [194/316], Loss: 0.0084\n",
      "Epoch [15/20], Step [195/316], Loss: 0.0051\n",
      "Epoch [15/20], Step [196/316], Loss: 0.0733\n",
      "Epoch [15/20], Step [197/316], Loss: 0.0115\n",
      "Epoch [15/20], Step [198/316], Loss: 0.0540\n",
      "Epoch [15/20], Step [199/316], Loss: 0.0196\n",
      "Epoch [15/20], Step [200/316], Loss: 0.0167\n",
      "Epoch [15/20], Step [201/316], Loss: 0.0096\n",
      "Epoch [15/20], Step [202/316], Loss: 0.0318\n",
      "Epoch [15/20], Step [203/316], Loss: 0.0384\n",
      "Epoch [15/20], Step [204/316], Loss: 0.0082\n",
      "Epoch [15/20], Step [205/316], Loss: 0.0107\n",
      "Epoch [15/20], Step [206/316], Loss: 0.0058\n",
      "Epoch [15/20], Step [207/316], Loss: 0.0012\n",
      "Epoch [15/20], Step [208/316], Loss: 0.0071\n",
      "Epoch [15/20], Step [209/316], Loss: 0.2233\n",
      "Epoch [15/20], Step [210/316], Loss: 0.1252\n",
      "Epoch [15/20], Step [211/316], Loss: 0.0041\n",
      "Epoch [15/20], Step [212/316], Loss: 0.1178\n",
      "Epoch [15/20], Step [213/316], Loss: 0.0078\n",
      "Epoch [15/20], Step [214/316], Loss: 0.1231\n",
      "Epoch [15/20], Step [215/316], Loss: 0.0056\n",
      "Epoch [15/20], Step [216/316], Loss: 0.0130\n",
      "Epoch [15/20], Step [217/316], Loss: 0.2458\n",
      "Epoch [15/20], Step [218/316], Loss: 0.1171\n",
      "Epoch [15/20], Step [219/316], Loss: 0.0421\n",
      "Epoch [15/20], Step [220/316], Loss: 0.0277\n",
      "Epoch [15/20], Step [221/316], Loss: 0.0078\n",
      "Epoch [15/20], Step [222/316], Loss: 0.0211\n",
      "Epoch [15/20], Step [223/316], Loss: 0.1197\n",
      "Epoch [15/20], Step [224/316], Loss: 0.0007\n",
      "Epoch [15/20], Step [225/316], Loss: 0.0059\n",
      "Epoch [15/20], Step [226/316], Loss: 0.1224\n",
      "Epoch [15/20], Step [227/316], Loss: 0.2035\n",
      "Epoch [15/20], Step [228/316], Loss: 0.0027\n",
      "Epoch [15/20], Step [229/316], Loss: 0.0103\n",
      "Epoch [15/20], Step [230/316], Loss: 0.0276\n",
      "Epoch [15/20], Step [231/316], Loss: 0.0042\n",
      "Epoch [15/20], Step [232/316], Loss: 0.0218\n",
      "Epoch [15/20], Step [233/316], Loss: 0.0262\n",
      "Epoch [15/20], Step [234/316], Loss: 0.0074\n",
      "Epoch [15/20], Step [235/316], Loss: 0.0110\n",
      "Epoch [15/20], Step [236/316], Loss: 0.0016\n",
      "Epoch [15/20], Step [237/316], Loss: 0.0044\n",
      "Epoch [15/20], Step [238/316], Loss: 0.0806\n",
      "Epoch [15/20], Step [239/316], Loss: 0.0061\n",
      "Epoch [15/20], Step [240/316], Loss: 0.0089\n",
      "Epoch [15/20], Step [241/316], Loss: 0.0053\n",
      "Epoch [15/20], Step [242/316], Loss: 0.0149\n",
      "Epoch [15/20], Step [243/316], Loss: 0.0057\n",
      "Epoch [15/20], Step [244/316], Loss: 0.0070\n",
      "Epoch [15/20], Step [245/316], Loss: 0.0155\n",
      "Epoch [15/20], Step [246/316], Loss: 0.0076\n",
      "Epoch [15/20], Step [247/316], Loss: 0.0090\n",
      "Epoch [15/20], Step [248/316], Loss: 0.0092\n",
      "Epoch [15/20], Step [249/316], Loss: 0.0274\n",
      "Epoch [15/20], Step [250/316], Loss: 0.0249\n",
      "Epoch [15/20], Step [251/316], Loss: 0.0155\n",
      "Epoch [15/20], Step [252/316], Loss: 0.0017\n",
      "Epoch [15/20], Step [253/316], Loss: 0.0018\n",
      "Epoch [15/20], Step [254/316], Loss: 0.0246\n",
      "Epoch [15/20], Step [255/316], Loss: 0.0064\n",
      "Epoch [15/20], Step [256/316], Loss: 0.0093\n",
      "Epoch [15/20], Step [257/316], Loss: 0.0228\n",
      "Epoch [15/20], Step [258/316], Loss: 0.0104\n",
      "Epoch [15/20], Step [259/316], Loss: 0.0090\n",
      "Epoch [15/20], Step [260/316], Loss: 0.0053\n",
      "Epoch [15/20], Step [261/316], Loss: 0.0125\n",
      "Epoch [15/20], Step [262/316], Loss: 0.0038\n",
      "Epoch [15/20], Step [263/316], Loss: 0.0029\n",
      "Epoch [15/20], Step [264/316], Loss: 0.0241\n",
      "Epoch [15/20], Step [265/316], Loss: 0.0022\n",
      "Epoch [15/20], Step [266/316], Loss: 0.0030\n",
      "Epoch [15/20], Step [267/316], Loss: 0.1266\n",
      "Epoch [15/20], Step [268/316], Loss: 0.0209\n",
      "Epoch [15/20], Step [269/316], Loss: 0.0043\n",
      "Epoch [15/20], Step [270/316], Loss: 0.0130\n",
      "Epoch [15/20], Step [271/316], Loss: 0.0939\n",
      "Epoch [15/20], Step [272/316], Loss: 0.0066\n",
      "Epoch [15/20], Step [273/316], Loss: 0.0373\n",
      "Epoch [15/20], Step [274/316], Loss: 0.0063\n",
      "Epoch [15/20], Step [275/316], Loss: 0.0702\n",
      "Epoch [15/20], Step [276/316], Loss: 0.0782\n",
      "Epoch [15/20], Step [277/316], Loss: 0.0052\n",
      "Epoch [15/20], Step [278/316], Loss: 0.0112\n",
      "Epoch [15/20], Step [279/316], Loss: 0.0011\n",
      "Epoch [15/20], Step [280/316], Loss: 0.0066\n",
      "Epoch [15/20], Step [281/316], Loss: 0.0111\n",
      "Epoch [15/20], Step [282/316], Loss: 0.0678\n",
      "Epoch [15/20], Step [283/316], Loss: 0.0100\n",
      "Epoch [15/20], Step [284/316], Loss: 0.0045\n",
      "Epoch [15/20], Step [285/316], Loss: 0.0058\n",
      "Epoch [15/20], Step [286/316], Loss: 0.0022\n",
      "Epoch [15/20], Step [287/316], Loss: 0.0316\n",
      "Epoch [15/20], Step [288/316], Loss: 0.1028\n",
      "Epoch [15/20], Step [289/316], Loss: 0.0108\n",
      "Epoch [15/20], Step [290/316], Loss: 0.0073\n",
      "Epoch [15/20], Step [291/316], Loss: 0.0074\n",
      "Epoch [15/20], Step [292/316], Loss: 0.0164\n",
      "Epoch [15/20], Step [293/316], Loss: 0.0033\n",
      "Epoch [15/20], Step [294/316], Loss: 0.0253\n",
      "Epoch [15/20], Step [295/316], Loss: 0.0726\n",
      "Epoch [15/20], Step [296/316], Loss: 0.0030\n",
      "Epoch [15/20], Step [297/316], Loss: 0.0822\n",
      "Epoch [15/20], Step [298/316], Loss: 0.0120\n",
      "Epoch [15/20], Step [299/316], Loss: 0.0797\n",
      "Epoch [15/20], Step [300/316], Loss: 0.0028\n",
      "Epoch [15/20], Step [301/316], Loss: 0.0053\n",
      "Epoch [15/20], Step [302/316], Loss: 0.0185\n",
      "Epoch [15/20], Step [303/316], Loss: 0.0035\n",
      "Epoch [15/20], Step [304/316], Loss: 0.0938\n",
      "Epoch [15/20], Step [305/316], Loss: 0.0035\n",
      "Epoch [15/20], Step [306/316], Loss: 0.0129\n",
      "Epoch [15/20], Step [307/316], Loss: 0.0059\n",
      "Epoch [15/20], Step [308/316], Loss: 0.0061\n",
      "Epoch [15/20], Step [309/316], Loss: 0.0171\n",
      "Epoch [15/20], Step [310/316], Loss: 0.0188\n",
      "Epoch [15/20], Step [311/316], Loss: 0.0121\n",
      "Epoch [15/20], Step [312/316], Loss: 0.0134\n",
      "Epoch [15/20], Step [313/316], Loss: 0.0065\n",
      "Epoch [15/20], Step [314/316], Loss: 0.0509\n",
      "Epoch [15/20], Step [315/316], Loss: 0.0239\n",
      "Epoch [15/20], Step [316/316], Loss: 0.0040\n",
      "Epoch [15/20], Train Loss: 0.0288\n",
      "Epoch [15/20], Validation Loss: 0.1678\n",
      "Epoch [15/20], Validation Accuracy: 0.8083\n",
      "Epoch [16/20], Step [1/316], Loss: 0.0046\n",
      "Epoch [16/20], Step [2/316], Loss: 0.0068\n",
      "Epoch [16/20], Step [3/316], Loss: 0.0045\n",
      "Epoch [16/20], Step [4/316], Loss: 0.0057\n",
      "Epoch [16/20], Step [5/316], Loss: 0.0066\n",
      "Epoch [16/20], Step [6/316], Loss: 0.0098\n",
      "Epoch [16/20], Step [7/316], Loss: 0.0401\n",
      "Epoch [16/20], Step [8/316], Loss: 0.0054\n",
      "Epoch [16/20], Step [9/316], Loss: 0.0119\n",
      "Epoch [16/20], Step [10/316], Loss: 0.0029\n",
      "Epoch [16/20], Step [11/316], Loss: 0.0025\n",
      "Epoch [16/20], Step [12/316], Loss: 0.0407\n",
      "Epoch [16/20], Step [13/316], Loss: 0.0085\n",
      "Epoch [16/20], Step [14/316], Loss: 0.0076\n",
      "Epoch [16/20], Step [15/316], Loss: 0.1614\n",
      "Epoch [16/20], Step [16/316], Loss: 0.0019\n",
      "Epoch [16/20], Step [17/316], Loss: 0.1110\n",
      "Epoch [16/20], Step [18/316], Loss: 0.0116\n",
      "Epoch [16/20], Step [19/316], Loss: 0.0119\n",
      "Epoch [16/20], Step [20/316], Loss: 0.0008\n",
      "Epoch [16/20], Step [21/316], Loss: 0.0952\n",
      "Epoch [16/20], Step [22/316], Loss: 0.0071\n",
      "Epoch [16/20], Step [23/316], Loss: 0.0502\n",
      "Epoch [16/20], Step [24/316], Loss: 0.0036\n",
      "Epoch [16/20], Step [25/316], Loss: 0.1030\n",
      "Epoch [16/20], Step [26/316], Loss: 0.0949\n",
      "Epoch [16/20], Step [27/316], Loss: 0.0036\n",
      "Epoch [16/20], Step [28/316], Loss: 0.0125\n",
      "Epoch [16/20], Step [29/316], Loss: 0.1317\n",
      "Epoch [16/20], Step [30/316], Loss: 0.0018\n",
      "Epoch [16/20], Step [31/316], Loss: 0.0011\n",
      "Epoch [16/20], Step [32/316], Loss: 0.0104\n",
      "Epoch [16/20], Step [33/316], Loss: 0.0026\n",
      "Epoch [16/20], Step [34/316], Loss: 0.0036\n",
      "Epoch [16/20], Step [35/316], Loss: 0.0021\n",
      "Epoch [16/20], Step [36/316], Loss: 0.0044\n",
      "Epoch [16/20], Step [37/316], Loss: 0.0154\n",
      "Epoch [16/20], Step [38/316], Loss: 0.0046\n",
      "Epoch [16/20], Step [39/316], Loss: 0.0858\n",
      "Epoch [16/20], Step [40/316], Loss: 0.0139\n",
      "Epoch [16/20], Step [41/316], Loss: 0.1018\n",
      "Epoch [16/20], Step [42/316], Loss: 0.0305\n",
      "Epoch [16/20], Step [43/316], Loss: 0.0256\n",
      "Epoch [16/20], Step [44/316], Loss: 0.0661\n",
      "Epoch [16/20], Step [45/316], Loss: 0.0807\n",
      "Epoch [16/20], Step [46/316], Loss: 0.0440\n",
      "Epoch [16/20], Step [47/316], Loss: 0.0104\n",
      "Epoch [16/20], Step [48/316], Loss: 0.0819\n",
      "Epoch [16/20], Step [49/316], Loss: 0.0029\n",
      "Epoch [16/20], Step [50/316], Loss: 0.0189\n",
      "Epoch [16/20], Step [51/316], Loss: 0.1235\n",
      "Epoch [16/20], Step [52/316], Loss: 0.0018\n",
      "Epoch [16/20], Step [53/316], Loss: 0.0448\n",
      "Epoch [16/20], Step [54/316], Loss: 0.0028\n",
      "Epoch [16/20], Step [55/316], Loss: 0.0058\n",
      "Epoch [16/20], Step [56/316], Loss: 0.0121\n",
      "Epoch [16/20], Step [57/316], Loss: 0.0018\n",
      "Epoch [16/20], Step [58/316], Loss: 0.0014\n",
      "Epoch [16/20], Step [59/316], Loss: 0.0096\n",
      "Epoch [16/20], Step [60/316], Loss: 0.0141\n",
      "Epoch [16/20], Step [61/316], Loss: 0.0037\n",
      "Epoch [16/20], Step [62/316], Loss: 0.0039\n",
      "Epoch [16/20], Step [63/316], Loss: 0.0103\n",
      "Epoch [16/20], Step [64/316], Loss: 0.0044\n",
      "Epoch [16/20], Step [65/316], Loss: 0.0014\n",
      "Epoch [16/20], Step [66/316], Loss: 0.0671\n",
      "Epoch [16/20], Step [67/316], Loss: 0.0077\n",
      "Epoch [16/20], Step [68/316], Loss: 0.0037\n",
      "Epoch [16/20], Step [69/316], Loss: 0.0012\n",
      "Epoch [16/20], Step [70/316], Loss: 0.1406\n",
      "Epoch [16/20], Step [71/316], Loss: 0.0462\n",
      "Epoch [16/20], Step [72/316], Loss: 0.0500\n",
      "Epoch [16/20], Step [73/316], Loss: 0.0042\n",
      "Epoch [16/20], Step [74/316], Loss: 0.0097\n",
      "Epoch [16/20], Step [75/316], Loss: 0.0065\n",
      "Epoch [16/20], Step [76/316], Loss: 0.0108\n",
      "Epoch [16/20], Step [77/316], Loss: 0.0054\n",
      "Epoch [16/20], Step [78/316], Loss: 0.0013\n",
      "Epoch [16/20], Step [79/316], Loss: 0.0089\n",
      "Epoch [16/20], Step [80/316], Loss: 0.1790\n",
      "Epoch [16/20], Step [81/316], Loss: 0.0094\n",
      "Epoch [16/20], Step [82/316], Loss: 0.0037\n",
      "Epoch [16/20], Step [83/316], Loss: 0.0057\n",
      "Epoch [16/20], Step [84/316], Loss: 0.0045\n",
      "Epoch [16/20], Step [85/316], Loss: 0.0073\n",
      "Epoch [16/20], Step [86/316], Loss: 0.0056\n",
      "Epoch [16/20], Step [87/316], Loss: 0.1045\n",
      "Epoch [16/20], Step [88/316], Loss: 0.0060\n",
      "Epoch [16/20], Step [89/316], Loss: 0.0127\n",
      "Epoch [16/20], Step [90/316], Loss: 0.0879\n",
      "Epoch [16/20], Step [91/316], Loss: 0.0030\n",
      "Epoch [16/20], Step [92/316], Loss: 0.0039\n",
      "Epoch [16/20], Step [93/316], Loss: 0.0027\n",
      "Epoch [16/20], Step [94/316], Loss: 0.0014\n",
      "Epoch [16/20], Step [95/316], Loss: 0.0064\n",
      "Epoch [16/20], Step [96/316], Loss: 0.0944\n",
      "Epoch [16/20], Step [97/316], Loss: 0.0024\n",
      "Epoch [16/20], Step [98/316], Loss: 0.0692\n",
      "Epoch [16/20], Step [99/316], Loss: 0.0110\n",
      "Epoch [16/20], Step [100/316], Loss: 0.1941\n",
      "Epoch [16/20], Step [101/316], Loss: 0.0005\n",
      "Epoch [16/20], Step [102/316], Loss: 0.0019\n",
      "Epoch [16/20], Step [103/316], Loss: 0.1226\n",
      "Epoch [16/20], Step [104/316], Loss: 0.0043\n",
      "Epoch [16/20], Step [105/316], Loss: 0.2090\n",
      "Epoch [16/20], Step [106/316], Loss: 0.0297\n",
      "Epoch [16/20], Step [107/316], Loss: 0.0028\n",
      "Epoch [16/20], Step [108/316], Loss: 0.0019\n",
      "Epoch [16/20], Step [109/316], Loss: 0.0052\n",
      "Epoch [16/20], Step [110/316], Loss: 0.0264\n",
      "Epoch [16/20], Step [111/316], Loss: 0.1157\n",
      "Epoch [16/20], Step [112/316], Loss: 0.0057\n",
      "Epoch [16/20], Step [113/316], Loss: 0.0064\n",
      "Epoch [16/20], Step [114/316], Loss: 0.0024\n",
      "Epoch [16/20], Step [115/316], Loss: 0.0082\n",
      "Epoch [16/20], Step [116/316], Loss: 0.0009\n",
      "Epoch [16/20], Step [117/316], Loss: 0.0050\n",
      "Epoch [16/20], Step [118/316], Loss: 0.0018\n",
      "Epoch [16/20], Step [119/316], Loss: 0.0010\n",
      "Epoch [16/20], Step [120/316], Loss: 0.0030\n",
      "Epoch [16/20], Step [121/316], Loss: 0.0061\n",
      "Epoch [16/20], Step [122/316], Loss: 0.0034\n",
      "Epoch [16/20], Step [123/316], Loss: 0.0025\n",
      "Epoch [16/20], Step [124/316], Loss: 0.1397\n",
      "Epoch [16/20], Step [125/316], Loss: 0.0033\n",
      "Epoch [16/20], Step [126/316], Loss: 0.1407\n",
      "Epoch [16/20], Step [127/316], Loss: 0.0426\n",
      "Epoch [16/20], Step [128/316], Loss: 0.0012\n",
      "Epoch [16/20], Step [129/316], Loss: 0.0023\n",
      "Epoch [16/20], Step [130/316], Loss: 0.0036\n",
      "Epoch [16/20], Step [131/316], Loss: 0.1468\n",
      "Epoch [16/20], Step [132/316], Loss: 0.0099\n",
      "Epoch [16/20], Step [133/316], Loss: 0.0047\n",
      "Epoch [16/20], Step [134/316], Loss: 0.0022\n",
      "Epoch [16/20], Step [135/316], Loss: 0.2429\n",
      "Epoch [16/20], Step [136/316], Loss: 0.0081\n",
      "Epoch [16/20], Step [137/316], Loss: 0.0055\n",
      "Epoch [16/20], Step [138/316], Loss: 0.0026\n",
      "Epoch [16/20], Step [139/316], Loss: 0.0043\n",
      "Epoch [16/20], Step [140/316], Loss: 0.0150\n",
      "Epoch [16/20], Step [141/316], Loss: 0.0044\n",
      "Epoch [16/20], Step [142/316], Loss: 0.0042\n",
      "Epoch [16/20], Step [143/316], Loss: 0.0206\n",
      "Epoch [16/20], Step [144/316], Loss: 0.0032\n",
      "Epoch [16/20], Step [145/316], Loss: 0.0024\n",
      "Epoch [16/20], Step [146/316], Loss: 0.0033\n",
      "Epoch [16/20], Step [147/316], Loss: 0.0063\n",
      "Epoch [16/20], Step [148/316], Loss: 0.0048\n",
      "Epoch [16/20], Step [149/316], Loss: 0.0024\n",
      "Epoch [16/20], Step [150/316], Loss: 0.0040\n",
      "Epoch [16/20], Step [151/316], Loss: 0.0056\n",
      "Epoch [16/20], Step [152/316], Loss: 0.0024\n",
      "Epoch [16/20], Step [153/316], Loss: 0.0033\n",
      "Epoch [16/20], Step [154/316], Loss: 0.0216\n",
      "Epoch [16/20], Step [155/316], Loss: 0.0069\n",
      "Epoch [16/20], Step [156/316], Loss: 0.1328\n",
      "Epoch [16/20], Step [157/316], Loss: 0.0030\n",
      "Epoch [16/20], Step [158/316], Loss: 0.0427\n",
      "Epoch [16/20], Step [159/316], Loss: 0.0025\n",
      "Epoch [16/20], Step [160/316], Loss: 0.0064\n",
      "Epoch [16/20], Step [161/316], Loss: 0.1322\n",
      "Epoch [16/20], Step [162/316], Loss: 0.0070\n",
      "Epoch [16/20], Step [163/316], Loss: 0.0025\n",
      "Epoch [16/20], Step [164/316], Loss: 0.0030\n",
      "Epoch [16/20], Step [165/316], Loss: 0.0046\n",
      "Epoch [16/20], Step [166/316], Loss: 0.0003\n",
      "Epoch [16/20], Step [167/316], Loss: 0.0011\n",
      "Epoch [16/20], Step [168/316], Loss: 0.0150\n",
      "Epoch [16/20], Step [169/316], Loss: 0.0003\n",
      "Epoch [16/20], Step [170/316], Loss: 0.0049\n",
      "Epoch [16/20], Step [171/316], Loss: 0.0018\n",
      "Epoch [16/20], Step [172/316], Loss: 0.0058\n",
      "Epoch [16/20], Step [173/316], Loss: 0.1117\n",
      "Epoch [16/20], Step [174/316], Loss: 0.0028\n",
      "Epoch [16/20], Step [175/316], Loss: 0.1268\n",
      "Epoch [16/20], Step [176/316], Loss: 0.0896\n",
      "Epoch [16/20], Step [177/316], Loss: 0.0019\n",
      "Epoch [16/20], Step [178/316], Loss: 0.0051\n",
      "Epoch [16/20], Step [179/316], Loss: 0.1060\n",
      "Epoch [16/20], Step [180/316], Loss: 0.0187\n",
      "Epoch [16/20], Step [181/316], Loss: 0.0083\n",
      "Epoch [16/20], Step [182/316], Loss: 0.0560\n",
      "Epoch [16/20], Step [183/316], Loss: 0.0025\n",
      "Epoch [16/20], Step [184/316], Loss: 0.0040\n",
      "Epoch [16/20], Step [185/316], Loss: 0.0414\n",
      "Epoch [16/20], Step [186/316], Loss: 0.0026\n",
      "Epoch [16/20], Step [187/316], Loss: 0.0008\n",
      "Epoch [16/20], Step [188/316], Loss: 0.0100\n",
      "Epoch [16/20], Step [189/316], Loss: 0.0175\n",
      "Epoch [16/20], Step [190/316], Loss: 0.0035\n",
      "Epoch [16/20], Step [191/316], Loss: 0.0087\n",
      "Epoch [16/20], Step [192/316], Loss: 0.0073\n",
      "Epoch [16/20], Step [193/316], Loss: 0.0104\n",
      "Epoch [16/20], Step [194/316], Loss: 0.0075\n",
      "Epoch [16/20], Step [195/316], Loss: 0.0310\n",
      "Epoch [16/20], Step [196/316], Loss: 0.0106\n",
      "Epoch [16/20], Step [197/316], Loss: 0.2483\n",
      "Epoch [16/20], Step [198/316], Loss: 0.0230\n",
      "Epoch [16/20], Step [199/316], Loss: 0.0045\n",
      "Epoch [16/20], Step [200/316], Loss: 0.0046\n",
      "Epoch [16/20], Step [201/316], Loss: 0.0081\n",
      "Epoch [16/20], Step [202/316], Loss: 0.0077\n",
      "Epoch [16/20], Step [203/316], Loss: 0.0168\n",
      "Epoch [16/20], Step [204/316], Loss: 0.0978\n",
      "Epoch [16/20], Step [205/316], Loss: 0.0087\n",
      "Epoch [16/20], Step [206/316], Loss: 0.0073\n",
      "Epoch [16/20], Step [207/316], Loss: 0.0289\n",
      "Epoch [16/20], Step [208/316], Loss: 0.0055\n",
      "Epoch [16/20], Step [209/316], Loss: 0.0077\n",
      "Epoch [16/20], Step [210/316], Loss: 0.0065\n",
      "Epoch [16/20], Step [211/316], Loss: 0.0092\n",
      "Epoch [16/20], Step [212/316], Loss: 0.0111\n",
      "Epoch [16/20], Step [213/316], Loss: 0.0245\n",
      "Epoch [16/20], Step [214/316], Loss: 0.1373\n",
      "Epoch [16/20], Step [215/316], Loss: 0.0040\n",
      "Epoch [16/20], Step [216/316], Loss: 0.1456\n",
      "Epoch [16/20], Step [217/316], Loss: 0.0050\n",
      "Epoch [16/20], Step [218/316], Loss: 0.0029\n",
      "Epoch [16/20], Step [219/316], Loss: 0.0066\n",
      "Epoch [16/20], Step [220/316], Loss: 0.0366\n",
      "Epoch [16/20], Step [221/316], Loss: 0.0273\n",
      "Epoch [16/20], Step [222/316], Loss: 0.0402\n",
      "Epoch [16/20], Step [223/316], Loss: 0.0068\n",
      "Epoch [16/20], Step [224/316], Loss: 0.0073\n",
      "Epoch [16/20], Step [225/316], Loss: 0.0038\n",
      "Epoch [16/20], Step [226/316], Loss: 0.0687\n",
      "Epoch [16/20], Step [227/316], Loss: 0.0033\n",
      "Epoch [16/20], Step [228/316], Loss: 0.2827\n",
      "Epoch [16/20], Step [229/316], Loss: 0.0025\n",
      "Epoch [16/20], Step [230/316], Loss: 0.0038\n",
      "Epoch [16/20], Step [231/316], Loss: 0.1632\n",
      "Epoch [16/20], Step [232/316], Loss: 0.0816\n",
      "Epoch [16/20], Step [233/316], Loss: 0.0045\n",
      "Epoch [16/20], Step [234/316], Loss: 0.1034\n",
      "Epoch [16/20], Step [235/316], Loss: 0.0077\n",
      "Epoch [16/20], Step [236/316], Loss: 0.0024\n",
      "Epoch [16/20], Step [237/316], Loss: 0.0076\n",
      "Epoch [16/20], Step [238/316], Loss: 0.0116\n",
      "Epoch [16/20], Step [239/316], Loss: 0.0022\n",
      "Epoch [16/20], Step [240/316], Loss: 0.0075\n",
      "Epoch [16/20], Step [241/316], Loss: 0.0039\n",
      "Epoch [16/20], Step [242/316], Loss: 0.0045\n",
      "Epoch [16/20], Step [243/316], Loss: 0.1097\n",
      "Epoch [16/20], Step [244/316], Loss: 0.2331\n",
      "Epoch [16/20], Step [245/316], Loss: 0.0046\n",
      "Epoch [16/20], Step [246/316], Loss: 0.0023\n",
      "Epoch [16/20], Step [247/316], Loss: 0.0021\n",
      "Epoch [16/20], Step [248/316], Loss: 0.0106\n",
      "Epoch [16/20], Step [249/316], Loss: 0.0981\n",
      "Epoch [16/20], Step [250/316], Loss: 0.0059\n",
      "Epoch [16/20], Step [251/316], Loss: 0.0059\n",
      "Epoch [16/20], Step [252/316], Loss: 0.0045\n",
      "Epoch [16/20], Step [253/316], Loss: 0.0536\n",
      "Epoch [16/20], Step [254/316], Loss: 0.0203\n",
      "Epoch [16/20], Step [255/316], Loss: 0.0095\n",
      "Epoch [16/20], Step [256/316], Loss: 0.0058\n",
      "Epoch [16/20], Step [257/316], Loss: 0.0913\n",
      "Epoch [16/20], Step [258/316], Loss: 0.0996\n",
      "Epoch [16/20], Step [259/316], Loss: 0.0037\n",
      "Epoch [16/20], Step [260/316], Loss: 0.0052\n",
      "Epoch [16/20], Step [261/316], Loss: 0.0740\n",
      "Epoch [16/20], Step [262/316], Loss: 0.0076\n",
      "Epoch [16/20], Step [263/316], Loss: 0.0069\n",
      "Epoch [16/20], Step [264/316], Loss: 0.0037\n",
      "Epoch [16/20], Step [265/316], Loss: 0.0277\n",
      "Epoch [16/20], Step [266/316], Loss: 0.0081\n",
      "Epoch [16/20], Step [267/316], Loss: 0.0076\n",
      "Epoch [16/20], Step [268/316], Loss: 0.0443\n",
      "Epoch [16/20], Step [269/316], Loss: 0.0051\n",
      "Epoch [16/20], Step [270/316], Loss: 0.0035\n",
      "Epoch [16/20], Step [271/316], Loss: 0.0958\n",
      "Epoch [16/20], Step [272/316], Loss: 0.0086\n",
      "Epoch [16/20], Step [273/316], Loss: 0.0849\n",
      "Epoch [16/20], Step [274/316], Loss: 0.0070\n",
      "Epoch [16/20], Step [275/316], Loss: 0.0090\n",
      "Epoch [16/20], Step [276/316], Loss: 0.0107\n",
      "Epoch [16/20], Step [277/316], Loss: 0.1337\n",
      "Epoch [16/20], Step [278/316], Loss: 0.0387\n",
      "Epoch [16/20], Step [279/316], Loss: 0.0060\n",
      "Epoch [16/20], Step [280/316], Loss: 0.0060\n",
      "Epoch [16/20], Step [281/316], Loss: 0.0157\n",
      "Epoch [16/20], Step [282/316], Loss: 0.0089\n",
      "Epoch [16/20], Step [283/316], Loss: 0.0148\n",
      "Epoch [16/20], Step [284/316], Loss: 0.0029\n",
      "Epoch [16/20], Step [285/316], Loss: 0.0103\n",
      "Epoch [16/20], Step [286/316], Loss: 0.0074\n",
      "Epoch [16/20], Step [287/316], Loss: 0.0255\n",
      "Epoch [16/20], Step [288/316], Loss: 0.0081\n",
      "Epoch [16/20], Step [289/316], Loss: 0.0172\n",
      "Epoch [16/20], Step [290/316], Loss: 0.0398\n",
      "Epoch [16/20], Step [291/316], Loss: 0.0025\n",
      "Epoch [16/20], Step [292/316], Loss: 0.0773\n",
      "Epoch [16/20], Step [293/316], Loss: 0.1057\n",
      "Epoch [16/20], Step [294/316], Loss: 0.1905\n",
      "Epoch [16/20], Step [295/316], Loss: 0.0162\n",
      "Epoch [16/20], Step [296/316], Loss: 0.0058\n",
      "Epoch [16/20], Step [297/316], Loss: 0.0425\n",
      "Epoch [16/20], Step [298/316], Loss: 0.0060\n",
      "Epoch [16/20], Step [299/316], Loss: 0.0032\n",
      "Epoch [16/20], Step [300/316], Loss: 0.0138\n",
      "Epoch [16/20], Step [301/316], Loss: 0.0107\n",
      "Epoch [16/20], Step [302/316], Loss: 0.0023\n",
      "Epoch [16/20], Step [303/316], Loss: 0.0912\n",
      "Epoch [16/20], Step [304/316], Loss: 0.0072\n",
      "Epoch [16/20], Step [305/316], Loss: 0.0072\n",
      "Epoch [16/20], Step [306/316], Loss: 0.0835\n",
      "Epoch [16/20], Step [307/316], Loss: 0.0012\n",
      "Epoch [16/20], Step [308/316], Loss: 0.0307\n",
      "Epoch [16/20], Step [309/316], Loss: 0.0037\n",
      "Epoch [16/20], Step [310/316], Loss: 0.0232\n",
      "Epoch [16/20], Step [311/316], Loss: 0.1298\n",
      "Epoch [16/20], Step [312/316], Loss: 0.1271\n",
      "Epoch [16/20], Step [313/316], Loss: 0.0068\n",
      "Epoch [16/20], Step [314/316], Loss: 0.0023\n",
      "Epoch [16/20], Step [315/316], Loss: 0.0088\n",
      "Epoch [16/20], Step [316/316], Loss: 0.0053\n",
      "Epoch [16/20], Train Loss: 0.0307\n",
      "Epoch [16/20], Validation Loss: 0.1694\n",
      "Epoch [16/20], Validation Accuracy: 0.8128\n",
      "Epoch [17/20], Step [1/316], Loss: 0.1302\n",
      "Epoch [17/20], Step [2/316], Loss: 0.0088\n",
      "Epoch [17/20], Step [3/316], Loss: 0.2997\n",
      "Epoch [17/20], Step [4/316], Loss: 0.0263\n",
      "Epoch [17/20], Step [5/316], Loss: 0.0021\n",
      "Epoch [17/20], Step [6/316], Loss: 0.0076\n",
      "Epoch [17/20], Step [7/316], Loss: 0.1258\n",
      "Epoch [17/20], Step [8/316], Loss: 0.0166\n",
      "Epoch [17/20], Step [9/316], Loss: 0.1316\n",
      "Epoch [17/20], Step [10/316], Loss: 0.0069\n",
      "Epoch [17/20], Step [11/316], Loss: 0.0077\n",
      "Epoch [17/20], Step [12/316], Loss: 0.0105\n",
      "Epoch [17/20], Step [13/316], Loss: 0.0049\n",
      "Epoch [17/20], Step [14/316], Loss: 0.0094\n",
      "Epoch [17/20], Step [15/316], Loss: 0.0165\n",
      "Epoch [17/20], Step [16/316], Loss: 0.0095\n",
      "Epoch [17/20], Step [17/316], Loss: 0.0308\n",
      "Epoch [17/20], Step [18/316], Loss: 0.1186\n",
      "Epoch [17/20], Step [19/316], Loss: 0.0067\n",
      "Epoch [17/20], Step [20/316], Loss: 0.0139\n",
      "Epoch [17/20], Step [21/316], Loss: 0.0052\n",
      "Epoch [17/20], Step [22/316], Loss: 0.0811\n",
      "Epoch [17/20], Step [23/316], Loss: 0.0149\n",
      "Epoch [17/20], Step [24/316], Loss: 0.0055\n",
      "Epoch [17/20], Step [25/316], Loss: 0.0090\n",
      "Epoch [17/20], Step [26/316], Loss: 0.0040\n",
      "Epoch [17/20], Step [27/316], Loss: 0.1662\n",
      "Epoch [17/20], Step [28/316], Loss: 0.0347\n",
      "Epoch [17/20], Step [29/316], Loss: 0.0046\n",
      "Epoch [17/20], Step [30/316], Loss: 0.0057\n",
      "Epoch [17/20], Step [31/316], Loss: 0.0196\n",
      "Epoch [17/20], Step [32/316], Loss: 0.0036\n",
      "Epoch [17/20], Step [33/316], Loss: 0.0472\n",
      "Epoch [17/20], Step [34/316], Loss: 0.0022\n",
      "Epoch [17/20], Step [35/316], Loss: 0.0358\n",
      "Epoch [17/20], Step [36/316], Loss: 0.0033\n",
      "Epoch [17/20], Step [37/316], Loss: 0.0208\n",
      "Epoch [17/20], Step [38/316], Loss: 0.0025\n",
      "Epoch [17/20], Step [39/316], Loss: 0.0063\n",
      "Epoch [17/20], Step [40/316], Loss: 0.0055\n",
      "Epoch [17/20], Step [41/316], Loss: 0.0081\n",
      "Epoch [17/20], Step [42/316], Loss: 0.0118\n",
      "Epoch [17/20], Step [43/316], Loss: 0.1131\n",
      "Epoch [17/20], Step [44/316], Loss: 0.1327\n",
      "Epoch [17/20], Step [45/316], Loss: 0.0045\n",
      "Epoch [17/20], Step [46/316], Loss: 0.0077\n",
      "Epoch [17/20], Step [47/316], Loss: 0.0086\n",
      "Epoch [17/20], Step [48/316], Loss: 0.0095\n",
      "Epoch [17/20], Step [49/316], Loss: 0.0222\n",
      "Epoch [17/20], Step [50/316], Loss: 0.0023\n",
      "Epoch [17/20], Step [51/316], Loss: 0.0054\n",
      "Epoch [17/20], Step [52/316], Loss: 0.0065\n",
      "Epoch [17/20], Step [53/316], Loss: 0.1006\n",
      "Epoch [17/20], Step [54/316], Loss: 0.0080\n",
      "Epoch [17/20], Step [55/316], Loss: 0.0063\n",
      "Epoch [17/20], Step [56/316], Loss: 0.0058\n",
      "Epoch [17/20], Step [57/316], Loss: 0.0028\n",
      "Epoch [17/20], Step [58/316], Loss: 0.0030\n",
      "Epoch [17/20], Step [59/316], Loss: 0.0051\n",
      "Epoch [17/20], Step [60/316], Loss: 0.0029\n",
      "Epoch [17/20], Step [61/316], Loss: 0.0096\n",
      "Epoch [17/20], Step [62/316], Loss: 0.0026\n",
      "Epoch [17/20], Step [63/316], Loss: 0.0139\n",
      "Epoch [17/20], Step [64/316], Loss: 0.0024\n",
      "Epoch [17/20], Step [65/316], Loss: 0.0061\n",
      "Epoch [17/20], Step [66/316], Loss: 0.0098\n",
      "Epoch [17/20], Step [67/316], Loss: 0.0051\n",
      "Epoch [17/20], Step [68/316], Loss: 0.0083\n",
      "Epoch [17/20], Step [69/316], Loss: 0.0036\n",
      "Epoch [17/20], Step [70/316], Loss: 0.0427\n",
      "Epoch [17/20], Step [71/316], Loss: 0.0035\n",
      "Epoch [17/20], Step [72/316], Loss: 0.0069\n",
      "Epoch [17/20], Step [73/316], Loss: 0.0065\n",
      "Epoch [17/20], Step [74/316], Loss: 0.0039\n",
      "Epoch [17/20], Step [75/316], Loss: 0.0019\n",
      "Epoch [17/20], Step [76/316], Loss: 0.0053\n",
      "Epoch [17/20], Step [77/316], Loss: 0.0105\n",
      "Epoch [17/20], Step [78/316], Loss: 0.0051\n",
      "Epoch [17/20], Step [79/316], Loss: 0.0020\n",
      "Epoch [17/20], Step [80/316], Loss: 0.0109\n",
      "Epoch [17/20], Step [81/316], Loss: 0.0091\n",
      "Epoch [17/20], Step [82/316], Loss: 0.0047\n",
      "Epoch [17/20], Step [83/316], Loss: 0.0017\n",
      "Epoch [17/20], Step [84/316], Loss: 0.1877\n",
      "Epoch [17/20], Step [85/316], Loss: 0.0041\n",
      "Epoch [17/20], Step [86/316], Loss: 0.0114\n",
      "Epoch [17/20], Step [87/316], Loss: 0.0028\n",
      "Epoch [17/20], Step [88/316], Loss: 0.1235\n",
      "Epoch [17/20], Step [89/316], Loss: 0.0093\n",
      "Epoch [17/20], Step [90/316], Loss: 0.0136\n",
      "Epoch [17/20], Step [91/316], Loss: 0.0019\n",
      "Epoch [17/20], Step [92/316], Loss: 0.0027\n",
      "Epoch [17/20], Step [93/316], Loss: 0.0026\n",
      "Epoch [17/20], Step [94/316], Loss: 0.0035\n",
      "Epoch [17/20], Step [95/316], Loss: 0.1071\n",
      "Epoch [17/20], Step [96/316], Loss: 0.0018\n",
      "Epoch [17/20], Step [97/316], Loss: 0.0063\n",
      "Epoch [17/20], Step [98/316], Loss: 0.0033\n",
      "Epoch [17/20], Step [99/316], Loss: 0.0012\n",
      "Epoch [17/20], Step [100/316], Loss: 0.0121\n",
      "Epoch [17/20], Step [101/316], Loss: 0.0147\n",
      "Epoch [17/20], Step [102/316], Loss: 0.0049\n",
      "Epoch [17/20], Step [103/316], Loss: 0.0038\n",
      "Epoch [17/20], Step [104/316], Loss: 0.0435\n",
      "Epoch [17/20], Step [105/316], Loss: 0.0745\n",
      "Epoch [17/20], Step [106/316], Loss: 0.1299\n",
      "Epoch [17/20], Step [107/316], Loss: 0.0066\n",
      "Epoch [17/20], Step [108/316], Loss: 0.0053\n",
      "Epoch [17/20], Step [109/316], Loss: 0.0022\n",
      "Epoch [17/20], Step [110/316], Loss: 0.0165\n",
      "Epoch [17/20], Step [111/316], Loss: 0.0095\n",
      "Epoch [17/20], Step [112/316], Loss: 0.0207\n",
      "Epoch [17/20], Step [113/316], Loss: 0.0020\n",
      "Epoch [17/20], Step [114/316], Loss: 0.0061\n",
      "Epoch [17/20], Step [115/316], Loss: 0.0053\n",
      "Epoch [17/20], Step [116/316], Loss: 0.0075\n",
      "Epoch [17/20], Step [117/316], Loss: 0.0060\n",
      "Epoch [17/20], Step [118/316], Loss: 0.0219\n",
      "Epoch [17/20], Step [119/316], Loss: 0.0413\n",
      "Epoch [17/20], Step [120/316], Loss: 0.0037\n",
      "Epoch [17/20], Step [121/316], Loss: 0.0245\n",
      "Epoch [17/20], Step [122/316], Loss: 0.0173\n",
      "Epoch [17/20], Step [123/316], Loss: 0.0388\n",
      "Epoch [17/20], Step [124/316], Loss: 0.0090\n",
      "Epoch [17/20], Step [125/316], Loss: 0.0434\n",
      "Epoch [17/20], Step [126/316], Loss: 0.0028\n",
      "Epoch [17/20], Step [127/316], Loss: 0.0013\n",
      "Epoch [17/20], Step [128/316], Loss: 0.0051\n",
      "Epoch [17/20], Step [129/316], Loss: 0.0036\n",
      "Epoch [17/20], Step [130/316], Loss: 0.0044\n",
      "Epoch [17/20], Step [131/316], Loss: 0.1244\n",
      "Epoch [17/20], Step [132/316], Loss: 0.0062\n",
      "Epoch [17/20], Step [133/316], Loss: 0.0246\n",
      "Epoch [17/20], Step [134/316], Loss: 0.1564\n",
      "Epoch [17/20], Step [135/316], Loss: 0.0831\n",
      "Epoch [17/20], Step [136/316], Loss: 0.0023\n",
      "Epoch [17/20], Step [137/316], Loss: 0.0029\n",
      "Epoch [17/20], Step [138/316], Loss: 0.0031\n",
      "Epoch [17/20], Step [139/316], Loss: 0.0050\n",
      "Epoch [17/20], Step [140/316], Loss: 0.0022\n",
      "Epoch [17/20], Step [141/316], Loss: 0.0058\n",
      "Epoch [17/20], Step [142/316], Loss: 0.0060\n",
      "Epoch [17/20], Step [143/316], Loss: 0.1372\n",
      "Epoch [17/20], Step [144/316], Loss: 0.0686\n",
      "Epoch [17/20], Step [145/316], Loss: 0.0061\n",
      "Epoch [17/20], Step [146/316], Loss: 0.0107\n",
      "Epoch [17/20], Step [147/316], Loss: 0.0146\n",
      "Epoch [17/20], Step [148/316], Loss: 0.0013\n",
      "Epoch [17/20], Step [149/316], Loss: 0.0403\n",
      "Epoch [17/20], Step [150/316], Loss: 0.0050\n",
      "Epoch [17/20], Step [151/316], Loss: 0.0079\n",
      "Epoch [17/20], Step [152/316], Loss: 0.0052\n",
      "Epoch [17/20], Step [153/316], Loss: 0.0001\n",
      "Epoch [17/20], Step [154/316], Loss: 0.1219\n",
      "Epoch [17/20], Step [155/316], Loss: 0.0067\n",
      "Epoch [17/20], Step [156/316], Loss: 0.0022\n",
      "Epoch [17/20], Step [157/316], Loss: 0.0056\n",
      "Epoch [17/20], Step [158/316], Loss: 0.0050\n",
      "Epoch [17/20], Step [159/316], Loss: 0.2135\n",
      "Epoch [17/20], Step [160/316], Loss: 0.0057\n",
      "Epoch [17/20], Step [161/316], Loss: 0.0022\n",
      "Epoch [17/20], Step [162/316], Loss: 0.0079\n",
      "Epoch [17/20], Step [163/316], Loss: 0.0022\n",
      "Epoch [17/20], Step [164/316], Loss: 0.0605\n",
      "Epoch [17/20], Step [165/316], Loss: 0.0009\n",
      "Epoch [17/20], Step [166/316], Loss: 0.0058\n",
      "Epoch [17/20], Step [167/316], Loss: 0.0756\n",
      "Epoch [17/20], Step [168/316], Loss: 0.0021\n",
      "Epoch [17/20], Step [169/316], Loss: 0.0067\n",
      "Epoch [17/20], Step [170/316], Loss: 0.0053\n",
      "Epoch [17/20], Step [171/316], Loss: 0.0017\n",
      "Epoch [17/20], Step [172/316], Loss: 0.0057\n",
      "Epoch [17/20], Step [173/316], Loss: 0.0010\n",
      "Epoch [17/20], Step [174/316], Loss: 0.0094\n",
      "Epoch [17/20], Step [175/316], Loss: 0.0078\n",
      "Epoch [17/20], Step [176/316], Loss: 0.0067\n",
      "Epoch [17/20], Step [177/316], Loss: 0.0170\n",
      "Epoch [17/20], Step [178/316], Loss: 0.1161\n",
      "Epoch [17/20], Step [179/316], Loss: 0.0082\n",
      "Epoch [17/20], Step [180/316], Loss: 0.0073\n",
      "Epoch [17/20], Step [181/316], Loss: 0.0085\n",
      "Epoch [17/20], Step [182/316], Loss: 0.0382\n",
      "Epoch [17/20], Step [183/316], Loss: 0.0056\n",
      "Epoch [17/20], Step [184/316], Loss: 0.0022\n",
      "Epoch [17/20], Step [185/316], Loss: 0.0097\n",
      "Epoch [17/20], Step [186/316], Loss: 0.0095\n",
      "Epoch [17/20], Step [187/316], Loss: 0.0038\n",
      "Epoch [17/20], Step [188/316], Loss: 0.0044\n",
      "Epoch [17/20], Step [189/316], Loss: 0.0628\n",
      "Epoch [17/20], Step [190/316], Loss: 0.0070\n",
      "Epoch [17/20], Step [191/316], Loss: 0.0035\n",
      "Epoch [17/20], Step [192/316], Loss: 0.0559\n",
      "Epoch [17/20], Step [193/316], Loss: 0.0476\n",
      "Epoch [17/20], Step [194/316], Loss: 0.0029\n",
      "Epoch [17/20], Step [195/316], Loss: 0.0197\n",
      "Epoch [17/20], Step [196/316], Loss: 0.0981\n",
      "Epoch [17/20], Step [197/316], Loss: 0.0070\n",
      "Epoch [17/20], Step [198/316], Loss: 0.0035\n",
      "Epoch [17/20], Step [199/316], Loss: 0.0048\n",
      "Epoch [17/20], Step [200/316], Loss: 0.0028\n",
      "Epoch [17/20], Step [201/316], Loss: 0.0072\n",
      "Epoch [17/20], Step [202/316], Loss: 0.0240\n",
      "Epoch [17/20], Step [203/316], Loss: 0.0160\n",
      "Epoch [17/20], Step [204/316], Loss: 0.0043\n",
      "Epoch [17/20], Step [205/316], Loss: 0.0034\n",
      "Epoch [17/20], Step [206/316], Loss: 0.1008\n",
      "Epoch [17/20], Step [207/316], Loss: 0.0026\n",
      "Epoch [17/20], Step [208/316], Loss: 0.0742\n",
      "Epoch [17/20], Step [209/316], Loss: 0.0029\n",
      "Epoch [17/20], Step [210/316], Loss: 0.0082\n",
      "Epoch [17/20], Step [211/316], Loss: 0.0113\n",
      "Epoch [17/20], Step [212/316], Loss: 0.0023\n",
      "Epoch [17/20], Step [213/316], Loss: 0.0024\n",
      "Epoch [17/20], Step [214/316], Loss: 0.1430\n",
      "Epoch [17/20], Step [215/316], Loss: 0.1316\n",
      "Epoch [17/20], Step [216/316], Loss: 0.0525\n",
      "Epoch [17/20], Step [217/316], Loss: 0.0142\n",
      "Epoch [17/20], Step [218/316], Loss: 0.0031\n",
      "Epoch [17/20], Step [219/316], Loss: 0.0102\n",
      "Epoch [17/20], Step [220/316], Loss: 0.0057\n",
      "Epoch [17/20], Step [221/316], Loss: 0.0918\n",
      "Epoch [17/20], Step [222/316], Loss: 0.0061\n",
      "Epoch [17/20], Step [223/316], Loss: 0.0013\n",
      "Epoch [17/20], Step [224/316], Loss: 0.0159\n",
      "Epoch [17/20], Step [225/316], Loss: 0.0020\n",
      "Epoch [17/20], Step [226/316], Loss: 0.0058\n",
      "Epoch [17/20], Step [227/316], Loss: 0.0045\n",
      "Epoch [17/20], Step [228/316], Loss: 0.1349\n",
      "Epoch [17/20], Step [229/316], Loss: 0.0072\n",
      "Epoch [17/20], Step [230/316], Loss: 0.0174\n",
      "Epoch [17/20], Step [231/316], Loss: 0.0056\n",
      "Epoch [17/20], Step [232/316], Loss: 0.1199\n",
      "Epoch [17/20], Step [233/316], Loss: 0.0048\n",
      "Epoch [17/20], Step [234/316], Loss: 0.0099\n",
      "Epoch [17/20], Step [235/316], Loss: 0.0014\n",
      "Epoch [17/20], Step [236/316], Loss: 0.0248\n",
      "Epoch [17/20], Step [237/316], Loss: 0.0081\n",
      "Epoch [17/20], Step [238/316], Loss: 0.0107\n",
      "Epoch [17/20], Step [239/316], Loss: 0.0048\n",
      "Epoch [17/20], Step [240/316], Loss: 0.0060\n",
      "Epoch [17/20], Step [241/316], Loss: 0.0604\n",
      "Epoch [17/20], Step [242/316], Loss: 0.0061\n",
      "Epoch [17/20], Step [243/316], Loss: 0.0027\n",
      "Epoch [17/20], Step [244/316], Loss: 0.0046\n",
      "Epoch [17/20], Step [245/316], Loss: 0.0042\n",
      "Epoch [17/20], Step [246/316], Loss: 0.0093\n",
      "Epoch [17/20], Step [247/316], Loss: 0.0066\n",
      "Epoch [17/20], Step [248/316], Loss: 0.0034\n",
      "Epoch [17/20], Step [249/316], Loss: 0.0568\n",
      "Epoch [17/20], Step [250/316], Loss: 0.0058\n",
      "Epoch [17/20], Step [251/316], Loss: 0.0112\n",
      "Epoch [17/20], Step [252/316], Loss: 0.0029\n",
      "Epoch [17/20], Step [253/316], Loss: 0.0051\n",
      "Epoch [17/20], Step [254/316], Loss: 0.0078\n",
      "Epoch [17/20], Step [255/316], Loss: 0.0960\n",
      "Epoch [17/20], Step [256/316], Loss: 0.0064\n",
      "Epoch [17/20], Step [257/316], Loss: 0.0060\n",
      "Epoch [17/20], Step [258/316], Loss: 0.0031\n",
      "Epoch [17/20], Step [259/316], Loss: 0.0051\n",
      "Epoch [17/20], Step [260/316], Loss: 0.1299\n",
      "Epoch [17/20], Step [261/316], Loss: 0.0041\n",
      "Epoch [17/20], Step [262/316], Loss: 0.1053\n",
      "Epoch [17/20], Step [263/316], Loss: 0.0042\n",
      "Epoch [17/20], Step [264/316], Loss: 0.0056\n",
      "Epoch [17/20], Step [265/316], Loss: 0.0628\n",
      "Epoch [17/20], Step [266/316], Loss: 0.0434\n",
      "Epoch [17/20], Step [267/316], Loss: 0.0014\n",
      "Epoch [17/20], Step [268/316], Loss: 0.0051\n",
      "Epoch [17/20], Step [269/316], Loss: 0.0338\n",
      "Epoch [17/20], Step [270/316], Loss: 0.1244\n",
      "Epoch [17/20], Step [271/316], Loss: 0.0079\n",
      "Epoch [17/20], Step [272/316], Loss: 0.0009\n",
      "Epoch [17/20], Step [273/316], Loss: 0.0049\n",
      "Epoch [17/20], Step [274/316], Loss: 0.0039\n",
      "Epoch [17/20], Step [275/316], Loss: 0.0011\n",
      "Epoch [17/20], Step [276/316], Loss: 0.0018\n",
      "Epoch [17/20], Step [277/316], Loss: 0.0063\n",
      "Epoch [17/20], Step [278/316], Loss: 0.0206\n",
      "Epoch [17/20], Step [279/316], Loss: 0.0066\n",
      "Epoch [17/20], Step [280/316], Loss: 0.0179\n",
      "Epoch [17/20], Step [281/316], Loss: 0.0017\n",
      "Epoch [17/20], Step [282/316], Loss: 0.0354\n",
      "Epoch [17/20], Step [283/316], Loss: 0.0646\n",
      "Epoch [17/20], Step [284/316], Loss: 0.0030\n",
      "Epoch [17/20], Step [285/316], Loss: 0.0034\n",
      "Epoch [17/20], Step [286/316], Loss: 0.0011\n",
      "Epoch [17/20], Step [287/316], Loss: 0.0128\n",
      "Epoch [17/20], Step [288/316], Loss: 0.0919\n",
      "Epoch [17/20], Step [289/316], Loss: 0.0036\n",
      "Epoch [17/20], Step [290/316], Loss: 0.1546\n",
      "Epoch [17/20], Step [291/316], Loss: 0.0027\n",
      "Epoch [17/20], Step [292/316], Loss: 0.0403\n",
      "Epoch [17/20], Step [293/316], Loss: 0.0021\n",
      "Epoch [17/20], Step [294/316], Loss: 0.0079\n",
      "Epoch [17/20], Step [295/316], Loss: 0.0016\n",
      "Epoch [17/20], Step [296/316], Loss: 0.0028\n",
      "Epoch [17/20], Step [297/316], Loss: 0.0075\n",
      "Epoch [17/20], Step [298/316], Loss: 0.0028\n",
      "Epoch [17/20], Step [299/316], Loss: 0.0012\n",
      "Epoch [17/20], Step [300/316], Loss: 0.0043\n",
      "Epoch [17/20], Step [301/316], Loss: 0.0015\n",
      "Epoch [17/20], Step [302/316], Loss: 0.0183\n",
      "Epoch [17/20], Step [303/316], Loss: 0.0424\n",
      "Epoch [17/20], Step [304/316], Loss: 0.1377\n",
      "Epoch [17/20], Step [305/316], Loss: 0.0014\n",
      "Epoch [17/20], Step [306/316], Loss: 0.0024\n",
      "Epoch [17/20], Step [307/316], Loss: 0.0290\n",
      "Epoch [17/20], Step [308/316], Loss: 0.0256\n",
      "Epoch [17/20], Step [309/316], Loss: 0.0717\n",
      "Epoch [17/20], Step [310/316], Loss: 0.0018\n",
      "Epoch [17/20], Step [311/316], Loss: 0.0092\n",
      "Epoch [17/20], Step [312/316], Loss: 0.0246\n",
      "Epoch [17/20], Step [313/316], Loss: 0.0107\n",
      "Epoch [17/20], Step [314/316], Loss: 0.0026\n",
      "Epoch [17/20], Step [315/316], Loss: 0.0056\n",
      "Epoch [17/20], Step [316/316], Loss: 0.0053\n",
      "Epoch [17/20], Train Loss: 0.0248\n",
      "Epoch [17/20], Validation Loss: 0.1634\n",
      "Epoch [17/20], Validation Accuracy: 0.8107\n",
      "Epoch [18/20], Step [1/316], Loss: 0.0114\n",
      "Epoch [18/20], Step [2/316], Loss: 0.0050\n",
      "Epoch [18/20], Step [3/316], Loss: 0.0150\n",
      "Epoch [18/20], Step [4/316], Loss: 0.0060\n",
      "Epoch [18/20], Step [5/316], Loss: 0.0008\n",
      "Epoch [18/20], Step [6/316], Loss: 0.0017\n",
      "Epoch [18/20], Step [7/316], Loss: 0.0028\n",
      "Epoch [18/20], Step [8/316], Loss: 0.0048\n",
      "Epoch [18/20], Step [9/316], Loss: 0.0122\n",
      "Epoch [18/20], Step [10/316], Loss: 0.0054\n",
      "Epoch [18/20], Step [11/316], Loss: 0.0037\n",
      "Epoch [18/20], Step [12/316], Loss: 0.0156\n",
      "Epoch [18/20], Step [13/316], Loss: 0.0019\n",
      "Epoch [18/20], Step [14/316], Loss: 0.0023\n",
      "Epoch [18/20], Step [15/316], Loss: 0.0500\n",
      "Epoch [18/20], Step [16/316], Loss: 0.0078\n",
      "Epoch [18/20], Step [17/316], Loss: 0.0232\n",
      "Epoch [18/20], Step [18/316], Loss: 0.0091\n",
      "Epoch [18/20], Step [19/316], Loss: 0.0935\n",
      "Epoch [18/20], Step [20/316], Loss: 0.0140\n",
      "Epoch [18/20], Step [21/316], Loss: 0.0026\n",
      "Epoch [18/20], Step [22/316], Loss: 0.0023\n",
      "Epoch [18/20], Step [23/316], Loss: 0.0045\n",
      "Epoch [18/20], Step [24/316], Loss: 0.0048\n",
      "Epoch [18/20], Step [25/316], Loss: 0.0036\n",
      "Epoch [18/20], Step [26/316], Loss: 0.0039\n",
      "Epoch [18/20], Step [27/316], Loss: 0.1174\n",
      "Epoch [18/20], Step [28/316], Loss: 0.0013\n",
      "Epoch [18/20], Step [29/316], Loss: 0.0729\n",
      "Epoch [18/20], Step [30/316], Loss: 0.0030\n",
      "Epoch [18/20], Step [31/316], Loss: 0.0035\n",
      "Epoch [18/20], Step [32/316], Loss: 0.0080\n",
      "Epoch [18/20], Step [33/316], Loss: 0.0007\n",
      "Epoch [18/20], Step [34/316], Loss: 0.0457\n",
      "Epoch [18/20], Step [35/316], Loss: 0.0059\n",
      "Epoch [18/20], Step [36/316], Loss: 0.0035\n",
      "Epoch [18/20], Step [37/316], Loss: 0.0660\n",
      "Epoch [18/20], Step [38/316], Loss: 0.0174\n",
      "Epoch [18/20], Step [39/316], Loss: 0.0055\n",
      "Epoch [18/20], Step [40/316], Loss: 0.1347\n",
      "Epoch [18/20], Step [41/316], Loss: 0.0041\n",
      "Epoch [18/20], Step [42/316], Loss: 0.0754\n",
      "Epoch [18/20], Step [43/316], Loss: 0.0051\n",
      "Epoch [18/20], Step [44/316], Loss: 0.0019\n",
      "Epoch [18/20], Step [45/316], Loss: 0.0038\n",
      "Epoch [18/20], Step [46/316], Loss: 0.0047\n",
      "Epoch [18/20], Step [47/316], Loss: 0.0037\n",
      "Epoch [18/20], Step [48/316], Loss: 0.0050\n",
      "Epoch [18/20], Step [49/316], Loss: 0.0034\n",
      "Epoch [18/20], Step [50/316], Loss: 0.0016\n",
      "Epoch [18/20], Step [51/316], Loss: 0.0017\n",
      "Epoch [18/20], Step [52/316], Loss: 0.2533\n",
      "Epoch [18/20], Step [53/316], Loss: 0.0085\n",
      "Epoch [18/20], Step [54/316], Loss: 0.0013\n",
      "Epoch [18/20], Step [55/316], Loss: 0.0040\n",
      "Epoch [18/20], Step [56/316], Loss: 0.0253\n",
      "Epoch [18/20], Step [57/316], Loss: 0.0112\n",
      "Epoch [18/20], Step [58/316], Loss: 0.0015\n",
      "Epoch [18/20], Step [59/316], Loss: 0.0726\n",
      "Epoch [18/20], Step [60/316], Loss: 0.0283\n",
      "Epoch [18/20], Step [61/316], Loss: 0.0082\n",
      "Epoch [18/20], Step [62/316], Loss: 0.0067\n",
      "Epoch [18/20], Step [63/316], Loss: 0.0086\n",
      "Epoch [18/20], Step [64/316], Loss: 0.0074\n",
      "Epoch [18/20], Step [65/316], Loss: 0.0027\n",
      "Epoch [18/20], Step [66/316], Loss: 0.1226\n",
      "Epoch [18/20], Step [67/316], Loss: 0.0026\n",
      "Epoch [18/20], Step [68/316], Loss: 0.0013\n",
      "Epoch [18/20], Step [69/316], Loss: 0.0031\n",
      "Epoch [18/20], Step [70/316], Loss: 0.0120\n",
      "Epoch [18/20], Step [71/316], Loss: 0.0054\n",
      "Epoch [18/20], Step [72/316], Loss: 0.0178\n",
      "Epoch [18/20], Step [73/316], Loss: 0.0044\n",
      "Epoch [18/20], Step [74/316], Loss: 0.0049\n",
      "Epoch [18/20], Step [75/316], Loss: 0.0531\n",
      "Epoch [18/20], Step [76/316], Loss: 0.0024\n",
      "Epoch [18/20], Step [77/316], Loss: 0.0234\n",
      "Epoch [18/20], Step [78/316], Loss: 0.0098\n",
      "Epoch [18/20], Step [79/316], Loss: 0.0029\n",
      "Epoch [18/20], Step [80/316], Loss: 0.0023\n",
      "Epoch [18/20], Step [81/316], Loss: 0.0119\n",
      "Epoch [18/20], Step [82/316], Loss: 0.0020\n",
      "Epoch [18/20], Step [83/316], Loss: 0.0056\n",
      "Epoch [18/20], Step [84/316], Loss: 0.1003\n",
      "Epoch [18/20], Step [85/316], Loss: 0.3629\n",
      "Epoch [18/20], Step [86/316], Loss: 0.0052\n",
      "Epoch [18/20], Step [87/316], Loss: 0.0026\n",
      "Epoch [18/20], Step [88/316], Loss: 0.0130\n",
      "Epoch [18/20], Step [89/316], Loss: 0.0032\n",
      "Epoch [18/20], Step [90/316], Loss: 0.0555\n",
      "Epoch [18/20], Step [91/316], Loss: 0.0835\n",
      "Epoch [18/20], Step [92/316], Loss: 0.0875\n",
      "Epoch [18/20], Step [93/316], Loss: 0.0074\n",
      "Epoch [18/20], Step [94/316], Loss: 0.0351\n",
      "Epoch [18/20], Step [95/316], Loss: 0.0026\n",
      "Epoch [18/20], Step [96/316], Loss: 0.0041\n",
      "Epoch [18/20], Step [97/316], Loss: 0.0021\n",
      "Epoch [18/20], Step [98/316], Loss: 0.0072\n",
      "Epoch [18/20], Step [99/316], Loss: 0.0047\n",
      "Epoch [18/20], Step [100/316], Loss: 0.0094\n",
      "Epoch [18/20], Step [101/316], Loss: 0.0060\n",
      "Epoch [18/20], Step [102/316], Loss: 0.0099\n",
      "Epoch [18/20], Step [103/316], Loss: 0.0041\n",
      "Epoch [18/20], Step [104/316], Loss: 0.0047\n",
      "Epoch [18/20], Step [105/316], Loss: 0.0007\n",
      "Epoch [18/20], Step [106/316], Loss: 0.0330\n",
      "Epoch [18/20], Step [107/316], Loss: 0.0038\n",
      "Epoch [18/20], Step [108/316], Loss: 0.0076\n",
      "Epoch [18/20], Step [109/316], Loss: 0.0025\n",
      "Epoch [18/20], Step [110/316], Loss: 0.1051\n",
      "Epoch [18/20], Step [111/316], Loss: 0.0431\n",
      "Epoch [18/20], Step [112/316], Loss: 0.0037\n",
      "Epoch [18/20], Step [113/316], Loss: 0.0042\n",
      "Epoch [18/20], Step [114/316], Loss: 0.1490\n",
      "Epoch [18/20], Step [115/316], Loss: 0.0614\n",
      "Epoch [18/20], Step [116/316], Loss: 0.0012\n",
      "Epoch [18/20], Step [117/316], Loss: 0.0041\n",
      "Epoch [18/20], Step [118/316], Loss: 0.0548\n",
      "Epoch [18/20], Step [119/316], Loss: 0.0061\n",
      "Epoch [18/20], Step [120/316], Loss: 0.0674\n",
      "Epoch [18/20], Step [121/316], Loss: 0.0043\n",
      "Epoch [18/20], Step [122/316], Loss: 0.0377\n",
      "Epoch [18/20], Step [123/316], Loss: 0.0068\n",
      "Epoch [18/20], Step [124/316], Loss: 0.1392\n",
      "Epoch [18/20], Step [125/316], Loss: 0.0027\n",
      "Epoch [18/20], Step [126/316], Loss: 0.0046\n",
      "Epoch [18/20], Step [127/316], Loss: 0.0177\n",
      "Epoch [18/20], Step [128/316], Loss: 0.0027\n",
      "Epoch [18/20], Step [129/316], Loss: 0.0107\n",
      "Epoch [18/20], Step [130/316], Loss: 0.0027\n",
      "Epoch [18/20], Step [131/316], Loss: 0.0020\n",
      "Epoch [18/20], Step [132/316], Loss: 0.2070\n",
      "Epoch [18/20], Step [133/316], Loss: 0.0018\n",
      "Epoch [18/20], Step [134/316], Loss: 0.1097\n",
      "Epoch [18/20], Step [135/316], Loss: 0.0366\n",
      "Epoch [18/20], Step [136/316], Loss: 0.0046\n",
      "Epoch [18/20], Step [137/316], Loss: 0.0215\n",
      "Epoch [18/20], Step [138/316], Loss: 0.1506\n",
      "Epoch [18/20], Step [139/316], Loss: 0.0047\n",
      "Epoch [18/20], Step [140/316], Loss: 0.0041\n",
      "Epoch [18/20], Step [141/316], Loss: 0.0025\n",
      "Epoch [18/20], Step [142/316], Loss: 0.0089\n",
      "Epoch [18/20], Step [143/316], Loss: 0.0321\n",
      "Epoch [18/20], Step [144/316], Loss: 0.0016\n",
      "Epoch [18/20], Step [145/316], Loss: 0.0051\n",
      "Epoch [18/20], Step [146/316], Loss: 0.0017\n",
      "Epoch [18/20], Step [147/316], Loss: 0.0107\n",
      "Epoch [18/20], Step [148/316], Loss: 0.0102\n",
      "Epoch [18/20], Step [149/316], Loss: 0.0223\n",
      "Epoch [18/20], Step [150/316], Loss: 0.0090\n",
      "Epoch [18/20], Step [151/316], Loss: 0.0074\n",
      "Epoch [18/20], Step [152/316], Loss: 0.0037\n",
      "Epoch [18/20], Step [153/316], Loss: 0.0035\n",
      "Epoch [18/20], Step [154/316], Loss: 0.0111\n",
      "Epoch [18/20], Step [155/316], Loss: 0.1116\n",
      "Epoch [18/20], Step [156/316], Loss: 0.0035\n",
      "Epoch [18/20], Step [157/316], Loss: 0.0126\n",
      "Epoch [18/20], Step [158/316], Loss: 0.0098\n",
      "Epoch [18/20], Step [159/316], Loss: 0.0017\n",
      "Epoch [18/20], Step [160/316], Loss: 0.0041\n",
      "Epoch [18/20], Step [161/316], Loss: 0.0019\n",
      "Epoch [18/20], Step [162/316], Loss: 0.0023\n",
      "Epoch [18/20], Step [163/316], Loss: 0.0647\n",
      "Epoch [18/20], Step [164/316], Loss: 0.0052\n",
      "Epoch [18/20], Step [165/316], Loss: 0.0973\n",
      "Epoch [18/20], Step [166/316], Loss: 0.0049\n",
      "Epoch [18/20], Step [167/316], Loss: 0.0016\n",
      "Epoch [18/20], Step [168/316], Loss: 0.0042\n",
      "Epoch [18/20], Step [169/316], Loss: 0.0145\n",
      "Epoch [18/20], Step [170/316], Loss: 0.0926\n",
      "Epoch [18/20], Step [171/316], Loss: 0.0694\n",
      "Epoch [18/20], Step [172/316], Loss: 0.1209\n",
      "Epoch [18/20], Step [173/316], Loss: 0.3128\n",
      "Epoch [18/20], Step [174/316], Loss: 0.0045\n",
      "Epoch [18/20], Step [175/316], Loss: 0.1178\n",
      "Epoch [18/20], Step [176/316], Loss: 0.0057\n",
      "Epoch [18/20], Step [177/316], Loss: 0.0045\n",
      "Epoch [18/20], Step [178/316], Loss: 0.0352\n",
      "Epoch [18/20], Step [179/316], Loss: 0.0056\n",
      "Epoch [18/20], Step [180/316], Loss: 0.0055\n",
      "Epoch [18/20], Step [181/316], Loss: 0.0998\n",
      "Epoch [18/20], Step [182/316], Loss: 0.0531\n",
      "Epoch [18/20], Step [183/316], Loss: 0.0402\n",
      "Epoch [18/20], Step [184/316], Loss: 0.0015\n",
      "Epoch [18/20], Step [185/316], Loss: 0.0618\n",
      "Epoch [18/20], Step [186/316], Loss: 0.0045\n",
      "Epoch [18/20], Step [187/316], Loss: 0.0038\n",
      "Epoch [18/20], Step [188/316], Loss: 0.1056\n",
      "Epoch [18/20], Step [189/316], Loss: 0.0022\n",
      "Epoch [18/20], Step [190/316], Loss: 0.0047\n",
      "Epoch [18/20], Step [191/316], Loss: 0.0110\n",
      "Epoch [18/20], Step [192/316], Loss: 0.0231\n",
      "Epoch [18/20], Step [193/316], Loss: 0.0242\n",
      "Epoch [18/20], Step [194/316], Loss: 0.0309\n",
      "Epoch [18/20], Step [195/316], Loss: 0.0015\n",
      "Epoch [18/20], Step [196/316], Loss: 0.0176\n",
      "Epoch [18/20], Step [197/316], Loss: 0.0042\n",
      "Epoch [18/20], Step [198/316], Loss: 0.0237\n",
      "Epoch [18/20], Step [199/316], Loss: 0.0014\n",
      "Epoch [18/20], Step [200/316], Loss: 0.0090\n",
      "Epoch [18/20], Step [201/316], Loss: 0.0217\n",
      "Epoch [18/20], Step [202/316], Loss: 0.0180\n",
      "Epoch [18/20], Step [203/316], Loss: 0.0077\n",
      "Epoch [18/20], Step [204/316], Loss: 0.0057\n",
      "Epoch [18/20], Step [205/316], Loss: 0.0144\n",
      "Epoch [18/20], Step [206/316], Loss: 0.0099\n",
      "Epoch [18/20], Step [207/316], Loss: 0.0027\n",
      "Epoch [18/20], Step [208/316], Loss: 0.2067\n",
      "Epoch [18/20], Step [209/316], Loss: 0.0477\n",
      "Epoch [18/20], Step [210/316], Loss: 0.0028\n",
      "Epoch [18/20], Step [211/316], Loss: 0.0025\n",
      "Epoch [18/20], Step [212/316], Loss: 0.0035\n",
      "Epoch [18/20], Step [213/316], Loss: 0.0093\n",
      "Epoch [18/20], Step [214/316], Loss: 0.0005\n",
      "Epoch [18/20], Step [215/316], Loss: 0.0331\n",
      "Epoch [18/20], Step [216/316], Loss: 0.0071\n",
      "Epoch [18/20], Step [217/316], Loss: 0.0069\n",
      "Epoch [18/20], Step [218/316], Loss: 0.0131\n",
      "Epoch [18/20], Step [219/316], Loss: 0.1479\n",
      "Epoch [18/20], Step [220/316], Loss: 0.1064\n",
      "Epoch [18/20], Step [221/316], Loss: 0.0096\n",
      "Epoch [18/20], Step [222/316], Loss: 0.0031\n",
      "Epoch [18/20], Step [223/316], Loss: 0.0168\n",
      "Epoch [18/20], Step [224/316], Loss: 0.0014\n",
      "Epoch [18/20], Step [225/316], Loss: 0.0022\n",
      "Epoch [18/20], Step [226/316], Loss: 0.0125\n",
      "Epoch [18/20], Step [227/316], Loss: 0.0076\n",
      "Epoch [18/20], Step [228/316], Loss: 0.0392\n",
      "Epoch [18/20], Step [229/316], Loss: 0.0911\n",
      "Epoch [18/20], Step [230/316], Loss: 0.0112\n",
      "Epoch [18/20], Step [231/316], Loss: 0.0036\n",
      "Epoch [18/20], Step [232/316], Loss: 0.0031\n",
      "Epoch [18/20], Step [233/316], Loss: 0.0324\n",
      "Epoch [18/20], Step [234/316], Loss: 0.0037\n",
      "Epoch [18/20], Step [235/316], Loss: 0.0021\n",
      "Epoch [18/20], Step [236/316], Loss: 0.1776\n",
      "Epoch [18/20], Step [237/316], Loss: 0.0059\n",
      "Epoch [18/20], Step [238/316], Loss: 0.0071\n",
      "Epoch [18/20], Step [239/316], Loss: 0.0680\n",
      "Epoch [18/20], Step [240/316], Loss: 0.1229\n",
      "Epoch [18/20], Step [241/316], Loss: 0.0012\n",
      "Epoch [18/20], Step [242/316], Loss: 0.0198\n",
      "Epoch [18/20], Step [243/316], Loss: 0.0027\n",
      "Epoch [18/20], Step [244/316], Loss: 0.0019\n",
      "Epoch [18/20], Step [245/316], Loss: 0.0624\n",
      "Epoch [18/20], Step [246/316], Loss: 0.0070\n",
      "Epoch [18/20], Step [247/316], Loss: 0.0748\n",
      "Epoch [18/20], Step [248/316], Loss: 0.0213\n",
      "Epoch [18/20], Step [249/316], Loss: 0.0126\n",
      "Epoch [18/20], Step [250/316], Loss: 0.1918\n",
      "Epoch [18/20], Step [251/316], Loss: 0.0068\n",
      "Epoch [18/20], Step [252/316], Loss: 0.0038\n",
      "Epoch [18/20], Step [253/316], Loss: 0.0044\n",
      "Epoch [18/20], Step [254/316], Loss: 0.0025\n",
      "Epoch [18/20], Step [255/316], Loss: 0.0570\n",
      "Epoch [18/20], Step [256/316], Loss: 0.0030\n",
      "Epoch [18/20], Step [257/316], Loss: 0.0494\n",
      "Epoch [18/20], Step [258/316], Loss: 0.0326\n",
      "Epoch [18/20], Step [259/316], Loss: 0.0020\n",
      "Epoch [18/20], Step [260/316], Loss: 0.0052\n",
      "Epoch [18/20], Step [261/316], Loss: 0.0019\n",
      "Epoch [18/20], Step [262/316], Loss: 0.0354\n",
      "Epoch [18/20], Step [263/316], Loss: 0.0416\n",
      "Epoch [18/20], Step [264/316], Loss: 0.0051\n",
      "Epoch [18/20], Step [265/316], Loss: 0.0053\n",
      "Epoch [18/20], Step [266/316], Loss: 0.0093\n",
      "Epoch [18/20], Step [267/316], Loss: 0.0457\n",
      "Epoch [18/20], Step [268/316], Loss: 0.0031\n",
      "Epoch [18/20], Step [269/316], Loss: 0.0112\n",
      "Epoch [18/20], Step [270/316], Loss: 0.0914\n",
      "Epoch [18/20], Step [271/316], Loss: 0.0022\n",
      "Epoch [18/20], Step [272/316], Loss: 0.0027\n",
      "Epoch [18/20], Step [273/316], Loss: 0.0054\n",
      "Epoch [18/20], Step [274/316], Loss: 0.0014\n",
      "Epoch [18/20], Step [275/316], Loss: 0.0011\n",
      "Epoch [18/20], Step [276/316], Loss: 0.0043\n",
      "Epoch [18/20], Step [277/316], Loss: 0.0055\n",
      "Epoch [18/20], Step [278/316], Loss: 0.0035\n",
      "Epoch [18/20], Step [279/316], Loss: 0.1435\n",
      "Epoch [18/20], Step [280/316], Loss: 0.0352\n",
      "Epoch [18/20], Step [281/316], Loss: 0.0071\n",
      "Epoch [18/20], Step [282/316], Loss: 0.0048\n",
      "Epoch [18/20], Step [283/316], Loss: 0.0031\n",
      "Epoch [18/20], Step [284/316], Loss: 0.0013\n",
      "Epoch [18/20], Step [285/316], Loss: 0.1189\n",
      "Epoch [18/20], Step [286/316], Loss: 0.0012\n",
      "Epoch [18/20], Step [287/316], Loss: 0.0065\n",
      "Epoch [18/20], Step [288/316], Loss: 0.1457\n",
      "Epoch [18/20], Step [289/316], Loss: 0.1060\n",
      "Epoch [18/20], Step [290/316], Loss: 0.0498\n",
      "Epoch [18/20], Step [291/316], Loss: 0.1287\n",
      "Epoch [18/20], Step [292/316], Loss: 0.0038\n",
      "Epoch [18/20], Step [293/316], Loss: 0.0391\n",
      "Epoch [18/20], Step [294/316], Loss: 0.0122\n",
      "Epoch [18/20], Step [295/316], Loss: 0.0139\n",
      "Epoch [18/20], Step [296/316], Loss: 0.0891\n",
      "Epoch [18/20], Step [297/316], Loss: 0.0084\n",
      "Epoch [18/20], Step [298/316], Loss: 0.0045\n",
      "Epoch [18/20], Step [299/316], Loss: 0.0098\n",
      "Epoch [18/20], Step [300/316], Loss: 0.0042\n",
      "Epoch [18/20], Step [301/316], Loss: 0.0104\n",
      "Epoch [18/20], Step [302/316], Loss: 0.0021\n",
      "Epoch [18/20], Step [303/316], Loss: 0.0007\n",
      "Epoch [18/20], Step [304/316], Loss: 0.0474\n",
      "Epoch [18/20], Step [305/316], Loss: 0.0112\n",
      "Epoch [18/20], Step [306/316], Loss: 0.0018\n",
      "Epoch [18/20], Step [307/316], Loss: 0.0014\n",
      "Epoch [18/20], Step [308/316], Loss: 0.0175\n",
      "Epoch [18/20], Step [309/316], Loss: 0.0034\n",
      "Epoch [18/20], Step [310/316], Loss: 0.0038\n",
      "Epoch [18/20], Step [311/316], Loss: 0.1729\n",
      "Epoch [18/20], Step [312/316], Loss: 0.0060\n",
      "Epoch [18/20], Step [313/316], Loss: 0.0082\n",
      "Epoch [18/20], Step [314/316], Loss: 0.0111\n",
      "Epoch [18/20], Step [315/316], Loss: 0.0076\n",
      "Epoch [18/20], Step [316/316], Loss: 0.0062\n",
      "Epoch [18/20], Train Loss: 0.0282\n",
      "Epoch [18/20], Validation Loss: 0.1646\n",
      "Epoch [18/20], Validation Accuracy: 0.8139\n",
      "Epoch [19/20], Step [1/316], Loss: 0.0041\n",
      "Epoch [19/20], Step [2/316], Loss: 0.0040\n",
      "Epoch [19/20], Step [3/316], Loss: 0.0116\n",
      "Epoch [19/20], Step [4/316], Loss: 0.0054\n",
      "Epoch [19/20], Step [5/316], Loss: 0.0427\n",
      "Epoch [19/20], Step [6/316], Loss: 0.0443\n",
      "Epoch [19/20], Step [7/316], Loss: 0.0013\n",
      "Epoch [19/20], Step [8/316], Loss: 0.1526\n",
      "Epoch [19/20], Step [9/316], Loss: 0.1617\n",
      "Epoch [19/20], Step [10/316], Loss: 0.0045\n",
      "Epoch [19/20], Step [11/316], Loss: 0.0074\n",
      "Epoch [19/20], Step [12/316], Loss: 0.0049\n",
      "Epoch [19/20], Step [13/316], Loss: 0.0041\n",
      "Epoch [19/20], Step [14/316], Loss: 0.0238\n",
      "Epoch [19/20], Step [15/316], Loss: 0.1207\n",
      "Epoch [19/20], Step [16/316], Loss: 0.2112\n",
      "Epoch [19/20], Step [17/316], Loss: 0.0022\n",
      "Epoch [19/20], Step [18/316], Loss: 0.0074\n",
      "Epoch [19/20], Step [19/316], Loss: 0.0218\n",
      "Epoch [19/20], Step [20/316], Loss: 0.0020\n",
      "Epoch [19/20], Step [21/316], Loss: 0.0138\n",
      "Epoch [19/20], Step [22/316], Loss: 0.0102\n",
      "Epoch [19/20], Step [23/316], Loss: 0.0036\n",
      "Epoch [19/20], Step [24/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [25/316], Loss: 0.0051\n",
      "Epoch [19/20], Step [26/316], Loss: 0.0833\n",
      "Epoch [19/20], Step [27/316], Loss: 0.0066\n",
      "Epoch [19/20], Step [28/316], Loss: 0.1550\n",
      "Epoch [19/20], Step [29/316], Loss: 0.0035\n",
      "Epoch [19/20], Step [30/316], Loss: 0.0050\n",
      "Epoch [19/20], Step [31/316], Loss: 0.0649\n",
      "Epoch [19/20], Step [32/316], Loss: 0.1054\n",
      "Epoch [19/20], Step [33/316], Loss: 0.0034\n",
      "Epoch [19/20], Step [34/316], Loss: 0.0543\n",
      "Epoch [19/20], Step [35/316], Loss: 0.0035\n",
      "Epoch [19/20], Step [36/316], Loss: 0.0095\n",
      "Epoch [19/20], Step [37/316], Loss: 0.0035\n",
      "Epoch [19/20], Step [38/316], Loss: 0.0065\n",
      "Epoch [19/20], Step [39/316], Loss: 0.0550\n",
      "Epoch [19/20], Step [40/316], Loss: 0.0050\n",
      "Epoch [19/20], Step [41/316], Loss: 0.0060\n",
      "Epoch [19/20], Step [42/316], Loss: 0.0062\n",
      "Epoch [19/20], Step [43/316], Loss: 0.0321\n",
      "Epoch [19/20], Step [44/316], Loss: 0.1152\n",
      "Epoch [19/20], Step [45/316], Loss: 0.1045\n",
      "Epoch [19/20], Step [46/316], Loss: 0.0047\n",
      "Epoch [19/20], Step [47/316], Loss: 0.0472\n",
      "Epoch [19/20], Step [48/316], Loss: 0.0024\n",
      "Epoch [19/20], Step [49/316], Loss: 0.0179\n",
      "Epoch [19/20], Step [50/316], Loss: 0.0260\n",
      "Epoch [19/20], Step [51/316], Loss: 0.0082\n",
      "Epoch [19/20], Step [52/316], Loss: 0.0463\n",
      "Epoch [19/20], Step [53/316], Loss: 0.0068\n",
      "Epoch [19/20], Step [54/316], Loss: 0.0123\n",
      "Epoch [19/20], Step [55/316], Loss: 0.0047\n",
      "Epoch [19/20], Step [56/316], Loss: 0.0598\n",
      "Epoch [19/20], Step [57/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [58/316], Loss: 0.0032\n",
      "Epoch [19/20], Step [59/316], Loss: 0.0210\n",
      "Epoch [19/20], Step [60/316], Loss: 0.0304\n",
      "Epoch [19/20], Step [61/316], Loss: 0.0095\n",
      "Epoch [19/20], Step [62/316], Loss: 0.0051\n",
      "Epoch [19/20], Step [63/316], Loss: 0.0702\n",
      "Epoch [19/20], Step [64/316], Loss: 0.0029\n",
      "Epoch [19/20], Step [65/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [66/316], Loss: 0.1248\n",
      "Epoch [19/20], Step [67/316], Loss: 0.0227\n",
      "Epoch [19/20], Step [68/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [69/316], Loss: 0.1372\n",
      "Epoch [19/20], Step [70/316], Loss: 0.1557\n",
      "Epoch [19/20], Step [71/316], Loss: 0.0042\n",
      "Epoch [19/20], Step [72/316], Loss: 0.0043\n",
      "Epoch [19/20], Step [73/316], Loss: 0.0120\n",
      "Epoch [19/20], Step [74/316], Loss: 0.0068\n",
      "Epoch [19/20], Step [75/316], Loss: 0.0066\n",
      "Epoch [19/20], Step [76/316], Loss: 0.1347\n",
      "Epoch [19/20], Step [77/316], Loss: 0.1561\n",
      "Epoch [19/20], Step [78/316], Loss: 0.1194\n",
      "Epoch [19/20], Step [79/316], Loss: 0.0177\n",
      "Epoch [19/20], Step [80/316], Loss: 0.0045\n",
      "Epoch [19/20], Step [81/316], Loss: 0.0466\n",
      "Epoch [19/20], Step [82/316], Loss: 0.0030\n",
      "Epoch [19/20], Step [83/316], Loss: 0.0046\n",
      "Epoch [19/20], Step [84/316], Loss: 0.0047\n",
      "Epoch [19/20], Step [85/316], Loss: 0.0011\n",
      "Epoch [19/20], Step [86/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [87/316], Loss: 0.0024\n",
      "Epoch [19/20], Step [88/316], Loss: 0.0179\n",
      "Epoch [19/20], Step [89/316], Loss: 0.0420\n",
      "Epoch [19/20], Step [90/316], Loss: 0.0063\n",
      "Epoch [19/20], Step [91/316], Loss: 0.0032\n",
      "Epoch [19/20], Step [92/316], Loss: 0.0612\n",
      "Epoch [19/20], Step [93/316], Loss: 0.0093\n",
      "Epoch [19/20], Step [94/316], Loss: 0.1297\n",
      "Epoch [19/20], Step [95/316], Loss: 0.0288\n",
      "Epoch [19/20], Step [96/316], Loss: 0.0022\n",
      "Epoch [19/20], Step [97/316], Loss: 0.0054\n",
      "Epoch [19/20], Step [98/316], Loss: 0.0089\n",
      "Epoch [19/20], Step [99/316], Loss: 0.0093\n",
      "Epoch [19/20], Step [100/316], Loss: 0.0012\n",
      "Epoch [19/20], Step [101/316], Loss: 0.0068\n",
      "Epoch [19/20], Step [102/316], Loss: 0.0100\n",
      "Epoch [19/20], Step [103/316], Loss: 0.0180\n",
      "Epoch [19/20], Step [104/316], Loss: 0.0273\n",
      "Epoch [19/20], Step [105/316], Loss: 0.0064\n",
      "Epoch [19/20], Step [106/316], Loss: 0.0232\n",
      "Epoch [19/20], Step [107/316], Loss: 0.1631\n",
      "Epoch [19/20], Step [108/316], Loss: 0.0080\n",
      "Epoch [19/20], Step [109/316], Loss: 0.0033\n",
      "Epoch [19/20], Step [110/316], Loss: 0.0574\n",
      "Epoch [19/20], Step [111/316], Loss: 0.0043\n",
      "Epoch [19/20], Step [112/316], Loss: 0.2356\n",
      "Epoch [19/20], Step [113/316], Loss: 0.0569\n",
      "Epoch [19/20], Step [114/316], Loss: 0.0030\n",
      "Epoch [19/20], Step [115/316], Loss: 0.0357\n",
      "Epoch [19/20], Step [116/316], Loss: 0.0169\n",
      "Epoch [19/20], Step [117/316], Loss: 0.1300\n",
      "Epoch [19/20], Step [118/316], Loss: 0.0060\n",
      "Epoch [19/20], Step [119/316], Loss: 0.0062\n",
      "Epoch [19/20], Step [120/316], Loss: 0.0047\n",
      "Epoch [19/20], Step [121/316], Loss: 0.0022\n",
      "Epoch [19/20], Step [122/316], Loss: 0.0049\n",
      "Epoch [19/20], Step [123/316], Loss: 0.0386\n",
      "Epoch [19/20], Step [124/316], Loss: 0.0009\n",
      "Epoch [19/20], Step [125/316], Loss: 0.0079\n",
      "Epoch [19/20], Step [126/316], Loss: 0.0096\n",
      "Epoch [19/20], Step [127/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [128/316], Loss: 0.0031\n",
      "Epoch [19/20], Step [129/316], Loss: 0.0248\n",
      "Epoch [19/20], Step [130/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [131/316], Loss: 0.0036\n",
      "Epoch [19/20], Step [132/316], Loss: 0.0080\n",
      "Epoch [19/20], Step [133/316], Loss: 0.1518\n",
      "Epoch [19/20], Step [134/316], Loss: 0.0115\n",
      "Epoch [19/20], Step [135/316], Loss: 0.0076\n",
      "Epoch [19/20], Step [136/316], Loss: 0.0034\n",
      "Epoch [19/20], Step [137/316], Loss: 0.0046\n",
      "Epoch [19/20], Step [138/316], Loss: 0.0018\n",
      "Epoch [19/20], Step [139/316], Loss: 0.0069\n",
      "Epoch [19/20], Step [140/316], Loss: 0.0209\n",
      "Epoch [19/20], Step [141/316], Loss: 0.1578\n",
      "Epoch [19/20], Step [142/316], Loss: 0.0044\n",
      "Epoch [19/20], Step [143/316], Loss: 0.0253\n",
      "Epoch [19/20], Step [144/316], Loss: 0.0080\n",
      "Epoch [19/20], Step [145/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [146/316], Loss: 0.0046\n",
      "Epoch [19/20], Step [147/316], Loss: 0.0013\n",
      "Epoch [19/20], Step [148/316], Loss: 0.0734\n",
      "Epoch [19/20], Step [149/316], Loss: 0.0055\n",
      "Epoch [19/20], Step [150/316], Loss: 0.1509\n",
      "Epoch [19/20], Step [151/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [152/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [153/316], Loss: 0.0035\n",
      "Epoch [19/20], Step [154/316], Loss: 0.0039\n",
      "Epoch [19/20], Step [155/316], Loss: 0.0069\n",
      "Epoch [19/20], Step [156/316], Loss: 0.0049\n",
      "Epoch [19/20], Step [157/316], Loss: 0.0158\n",
      "Epoch [19/20], Step [158/316], Loss: 0.0117\n",
      "Epoch [19/20], Step [159/316], Loss: 0.1274\n",
      "Epoch [19/20], Step [160/316], Loss: 0.0830\n",
      "Epoch [19/20], Step [161/316], Loss: 0.0125\n",
      "Epoch [19/20], Step [162/316], Loss: 0.1111\n",
      "Epoch [19/20], Step [163/316], Loss: 0.0050\n",
      "Epoch [19/20], Step [164/316], Loss: 0.0047\n",
      "Epoch [19/20], Step [165/316], Loss: 0.0005\n",
      "Epoch [19/20], Step [166/316], Loss: 0.0450\n",
      "Epoch [19/20], Step [167/316], Loss: 0.0055\n",
      "Epoch [19/20], Step [168/316], Loss: 0.0055\n",
      "Epoch [19/20], Step [169/316], Loss: 0.0032\n",
      "Epoch [19/20], Step [170/316], Loss: 0.0065\n",
      "Epoch [19/20], Step [171/316], Loss: 0.0076\n",
      "Epoch [19/20], Step [172/316], Loss: 0.0021\n",
      "Epoch [19/20], Step [173/316], Loss: 0.0952\n",
      "Epoch [19/20], Step [174/316], Loss: 0.0267\n",
      "Epoch [19/20], Step [175/316], Loss: 0.0099\n",
      "Epoch [19/20], Step [176/316], Loss: 0.0005\n",
      "Epoch [19/20], Step [177/316], Loss: 0.0352\n",
      "Epoch [19/20], Step [178/316], Loss: 0.0138\n",
      "Epoch [19/20], Step [179/316], Loss: 0.0021\n",
      "Epoch [19/20], Step [180/316], Loss: 0.1786\n",
      "Epoch [19/20], Step [181/316], Loss: 0.0021\n",
      "Epoch [19/20], Step [182/316], Loss: 0.0243\n",
      "Epoch [19/20], Step [183/316], Loss: 0.0014\n",
      "Epoch [19/20], Step [184/316], Loss: 0.0072\n",
      "Epoch [19/20], Step [185/316], Loss: 0.0053\n",
      "Epoch [19/20], Step [186/316], Loss: 0.0129\n",
      "Epoch [19/20], Step [187/316], Loss: 0.1151\n",
      "Epoch [19/20], Step [188/316], Loss: 0.0062\n",
      "Epoch [19/20], Step [189/316], Loss: 0.0017\n",
      "Epoch [19/20], Step [190/316], Loss: 0.1133\n",
      "Epoch [19/20], Step [191/316], Loss: 0.1051\n",
      "Epoch [19/20], Step [192/316], Loss: 0.0052\n",
      "Epoch [19/20], Step [193/316], Loss: 0.0036\n",
      "Epoch [19/20], Step [194/316], Loss: 0.0021\n",
      "Epoch [19/20], Step [195/316], Loss: 0.0087\n",
      "Epoch [19/20], Step [196/316], Loss: 0.0062\n",
      "Epoch [19/20], Step [197/316], Loss: 0.1219\n",
      "Epoch [19/20], Step [198/316], Loss: 0.0595\n",
      "Epoch [19/20], Step [199/316], Loss: 0.0062\n",
      "Epoch [19/20], Step [200/316], Loss: 0.0536\n",
      "Epoch [19/20], Step [201/316], Loss: 0.1248\n",
      "Epoch [19/20], Step [202/316], Loss: 0.0066\n",
      "Epoch [19/20], Step [203/316], Loss: 0.0064\n",
      "Epoch [19/20], Step [204/316], Loss: 0.0410\n",
      "Epoch [19/20], Step [205/316], Loss: 0.0083\n",
      "Epoch [19/20], Step [206/316], Loss: 0.0069\n",
      "Epoch [19/20], Step [207/316], Loss: 0.0043\n",
      "Epoch [19/20], Step [208/316], Loss: 0.2149\n",
      "Epoch [19/20], Step [209/316], Loss: 0.0136\n",
      "Epoch [19/20], Step [210/316], Loss: 0.0014\n",
      "Epoch [19/20], Step [211/316], Loss: 0.0032\n",
      "Epoch [19/20], Step [212/316], Loss: 0.0056\n",
      "Epoch [19/20], Step [213/316], Loss: 0.0021\n",
      "Epoch [19/20], Step [214/316], Loss: 0.0150\n",
      "Epoch [19/20], Step [215/316], Loss: 0.0058\n",
      "Epoch [19/20], Step [216/316], Loss: 0.0098\n",
      "Epoch [19/20], Step [217/316], Loss: 0.0225\n",
      "Epoch [19/20], Step [218/316], Loss: 0.0575\n",
      "Epoch [19/20], Step [219/316], Loss: 0.0300\n",
      "Epoch [19/20], Step [220/316], Loss: 0.0029\n",
      "Epoch [19/20], Step [221/316], Loss: 0.0048\n",
      "Epoch [19/20], Step [222/316], Loss: 0.0083\n",
      "Epoch [19/20], Step [223/316], Loss: 0.0069\n",
      "Epoch [19/20], Step [224/316], Loss: 0.0123\n",
      "Epoch [19/20], Step [225/316], Loss: 0.0041\n",
      "Epoch [19/20], Step [226/316], Loss: 0.0037\n",
      "Epoch [19/20], Step [227/316], Loss: 0.0054\n",
      "Epoch [19/20], Step [228/316], Loss: 0.0511\n",
      "Epoch [19/20], Step [229/316], Loss: 0.0059\n",
      "Epoch [19/20], Step [230/316], Loss: 0.0223\n",
      "Epoch [19/20], Step [231/316], Loss: 0.0384\n",
      "Epoch [19/20], Step [232/316], Loss: 0.0077\n",
      "Epoch [19/20], Step [233/316], Loss: 0.0071\n",
      "Epoch [19/20], Step [234/316], Loss: 0.0017\n",
      "Epoch [19/20], Step [235/316], Loss: 0.0063\n",
      "Epoch [19/20], Step [236/316], Loss: 0.0121\n",
      "Epoch [19/20], Step [237/316], Loss: 0.0014\n",
      "Epoch [19/20], Step [238/316], Loss: 0.0605\n",
      "Epoch [19/20], Step [239/316], Loss: 0.1100\n",
      "Epoch [19/20], Step [240/316], Loss: 0.1263\n",
      "Epoch [19/20], Step [241/316], Loss: 0.0012\n",
      "Epoch [19/20], Step [242/316], Loss: 0.1291\n",
      "Epoch [19/20], Step [243/316], Loss: 0.0048\n",
      "Epoch [19/20], Step [244/316], Loss: 0.0211\n",
      "Epoch [19/20], Step [245/316], Loss: 0.0042\n",
      "Epoch [19/20], Step [246/316], Loss: 0.0002\n",
      "Epoch [19/20], Step [247/316], Loss: 0.0190\n",
      "Epoch [19/20], Step [248/316], Loss: 0.0975\n",
      "Epoch [19/20], Step [249/316], Loss: 0.0024\n",
      "Epoch [19/20], Step [250/316], Loss: 0.0077\n",
      "Epoch [19/20], Step [251/316], Loss: 0.0055\n",
      "Epoch [19/20], Step [252/316], Loss: 0.0110\n",
      "Epoch [19/20], Step [253/316], Loss: 0.1202\n",
      "Epoch [19/20], Step [254/316], Loss: 0.0011\n",
      "Epoch [19/20], Step [255/316], Loss: 0.0047\n",
      "Epoch [19/20], Step [256/316], Loss: 0.1418\n",
      "Epoch [19/20], Step [257/316], Loss: 0.1387\n",
      "Epoch [19/20], Step [258/316], Loss: 0.0697\n",
      "Epoch [19/20], Step [259/316], Loss: 0.0064\n",
      "Epoch [19/20], Step [260/316], Loss: 0.0076\n",
      "Epoch [19/20], Step [261/316], Loss: 0.0068\n",
      "Epoch [19/20], Step [262/316], Loss: 0.0627\n",
      "Epoch [19/20], Step [263/316], Loss: 0.0993\n",
      "Epoch [19/20], Step [264/316], Loss: 0.0066\n",
      "Epoch [19/20], Step [265/316], Loss: 0.0045\n",
      "Epoch [19/20], Step [266/316], Loss: 0.0004\n",
      "Epoch [19/20], Step [267/316], Loss: 0.0044\n",
      "Epoch [19/20], Step [268/316], Loss: 0.0011\n",
      "Epoch [19/20], Step [269/316], Loss: 0.0037\n",
      "Epoch [19/20], Step [270/316], Loss: 0.0055\n",
      "Epoch [19/20], Step [271/316], Loss: 0.0045\n",
      "Epoch [19/20], Step [272/316], Loss: 0.0769\n",
      "Epoch [19/20], Step [273/316], Loss: 0.0012\n",
      "Epoch [19/20], Step [274/316], Loss: 0.0107\n",
      "Epoch [19/20], Step [275/316], Loss: 0.0030\n",
      "Epoch [19/20], Step [276/316], Loss: 0.0048\n",
      "Epoch [19/20], Step [277/316], Loss: 0.1347\n",
      "Epoch [19/20], Step [278/316], Loss: 0.0100\n",
      "Epoch [19/20], Step [279/316], Loss: 0.0028\n",
      "Epoch [19/20], Step [280/316], Loss: 0.0012\n",
      "Epoch [19/20], Step [281/316], Loss: 0.0056\n",
      "Epoch [19/20], Step [282/316], Loss: 0.0025\n",
      "Epoch [19/20], Step [283/316], Loss: 0.0058\n",
      "Epoch [19/20], Step [284/316], Loss: 0.0027\n",
      "Epoch [19/20], Step [285/316], Loss: 0.0222\n",
      "Epoch [19/20], Step [286/316], Loss: 0.1016\n",
      "Epoch [19/20], Step [287/316], Loss: 0.0234\n",
      "Epoch [19/20], Step [288/316], Loss: 0.1220\n",
      "Epoch [19/20], Step [289/316], Loss: 0.0057\n",
      "Epoch [19/20], Step [290/316], Loss: 0.0050\n",
      "Epoch [19/20], Step [291/316], Loss: 0.0257\n",
      "Epoch [19/20], Step [292/316], Loss: 0.0159\n",
      "Epoch [19/20], Step [293/316], Loss: 0.0134\n",
      "Epoch [19/20], Step [294/316], Loss: 0.0079\n",
      "Epoch [19/20], Step [295/316], Loss: 0.0025\n",
      "Epoch [19/20], Step [296/316], Loss: 0.0831\n",
      "Epoch [19/20], Step [297/316], Loss: 0.1421\n",
      "Epoch [19/20], Step [298/316], Loss: 0.0252\n",
      "Epoch [19/20], Step [299/316], Loss: 0.0028\n",
      "Epoch [19/20], Step [300/316], Loss: 0.0028\n",
      "Epoch [19/20], Step [301/316], Loss: 0.0028\n",
      "Epoch [19/20], Step [302/316], Loss: 0.0072\n",
      "Epoch [19/20], Step [303/316], Loss: 0.0078\n",
      "Epoch [19/20], Step [304/316], Loss: 0.0077\n",
      "Epoch [19/20], Step [305/316], Loss: 0.0261\n",
      "Epoch [19/20], Step [306/316], Loss: 0.0030\n",
      "Epoch [19/20], Step [307/316], Loss: 0.0030\n",
      "Epoch [19/20], Step [308/316], Loss: 0.0012\n",
      "Epoch [19/20], Step [309/316], Loss: 0.0213\n",
      "Epoch [19/20], Step [310/316], Loss: 0.0016\n",
      "Epoch [19/20], Step [311/316], Loss: 0.0035\n",
      "Epoch [19/20], Step [312/316], Loss: 0.0060\n",
      "Epoch [19/20], Step [313/316], Loss: 0.2245\n",
      "Epoch [19/20], Step [314/316], Loss: 0.0073\n",
      "Epoch [19/20], Step [315/316], Loss: 0.0223\n",
      "Epoch [19/20], Step [316/316], Loss: 0.0116\n",
      "Epoch [19/20], Train Loss: 0.0309\n",
      "Epoch [19/20], Validation Loss: 0.1577\n",
      "Epoch [19/20], Validation Accuracy: 0.8122\n",
      "Epoch [20/20], Step [1/316], Loss: 0.0488\n",
      "Epoch [20/20], Step [2/316], Loss: 0.0009\n",
      "Epoch [20/20], Step [3/316], Loss: 0.0108\n",
      "Epoch [20/20], Step [4/316], Loss: 0.0168\n",
      "Epoch [20/20], Step [5/316], Loss: 0.1682\n",
      "Epoch [20/20], Step [6/316], Loss: 0.0079\n",
      "Epoch [20/20], Step [7/316], Loss: 0.0052\n",
      "Epoch [20/20], Step [8/316], Loss: 0.0083\n",
      "Epoch [20/20], Step [9/316], Loss: 0.0064\n",
      "Epoch [20/20], Step [10/316], Loss: 0.0070\n",
      "Epoch [20/20], Step [11/316], Loss: 0.0050\n",
      "Epoch [20/20], Step [12/316], Loss: 0.0041\n",
      "Epoch [20/20], Step [13/316], Loss: 0.2228\n",
      "Epoch [20/20], Step [14/316], Loss: 0.0098\n",
      "Epoch [20/20], Step [15/316], Loss: 0.0250\n",
      "Epoch [20/20], Step [16/316], Loss: 0.0047\n",
      "Epoch [20/20], Step [17/316], Loss: 0.0157\n",
      "Epoch [20/20], Step [18/316], Loss: 0.0925\n",
      "Epoch [20/20], Step [19/316], Loss: 0.0022\n",
      "Epoch [20/20], Step [20/316], Loss: 0.0040\n",
      "Epoch [20/20], Step [21/316], Loss: 0.0056\n",
      "Epoch [20/20], Step [22/316], Loss: 0.0035\n",
      "Epoch [20/20], Step [23/316], Loss: 0.0035\n",
      "Epoch [20/20], Step [24/316], Loss: 0.0033\n",
      "Epoch [20/20], Step [25/316], Loss: 0.1092\n",
      "Epoch [20/20], Step [26/316], Loss: 0.0014\n",
      "Epoch [20/20], Step [27/316], Loss: 0.0970\n",
      "Epoch [20/20], Step [28/316], Loss: 0.0016\n",
      "Epoch [20/20], Step [29/316], Loss: 0.0012\n",
      "Epoch [20/20], Step [30/316], Loss: 0.0023\n",
      "Epoch [20/20], Step [31/316], Loss: 0.0040\n",
      "Epoch [20/20], Step [32/316], Loss: 0.0020\n",
      "Epoch [20/20], Step [33/316], Loss: 0.0018\n",
      "Epoch [20/20], Step [34/316], Loss: 0.0049\n",
      "Epoch [20/20], Step [35/316], Loss: 0.0061\n",
      "Epoch [20/20], Step [36/316], Loss: 0.0073\n",
      "Epoch [20/20], Step [37/316], Loss: 0.0004\n",
      "Epoch [20/20], Step [38/316], Loss: 0.0329\n",
      "Epoch [20/20], Step [39/316], Loss: 0.0025\n",
      "Epoch [20/20], Step [40/316], Loss: 0.0057\n",
      "Epoch [20/20], Step [41/316], Loss: 0.0085\n",
      "Epoch [20/20], Step [42/316], Loss: 0.0054\n",
      "Epoch [20/20], Step [43/316], Loss: 0.0025\n",
      "Epoch [20/20], Step [44/316], Loss: 0.0036\n",
      "Epoch [20/20], Step [45/316], Loss: 0.0110\n",
      "Epoch [20/20], Step [46/316], Loss: 0.0029\n",
      "Epoch [20/20], Step [47/316], Loss: 0.0862\n",
      "Epoch [20/20], Step [48/316], Loss: 0.1995\n",
      "Epoch [20/20], Step [49/316], Loss: 0.0119\n",
      "Epoch [20/20], Step [50/316], Loss: 0.0031\n",
      "Epoch [20/20], Step [51/316], Loss: 0.0113\n",
      "Epoch [20/20], Step [52/316], Loss: 0.0016\n",
      "Epoch [20/20], Step [53/316], Loss: 0.0023\n",
      "Epoch [20/20], Step [54/316], Loss: 0.0978\n",
      "Epoch [20/20], Step [55/316], Loss: 0.0010\n",
      "Epoch [20/20], Step [56/316], Loss: 0.0152\n",
      "Epoch [20/20], Step [57/316], Loss: 0.0059\n",
      "Epoch [20/20], Step [58/316], Loss: 0.0271\n",
      "Epoch [20/20], Step [59/316], Loss: 0.0040\n",
      "Epoch [20/20], Step [60/316], Loss: 0.0029\n",
      "Epoch [20/20], Step [61/316], Loss: 0.0031\n",
      "Epoch [20/20], Step [62/316], Loss: 0.0100\n",
      "Epoch [20/20], Step [63/316], Loss: 0.0017\n",
      "Epoch [20/20], Step [64/316], Loss: 0.1741\n",
      "Epoch [20/20], Step [65/316], Loss: 0.0037\n",
      "Epoch [20/20], Step [66/316], Loss: 0.0049\n",
      "Epoch [20/20], Step [67/316], Loss: 0.0033\n",
      "Epoch [20/20], Step [68/316], Loss: 0.0948\n",
      "Epoch [20/20], Step [69/316], Loss: 0.0028\n",
      "Epoch [20/20], Step [70/316], Loss: 0.0049\n",
      "Epoch [20/20], Step [71/316], Loss: 0.0018\n",
      "Epoch [20/20], Step [72/316], Loss: 0.0016\n",
      "Epoch [20/20], Step [73/316], Loss: 0.0089\n",
      "Epoch [20/20], Step [74/316], Loss: 0.0040\n",
      "Epoch [20/20], Step [75/316], Loss: 0.0035\n",
      "Epoch [20/20], Step [76/316], Loss: 0.1046\n",
      "Epoch [20/20], Step [77/316], Loss: 0.0065\n",
      "Epoch [20/20], Step [78/316], Loss: 0.0050\n",
      "Epoch [20/20], Step [79/316], Loss: 0.0035\n",
      "Epoch [20/20], Step [80/316], Loss: 0.0042\n",
      "Epoch [20/20], Step [81/316], Loss: 0.0009\n",
      "Epoch [20/20], Step [82/316], Loss: 0.0079\n",
      "Epoch [20/20], Step [83/316], Loss: 0.0046\n",
      "Epoch [20/20], Step [84/316], Loss: 0.1258\n",
      "Epoch [20/20], Step [85/316], Loss: 0.0011\n",
      "Epoch [20/20], Step [86/316], Loss: 0.0282\n",
      "Epoch [20/20], Step [87/316], Loss: 0.0986\n",
      "Epoch [20/20], Step [88/316], Loss: 0.0079\n",
      "Epoch [20/20], Step [89/316], Loss: 0.0038\n",
      "Epoch [20/20], Step [90/316], Loss: 0.0037\n",
      "Epoch [20/20], Step [91/316], Loss: 0.0571\n",
      "Epoch [20/20], Step [92/316], Loss: 0.1177\n",
      "Epoch [20/20], Step [93/316], Loss: 0.1188\n",
      "Epoch [20/20], Step [94/316], Loss: 0.0453\n",
      "Epoch [20/20], Step [95/316], Loss: 0.0356\n",
      "Epoch [20/20], Step [96/316], Loss: 0.1430\n",
      "Epoch [20/20], Step [97/316], Loss: 0.0062\n",
      "Epoch [20/20], Step [98/316], Loss: 0.1186\n",
      "Epoch [20/20], Step [99/316], Loss: 0.0072\n",
      "Epoch [20/20], Step [100/316], Loss: 0.0035\n",
      "Epoch [20/20], Step [101/316], Loss: 0.0110\n",
      "Epoch [20/20], Step [102/316], Loss: 0.0124\n",
      "Epoch [20/20], Step [103/316], Loss: 0.0179\n",
      "Epoch [20/20], Step [104/316], Loss: 0.0345\n",
      "Epoch [20/20], Step [105/316], Loss: 0.0027\n",
      "Epoch [20/20], Step [106/316], Loss: 0.0023\n",
      "Epoch [20/20], Step [107/316], Loss: 0.0136\n",
      "Epoch [20/20], Step [108/316], Loss: 0.0057\n",
      "Epoch [20/20], Step [109/316], Loss: 0.0056\n",
      "Epoch [20/20], Step [110/316], Loss: 0.0442\n",
      "Epoch [20/20], Step [111/316], Loss: 0.0009\n",
      "Epoch [20/20], Step [112/316], Loss: 0.0185\n",
      "Epoch [20/20], Step [113/316], Loss: 0.0063\n",
      "Epoch [20/20], Step [114/316], Loss: 0.0019\n",
      "Epoch [20/20], Step [115/316], Loss: 0.0024\n",
      "Epoch [20/20], Step [116/316], Loss: 0.0323\n",
      "Epoch [20/20], Step [117/316], Loss: 0.1399\n",
      "Epoch [20/20], Step [118/316], Loss: 0.0059\n",
      "Epoch [20/20], Step [119/316], Loss: 0.0046\n",
      "Epoch [20/20], Step [120/316], Loss: 0.1090\n",
      "Epoch [20/20], Step [121/316], Loss: 0.1295\n",
      "Epoch [20/20], Step [122/316], Loss: 0.0051\n",
      "Epoch [20/20], Step [123/316], Loss: 0.0038\n",
      "Epoch [20/20], Step [124/316], Loss: 0.0047\n",
      "Epoch [20/20], Step [125/316], Loss: 0.0368\n",
      "Epoch [20/20], Step [126/316], Loss: 0.0061\n",
      "Epoch [20/20], Step [127/316], Loss: 0.0061\n",
      "Epoch [20/20], Step [128/316], Loss: 0.0772\n",
      "Epoch [20/20], Step [129/316], Loss: 0.0055\n",
      "Epoch [20/20], Step [130/316], Loss: 0.0056\n",
      "Epoch [20/20], Step [131/316], Loss: 0.0069\n",
      "Epoch [20/20], Step [132/316], Loss: 0.0119\n",
      "Epoch [20/20], Step [133/316], Loss: 0.0402\n",
      "Epoch [20/20], Step [134/316], Loss: 0.0951\n",
      "Epoch [20/20], Step [135/316], Loss: 0.0070\n",
      "Epoch [20/20], Step [136/316], Loss: 0.0084\n",
      "Epoch [20/20], Step [137/316], Loss: 0.1040\n",
      "Epoch [20/20], Step [138/316], Loss: 0.0030\n",
      "Epoch [20/20], Step [139/316], Loss: 0.0166\n",
      "Epoch [20/20], Step [140/316], Loss: 0.2292\n",
      "Epoch [20/20], Step [141/316], Loss: 0.0104\n",
      "Epoch [20/20], Step [142/316], Loss: 0.0038\n",
      "Epoch [20/20], Step [143/316], Loss: 0.0813\n",
      "Epoch [20/20], Step [144/316], Loss: 0.0132\n",
      "Epoch [20/20], Step [145/316], Loss: 0.0020\n",
      "Epoch [20/20], Step [146/316], Loss: 0.0929\n",
      "Epoch [20/20], Step [147/316], Loss: 0.0086\n",
      "Epoch [20/20], Step [148/316], Loss: 0.0061\n",
      "Epoch [20/20], Step [149/316], Loss: 0.0078\n",
      "Epoch [20/20], Step [150/316], Loss: 0.0019\n",
      "Epoch [20/20], Step [151/316], Loss: 0.0001\n",
      "Epoch [20/20], Step [152/316], Loss: 0.0241\n",
      "Epoch [20/20], Step [153/316], Loss: 0.0131\n",
      "Epoch [20/20], Step [154/316], Loss: 0.0315\n",
      "Epoch [20/20], Step [155/316], Loss: 0.0215\n",
      "Epoch [20/20], Step [156/316], Loss: 0.0272\n",
      "Epoch [20/20], Step [157/316], Loss: 0.0025\n",
      "Epoch [20/20], Step [158/316], Loss: 0.0035\n",
      "Epoch [20/20], Step [159/316], Loss: 0.0056\n",
      "Epoch [20/20], Step [160/316], Loss: 0.0021\n",
      "Epoch [20/20], Step [161/316], Loss: 0.1786\n",
      "Epoch [20/20], Step [162/316], Loss: 0.0095\n",
      "Epoch [20/20], Step [163/316], Loss: 0.0031\n",
      "Epoch [20/20], Step [164/316], Loss: 0.0018\n",
      "Epoch [20/20], Step [165/316], Loss: 0.1670\n",
      "Epoch [20/20], Step [166/316], Loss: 0.0029\n",
      "Epoch [20/20], Step [167/316], Loss: 0.0433\n",
      "Epoch [20/20], Step [168/316], Loss: 0.1142\n",
      "Epoch [20/20], Step [169/316], Loss: 0.0032\n",
      "Epoch [20/20], Step [170/316], Loss: 0.0011\n",
      "Epoch [20/20], Step [171/316], Loss: 0.0049\n",
      "Epoch [20/20], Step [172/316], Loss: 0.0055\n",
      "Epoch [20/20], Step [173/316], Loss: 0.0050\n",
      "Epoch [20/20], Step [174/316], Loss: 0.0050\n",
      "Epoch [20/20], Step [175/316], Loss: 0.0319\n",
      "Epoch [20/20], Step [176/316], Loss: 0.0013\n",
      "Epoch [20/20], Step [177/316], Loss: 0.0034\n",
      "Epoch [20/20], Step [178/316], Loss: 0.0068\n",
      "Epoch [20/20], Step [179/316], Loss: 0.1049\n",
      "Epoch [20/20], Step [180/316], Loss: 0.0416\n",
      "Epoch [20/20], Step [181/316], Loss: 0.0092\n",
      "Epoch [20/20], Step [182/316], Loss: 0.0740\n",
      "Epoch [20/20], Step [183/316], Loss: 0.0072\n",
      "Epoch [20/20], Step [184/316], Loss: 0.1044\n",
      "Epoch [20/20], Step [185/316], Loss: 0.0132\n",
      "Epoch [20/20], Step [186/316], Loss: 0.0057\n",
      "Epoch [20/20], Step [187/316], Loss: 0.0047\n",
      "Epoch [20/20], Step [188/316], Loss: 0.0084\n",
      "Epoch [20/20], Step [189/316], Loss: 0.0084\n",
      "Epoch [20/20], Step [190/316], Loss: 0.0758\n",
      "Epoch [20/20], Step [191/316], Loss: 0.0068\n",
      "Epoch [20/20], Step [192/316], Loss: 0.0562\n",
      "Epoch [20/20], Step [193/316], Loss: 0.0038\n",
      "Epoch [20/20], Step [194/316], Loss: 0.0033\n",
      "Epoch [20/20], Step [195/316], Loss: 0.0051\n",
      "Epoch [20/20], Step [196/316], Loss: 0.0017\n",
      "Epoch [20/20], Step [197/316], Loss: 0.0605\n",
      "Epoch [20/20], Step [198/316], Loss: 0.1918\n",
      "Epoch [20/20], Step [199/316], Loss: 0.0055\n",
      "Epoch [20/20], Step [200/316], Loss: 0.0825\n",
      "Epoch [20/20], Step [201/316], Loss: 0.1392\n",
      "Epoch [20/20], Step [202/316], Loss: 0.0128\n",
      "Epoch [20/20], Step [203/316], Loss: 0.0565\n",
      "Epoch [20/20], Step [204/316], Loss: 0.0194\n",
      "Epoch [20/20], Step [205/316], Loss: 0.0080\n",
      "Epoch [20/20], Step [206/316], Loss: 0.0116\n",
      "Epoch [20/20], Step [207/316], Loss: 0.0050\n",
      "Epoch [20/20], Step [208/316], Loss: 0.0022\n",
      "Epoch [20/20], Step [209/316], Loss: 0.1343\n",
      "Epoch [20/20], Step [210/316], Loss: 0.0024\n",
      "Epoch [20/20], Step [211/316], Loss: 0.0014\n",
      "Epoch [20/20], Step [212/316], Loss: 0.0360\n",
      "Epoch [20/20], Step [213/316], Loss: 0.0087\n",
      "Epoch [20/20], Step [214/316], Loss: 0.0045\n",
      "Epoch [20/20], Step [215/316], Loss: 0.0059\n",
      "Epoch [20/20], Step [216/316], Loss: 0.0055\n",
      "Epoch [20/20], Step [217/316], Loss: 0.0028\n",
      "Epoch [20/20], Step [218/316], Loss: 0.0022\n",
      "Epoch [20/20], Step [219/316], Loss: 0.0051\n",
      "Epoch [20/20], Step [220/316], Loss: 0.0023\n",
      "Epoch [20/20], Step [221/316], Loss: 0.0044\n",
      "Epoch [20/20], Step [222/316], Loss: 0.0797\n",
      "Epoch [20/20], Step [223/316], Loss: 0.0052\n",
      "Epoch [20/20], Step [224/316], Loss: 0.0252\n",
      "Epoch [20/20], Step [225/316], Loss: 0.0011\n",
      "Epoch [20/20], Step [226/316], Loss: 0.0031\n",
      "Epoch [20/20], Step [227/316], Loss: 0.0116\n",
      "Epoch [20/20], Step [228/316], Loss: 0.0928\n",
      "Epoch [20/20], Step [229/316], Loss: 0.0002\n",
      "Epoch [20/20], Step [230/316], Loss: 0.0129\n",
      "Epoch [20/20], Step [231/316], Loss: 0.0023\n",
      "Epoch [20/20], Step [232/316], Loss: 0.0065\n",
      "Epoch [20/20], Step [233/316], Loss: 0.0030\n",
      "Epoch [20/20], Step [234/316], Loss: 0.0297\n",
      "Epoch [20/20], Step [235/316], Loss: 0.1736\n",
      "Epoch [20/20], Step [236/316], Loss: 0.0033\n",
      "Epoch [20/20], Step [237/316], Loss: 0.0084\n",
      "Epoch [20/20], Step [238/316], Loss: 0.0081\n",
      "Epoch [20/20], Step [239/316], Loss: 0.0049\n",
      "Epoch [20/20], Step [240/316], Loss: 0.0396\n",
      "Epoch [20/20], Step [241/316], Loss: 0.0038\n",
      "Epoch [20/20], Step [242/316], Loss: 0.0015\n",
      "Epoch [20/20], Step [243/316], Loss: 0.0345\n",
      "Epoch [20/20], Step [244/316], Loss: 0.0074\n",
      "Epoch [20/20], Step [245/316], Loss: 0.1871\n",
      "Epoch [20/20], Step [246/316], Loss: 0.0017\n",
      "Epoch [20/20], Step [247/316], Loss: 0.1072\n",
      "Epoch [20/20], Step [248/316], Loss: 0.1050\n",
      "Epoch [20/20], Step [249/316], Loss: 0.0035\n",
      "Epoch [20/20], Step [250/316], Loss: 0.0272\n",
      "Epoch [20/20], Step [251/316], Loss: 0.0036\n",
      "Epoch [20/20], Step [252/316], Loss: 0.1019\n",
      "Epoch [20/20], Step [253/316], Loss: 0.0208\n",
      "Epoch [20/20], Step [254/316], Loss: 0.0269\n",
      "Epoch [20/20], Step [255/316], Loss: 0.0266\n",
      "Epoch [20/20], Step [256/316], Loss: 0.0968\n",
      "Epoch [20/20], Step [257/316], Loss: 0.0187\n",
      "Epoch [20/20], Step [258/316], Loss: 0.0136\n",
      "Epoch [20/20], Step [259/316], Loss: 0.1752\n",
      "Epoch [20/20], Step [260/316], Loss: 0.0103\n",
      "Epoch [20/20], Step [261/316], Loss: 0.0048\n",
      "Epoch [20/20], Step [262/316], Loss: 0.0280\n",
      "Epoch [20/20], Step [263/316], Loss: 0.0045\n",
      "Epoch [20/20], Step [264/316], Loss: 0.0660\n",
      "Epoch [20/20], Step [265/316], Loss: 0.0423\n",
      "Epoch [20/20], Step [266/316], Loss: 0.0158\n",
      "Epoch [20/20], Step [267/316], Loss: 0.0506\n",
      "Epoch [20/20], Step [268/316], Loss: 0.1563\n",
      "Epoch [20/20], Step [269/316], Loss: 0.0223\n",
      "Epoch [20/20], Step [270/316], Loss: 0.0098\n",
      "Epoch [20/20], Step [271/316], Loss: 0.0480\n",
      "Epoch [20/20], Step [272/316], Loss: 0.0027\n",
      "Epoch [20/20], Step [273/316], Loss: 0.0090\n",
      "Epoch [20/20], Step [274/316], Loss: 0.0104\n",
      "Epoch [20/20], Step [275/316], Loss: 0.0063\n",
      "Epoch [20/20], Step [276/316], Loss: 0.0025\n",
      "Epoch [20/20], Step [277/316], Loss: 0.0045\n",
      "Epoch [20/20], Step [278/316], Loss: 0.0078\n",
      "Epoch [20/20], Step [279/316], Loss: 0.0751\n",
      "Epoch [20/20], Step [280/316], Loss: 0.0506\n",
      "Epoch [20/20], Step [281/316], Loss: 0.0861\n",
      "Epoch [20/20], Step [282/316], Loss: 0.0262\n",
      "Epoch [20/20], Step [283/316], Loss: 0.0056\n",
      "Epoch [20/20], Step [284/316], Loss: 0.3795\n",
      "Epoch [20/20], Step [285/316], Loss: 0.0146\n",
      "Epoch [20/20], Step [286/316], Loss: 0.0044\n",
      "Epoch [20/20], Step [287/316], Loss: 0.0024\n",
      "Epoch [20/20], Step [288/316], Loss: 0.0544\n",
      "Epoch [20/20], Step [289/316], Loss: 0.0033\n",
      "Epoch [20/20], Step [290/316], Loss: 0.0049\n",
      "Epoch [20/20], Step [291/316], Loss: 0.0408\n",
      "Epoch [20/20], Step [292/316], Loss: 0.0078\n",
      "Epoch [20/20], Step [293/316], Loss: 0.0152\n",
      "Epoch [20/20], Step [294/316], Loss: 0.0191\n",
      "Epoch [20/20], Step [295/316], Loss: 0.0466\n",
      "Epoch [20/20], Step [296/316], Loss: 0.0092\n",
      "Epoch [20/20], Step [297/316], Loss: 0.0079\n",
      "Epoch [20/20], Step [298/316], Loss: 0.0428\n",
      "Epoch [20/20], Step [299/316], Loss: 0.0473\n",
      "Epoch [20/20], Step [300/316], Loss: 0.1183\n",
      "Epoch [20/20], Step [301/316], Loss: 0.0036\n",
      "Epoch [20/20], Step [302/316], Loss: 0.0065\n",
      "Epoch [20/20], Step [303/316], Loss: 0.0032\n",
      "Epoch [20/20], Step [304/316], Loss: 0.0046\n",
      "Epoch [20/20], Step [305/316], Loss: 0.0077\n",
      "Epoch [20/20], Step [306/316], Loss: 0.0043\n",
      "Epoch [20/20], Step [307/316], Loss: 0.0044\n",
      "Epoch [20/20], Step [308/316], Loss: 0.0065\n",
      "Epoch [20/20], Step [309/316], Loss: 0.0168\n",
      "Epoch [20/20], Step [310/316], Loss: 0.0076\n",
      "Epoch [20/20], Step [311/316], Loss: 0.0064\n",
      "Epoch [20/20], Step [312/316], Loss: 0.0047\n",
      "Epoch [20/20], Step [313/316], Loss: 0.0108\n",
      "Epoch [20/20], Step [314/316], Loss: 0.0161\n",
      "Epoch [20/20], Step [315/316], Loss: 0.0065\n",
      "Epoch [20/20], Step [316/316], Loss: 0.0167\n",
      "Epoch [20/20], Train Loss: 0.0303\n",
      "Epoch [20/20], Validation Loss: 0.1586\n",
      "Epoch [20/20], Validation Accuracy: 0.8116\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_steps = 0\n",
    "    model.train()\n",
    "    for iteration, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_steps += 1\n",
    "    \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{iteration+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    train_loss_history.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in val_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            total_loss += criterion(outputs, batch_labels).item()\n",
    "            total_correct += (predicted_labels == batch_labels).sum().item()\n",
    "            total_samples += batch_labels.size(0)\n",
    "\n",
    "        accuracy = total_correct / total_samples\n",
    "        val_accuracy_history.append(accuracy)\n",
    "        val_loss_history.append(total_loss / total_samples)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {total_loss / total_samples:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "fBzJMWw0xnuj",
    "outputId": "57ee30e3-1ba4-4f13-8513-120995eef6dc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation categorical accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracy_history, label='Validation Categorical Accuracy')\n",
    "plt.title('Validation Categorical Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "V0lj4SWYbAK4"
   },
   "source": [
    "**2.** After you have looked at the graph, what do you think is an appropriate amount of `epochs`? Briefly explain at which amount of epochs the model seems to be underfitting or overfitting and how this depends on the learning rate?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GR9afmjh57P1"
   },
   "source": [
    "**TODO: Write your observations here**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "if8yjwQNwy_y"
   },
   "source": [
    "### d) Evaluate the Base Model\n",
    "\n",
    "The F1 score is a universal measurement of a test's accuracy. It is calculated as the harmonic mean of *precision* and *recall*.\n",
    "\n",
    "- **precision** refers to the number of true positives divided by the number of all positives\n",
    "- **recall** refers to the number of true positives divided by the number of relevant elements\n",
    "\n",
    "\n",
    "$$F_{1} = \\frac{2}{recall^{-1} + precision^{-1}} = \\frac{tp}{tp+\\frac{1}{2}(fp+fn)}$$\n",
    "\n",
    "where\n",
    "*   tp = true positives\n",
    "*   fp = false positives\n",
    "*   fn = false negatives\n",
    "\n",
    "**1.** Why would we prefer the F1 Score over only the precision?\n",
    "\n",
    "**2.** Evaluate the text model with an F1 score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4hRZ2e1DbAK4"
   },
   "source": [
    "**TODO 1: Write your explanation here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Q4OoPj1zw4kQ"
   },
   "outputs": [],
   "source": [
    "from src.evaluation import f1_score_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "hGsIDzD74VCg"
   },
   "outputs": [],
   "source": [
    "# Predicting the labels from the test set\n",
    "with torch.no_grad():\n",
    "    y_pred = model(torch.tensor(test_dataset.encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQkYnnTNw4au",
    "outputId": "e5b78ce0-6817-488b-d7ab-8dfec7445ad2"
   },
   "outputs": [],
   "source": [
    "# TODO 2: Evaluate text model\n",
    "# Hint: You can lookup the evaluation.py in the src folder for documentation\n",
    "label_mapping = {0: 'none', 1: 'racism', 2: 'sexism'}\n",
    "f1_score_overall(y_test, y_pred, label_mapping=label_mapping, label_encoder=encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xvegXhlxuUCN"
   },
   "source": [
    "## Task 5.3: Preprocessing for Social Model\n",
    "Now that we have evaluated our base model we can try to enhance it by using some sort of a social context. To do so, we are using our base model's prediction to compute an average hate score for each of the followers of an author. This means that for each author we take all his follower's tweets and predict the label. We then take the average of each prediction which results in our average hate score.\n",
    "\n",
    "For each tweet we then not only feed in the tweet itself, but also the hate score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "D4kmhJJT9Kpz"
   },
   "source": [
    "### Load adjacency matrix for users"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "w0YaL7CZJqpM"
   },
   "source": [
    "In order to create our social model, we first need to load the adjacency matrix. This matrix represents the follower network between all users that have written the 16849 tweets (crawled by Linda Jahn [6]). You can check the `extend_data.ipynb` file to find out how the adjacency matrix was created.\n",
    "\n",
    "The 16849 tweets were written by 2031 distinct users. This results in a 2031x2031 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXvkZYb_JXkg"
   },
   "outputs": [],
   "source": [
    "# Load users adjacency matrix\n",
    "users_adjacency_matrix = np.load(\"pickle_files/users_data/users_adjacency_matrix.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tHICY__D45Cp"
   },
   "source": [
    "### a) Graph visualization & manipulation\n",
    "Now we are going to plot the previously loaded adjacency matrix. Since we are going to feed the matrix to the Neural Network later, and because the user network is just a tiny subset of the whole Twitter network it is important to check if the network contains any useful information.\n",
    "\n",
    "\n",
    "**1.** Plot the graph corresponding to the given adjacency matrix.\n",
    "**Note:** For better visualization, the nodes are color-coded based on their degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfuRf30ommf7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rSv96Rqt45Cq",
    "outputId": "98a5b0e3-555e-4c95-c9c8-6eb6a06d93ad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_graph(users_adjacency_matrix):\n",
    "\n",
    "    # TODO 1:\n",
    "\n",
    "    ###\n",
    "    \n",
    "    graph.add_edges_from(edges)\n",
    "\n",
    "\n",
    "    print(\"Total number of nodes:\", nx.number_of_nodes(graph))\n",
    "\n",
    "    from matplotlib.pyplot import figure\n",
    "    figure(num=None, figsize=(64, 50), dpi=100)\n",
    "    nx.draw(graph,\n",
    "            node_size=300,\n",
    "            node_color=range(nx.number_of_nodes(graph)),\n",
    "            cmap=plt.cm.Reds,\n",
    "            pos=nx.spring_layout(graph)\n",
    "           )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_graph(users_adjacency_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "j2DJp3GGkzz3"
   },
   "source": [
    "**2.** Briefly describe the graph. How many communities do you think it depicts?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xEIoD_U6Tr2u"
   },
   "source": [
    "**TODO 2: Write your observations here**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8F9-nJc8k8JI"
   },
   "source": [
    "**3.** Now let us try to actually calculate the number of communities within this graph. First, get rid of the uninteresting nodes that have zero or very few edges and just inspect the \"core\" graph. Expand on the code that you have written in the exercise above.\n",
    "**Hint**: You can do this by excluding all nodes with an `nx.eigenvector_centrality()` lower than $10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sP3rrJEok-2z",
    "outputId": "488d5894-7526-4c28-a4d5-e23ec8423283"
   },
   "outputs": [],
   "source": [
    "def show_graph_core(users_adjacency_matrix):\n",
    "\n",
    "    # TODO 3:\n",
    "\n",
    "    ###\n",
    "    \n",
    "    graph.add_edges_from(edges)\n",
    "\n",
    "    print(\"Total number of nodes:\", nx.number_of_nodes(graph))\n",
    "\n",
    "    ###\n",
    "\n",
    "    print(\"Total number of significant nodes:\", nx.number_of_nodes(graph))\n",
    "\n",
    "    from matplotlib.pyplot import figure\n",
    "    figure(num=None, figsize=(64, 50), dpi=100)\n",
    "    nx.draw(graph,\n",
    "            node_size=1000,\n",
    "            node_color=range(nx.number_of_nodes(graph)),\n",
    "            cmap=plt.cm.Reds,\n",
    "            pos=nx.spring_layout(graph)\n",
    "           )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_graph_core(users_adjacency_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yxIMjFoImCFW"
   },
   "source": [
    "**4.** Do you think the social context could further improve our hate speech detection model? Find at least 2 pros and 2 cons."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "64zx0uLcbAK7"
   },
   "source": [
    "**TODO 4: Write your observations here**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "m67eUdbgbAK8"
   },
   "source": [
    "### b) Employment of our Base Model to predict the hatefulness of an author's followers\n",
    "\n",
    "Now that we have a trained model, we can use it to predict the hatefulness for any tweet. Therefore, we can use it to predict an average hate score for each follower of an author. This means that we predict the label for each tweet of an author's follower and then compute an average across all of these predictions.\n",
    "\n",
    "**1.** Predict all encoded tweets with the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyCKpMrNbAK8"
   },
   "outputs": [],
   "source": [
    "# TODO 1:\n",
    "\n",
    "###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "j9Vun92DbAK8"
   },
   "source": [
    "In the following code cell we load the authorship numpy array. It contains the author ID of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3myAb7_jIZKX"
   },
   "outputs": [],
   "source": [
    "# Loads authorship index\n",
    "authors_idx = np.load(\"pickle_files/users_data/authorship.npy\")\n",
    "authors_idx = np.reshape(authors_idx, newshape=(-1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mVnWuelvbAK8"
   },
   "source": [
    "**2.** Now for every tweet of our dataset we need to compute its authors hate score. Therefore:\n",
    " * First define a function `get_all_followers` that return all followers for a given user.\n",
    " * Then create a list `his_followers` that contains all followers for each user.\n",
    " * Now assign the hate predictions for each of all the followers tweets.\n",
    " * Finally in `user_avg_score` compute the hate score for each user by averaging out all his followers' tweets' hate scores. If there are no followers' tweets, assign our pre-computed average values `default_hate_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZUaBR2lbAK8"
   },
   "outputs": [],
   "source": [
    "default_hate_score = np.array([0.19446494, 0.75084399, 0.0546911])\n",
    "\n",
    "# TODO 2:\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9guRGbf8bAK8"
   },
   "outputs": [],
   "source": [
    "# Put authors' hate scores in the order of the tweets\n",
    "tweets_author_hate_score = list((map(lambda x: user_avg_score[int(x)], authors_idx)))\n",
    "\n",
    "tweets_author_hate_score = np.array(tweets_author_hate_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uj6T-8D9bAK8"
   },
   "source": [
    "## Task 5.4: Social Model\n",
    "\n",
    "Now that we have our social context prepared, we can build and train our Social Model using that information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yFUY_7bXbAK9"
   },
   "source": [
    "### a) Social Model creation\n",
    "\n",
    "With our social context we have 2 separate networks:\n",
    "* Our text network that processes the tweet (the same as the base model from before)\n",
    "* Our hate score network that basically decides how important the average hate score for the classification is\n",
    "\n",
    "Our 2 separate networks are concatenated and one last hidden layer with 100 nodes is added."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "j389MzycbAK9"
   },
   "source": [
    "![title](img/enhanced_model.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "g8VXwJ75bAK9"
   },
   "source": [
    "Now define our new neural network `social_model` according to the graphic above. You can lookup most of the syntax in exercise ***5.2 a)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4URjY0TLQxW"
   },
   "outputs": [],
   "source": [
    "# TODO: Define the social model\n",
    "\n",
    "###\n",
    "\n",
    "# Create an instance of the social model\n",
    "encoding_dim = # Specify the encoding dimension\n",
    "social_model = SocialModel(encoding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1nUh5w5O4Bz"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(social_model.parameters(), lr=0.0005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PNaZ21p_bAK9"
   },
   "source": [
    "### b) Train-Test split for Social Model\n",
    "Now split the data in Train/Val/Test 60/20/20 as seen in the Base Model. This time you have to create an additional set for the Hate Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tu9F4O52bAK9",
    "outputId": "966be4c7-af4f-43d6-8050-31af59cc7359"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "labels = factorized_labels[0]\n",
    "\n",
    "###\n",
    "\n",
    "print(\"Training data shape: {}, Labels shape: {}, Hate Score shape: {}\".format(X_train.shape, y_train.shape, a_train.shape))\n",
    "print(\"Test data shape: {}, Labels shape: {}, Hate Score shape: {}\".format(X_test.shape, y_test.shape, a_test.shape))\n",
    "print(\"Validation data shape: {}, Labels shape: {}, Hate Score shape: {}\".format(X_val.shape, y_val.shape, a_val.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JlE3px-gPYQx"
   },
   "source": [
    "Once again we need to create adequate Datasets and Dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFioOyu6PUju"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# TODO: Create a CustomDataset class for our Tweet data\n",
    "\n",
    "###\n",
    "\n",
    "# Create the Datasets\n",
    "train_dataset = ###\n",
    "val_dataset = ###\n",
    "test_dataset = ###\n",
    "\n",
    "# DataLoader for batching and parallel data loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i3IKfEb3bAK9"
   },
   "source": [
    "### c) Train and Evaluate the Enhanced Model\n",
    "Once again, train and evaluate the model with a F1 Score. Use 20 `epochs` and `batch_size` of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0wWvi8jbAK-",
    "outputId": "110cb687-63c9-4129-c129-911547ac3a2c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_steps = 0\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    social_model.train()\n",
    "    for inputs, hatescore, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = social_model(inputs, hatescore)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_steps += 1\n",
    "\n",
    "    social_model.eval()\n",
    "    with torch.no_grad():\n",
    "      for inputs, hatescore, labels in val_loader:\n",
    "        outputs = social_model(inputs, hatescore)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        _, val_labels = torch.max(outputs, dim=1)\n",
    "        val_acc += (val_labels == labels).sum().item() / labels.size(0)\n",
    "\n",
    "        val_steps += 1\n",
    "\n",
    "    train_loss_history.append(train_loss/train_steps)\n",
    "    val_loss_history.append(val_loss/val_steps)\n",
    "    val_accuracy_history.append(val_acc/val_steps)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Training loss={train_loss/train_steps}, validation loss={val_loss/val_steps}, validation accuracy={val_acc/val_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tjf6CQLObAK-",
    "outputId": "9c15b6e5-e534-4377-f2b7-15c2ca220f4c"
   },
   "outputs": [],
   "source": [
    "from src.evaluation import f1_score_overall\n",
    "\n",
    "# TODO: Evaluate the model with a F1 Score\n",
    "\n",
    "###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Brwe8cbSbAK-"
   },
   "source": [
    "## Task 5.5: Discussion and comparison\n",
    "\n",
    "* Compare the performances of our two models in your own words\n",
    "\n",
    "* Why do you think it improved?\n",
    "\n",
    "* Can you think of any other social context to further improve our model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c-VeHwtQbAK-"
   },
   "source": [
    "**TODO: Write your thoughts here**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "svmyf6kcbAK-"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Festinger, L., Pepitone, A. and Newcomb, T. (1952) *Some Consequences of De-Individuation in a Group.* Journal of Abnormal and Social Psychology, 47, 382-389.\n",
    "<br>[2] Waseem, Z., & Hovy, D. (2016). *Hateful symbols or hateful people? Predictive features for hate speech detection on Twitter.* In Proceedings of the naacl student research workshop (pp. 88-93).\n",
    "<br>[3] https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n",
    "<br>[4] https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "<br>[5] https://pytorch.org/docs/stable/nn.html\n",
    "<br>[6] Jahn, L. (2020). *Leveraging Social Network Data for Hate Speech Detection.* Master\n",
    "Thesis, Technical University of Munich, Department of Informatics."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
