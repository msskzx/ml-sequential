{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook we build extended inputs for our social/graph models\n",
    "\n",
    "Anything regarding building community graphs, graphs embedding and using social features is this notebook. We use the Waseem & Hovy data. The structure is as follow:\n",
    "\n",
    "* Import the Data (this time also user data)\n",
    "* Build graphs (community graph, extended graph, ...)\n",
    "* Create graph embedding (from graph to dense vectors)\n",
    "\n",
    "#### What 'raw data' do we use in general?\n",
    "For now, I sticked to simple inputs like\n",
    "* tweets (text)\n",
    "* tweets' authors\n",
    "* authors' friends\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data from MongoDB\n",
    "the data is imported from a mongodb client setup by Linda, ask her for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Mongo DB and read Data\n",
    "db_name = 'hatespeech_WaseemHovy'\n",
    "client = pymongo.MongoClient(port=27017)\n",
    "db = client[db_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So Drasko just said he was impressed the girls...</td>\n",
       "      <td>2498963143</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Drasko they didn't cook half a bird you idiot ...</td>\n",
       "      <td>110114783</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hopefully someone cooks Drasko in the next ep ...</td>\n",
       "      <td>38650214</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>of course you were born in serbia...you're as ...</td>\n",
       "      <td>2587278392</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>These girls are the equivalent of the irritati...</td>\n",
       "      <td>2601524623</td>\n",
       "      <td>racism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        user   label\n",
       "0  So Drasko just said he was impressed the girls...  2498963143  racism\n",
       "1  Drasko they didn't cook half a bird you idiot ...   110114783  racism\n",
       "2  Hopefully someone cooks Drasko in the next ep ...    38650214  racism\n",
       "3  of course you were born in serbia...you're as ...  2587278392  racism\n",
       "4  These girls are the equivalent of the irritati...  2601524623  racism"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>follower_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>16754078</td>\n",
       "      <td>949291260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8197942</td>\n",
       "      <td>949291260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>238250804</td>\n",
       "      <td>949291260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>224302874</td>\n",
       "      <td>949291260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>915643998</td>\n",
       "      <td>949291260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id follower_id\n",
       "0   16754078   949291260\n",
       "1    8197942   949291260\n",
       "2  238250804   949291260\n",
       "3  224302874   949291260\n",
       "4  915643998   949291260"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract tweet and follow relationship tables from mongodb \n",
    "annotated_tweets = pd.DataFrame(db.annotated_tweet.find())\n",
    "follow_relationships = pd.DataFrame(db.follower.find({},{'_id': 0}))\n",
    "\n",
    "# condense information (just take the information that we use)\n",
    "annotated_tweets = annotated_tweets[['text', 'user', 'label']]\n",
    "annotated_tweets['user'] = annotated_tweets['user'].transform(lambda x: x['id']) \n",
    "\n",
    "display(annotated_tweets.head(), follow_relationships.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Graph\n",
    "Here we create graphs with the data that we have, we use NetworkX <br>\n",
    "Docs for NetworkX: https://networkx.github.io/documentation/stable/\n",
    "\n",
    "**COMMUNITY GRAPH**: Author profiling for abuse detection (Mishra 2018) <br>\n",
    "**EXTENDED GRAPH**: Abusive language detection with graph convolutional networks (Mishra 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique users\n",
    "users = np.unique(annotated_tweets['user'].to_numpy())\n",
    "users = np.array(list(map(str, users)))\n",
    "\n",
    "# get relationships between users (this file was given to me by Linda)\n",
    "G_followers = nx.read_gpickle('pickle_files/users_data/user_root_follower_network_WaseemHovy_Linda.pkl')\n",
    "\n",
    "# get links between tweets and users\n",
    "tweets = np.unique(annotated_tweets['text'].to_numpy())\n",
    "author_relationship = annotated_tweets[['text', 'user']].to_numpy()\n",
    "author_relationship[:,1] = np.array(list(map(str, author_relationship[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLLOWER GRAPH\n",
      "Number of nodes:  1207  Number of edges:  7860\n",
      "COMMUNITY GRAPH\n",
      "Number of nodes:  2031  Number of edges:  5514\n"
     ]
    }
   ],
   "source": [
    "# create a community graph \n",
    "G_community = nx.Graph()\n",
    "G_community.add_nodes_from(users)\n",
    "G_community.add_edges_from(G_followers.edges())\n",
    "print('FOLLOWER GRAPH')\n",
    "print('Number of nodes: ', G_followers.number_of_nodes(), ' Number of edges: ', G_followers.number_of_edges())\n",
    "print('COMMUNITY GRAPH')\n",
    "print('Number of nodes: ', G_community.number_of_nodes(), ' Number of edges: ', G_community.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solitary users: 824, Edges check: 11028, Average Degree: 2.7149187592319053, Graph Density: 0.00267479680712503\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on degree and density\n",
    "count_empty = 0\n",
    "value_sum = 0\n",
    "for value in dict(G_community.degree()).values():\n",
    "    if value == 0:\n",
    "        count_empty += 1\n",
    "    else:\n",
    "        value_sum += value\n",
    "\n",
    "no_of_nodes = G_community.number_of_nodes()\n",
    "no_of_edges = G_community.number_of_edges()\n",
    "graph_density = 2*no_of_edges / (no_of_nodes*(no_of_nodes - 1))\n",
    "        \n",
    "print('Solitary users: {}, Edges check: {}, Average Degree: {}, Graph Density: {}'\\\n",
    "     .format(count_empty, value_sum, no_of_edges/no_of_nodes, graph_density))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWEETS GRAPH\n",
      "Number of nodes:  16849  Number of edges:  0\n",
      "AUTHOR GRAPH\n",
      "Number of nodes:  18873  Number of edges:  16849\n",
      "EXTENDED GRAPH\n",
      "Number of nodes:  18880  Number of edges:  22363\n"
     ]
    }
   ],
   "source": [
    "# create an extended graph\n",
    "G_extended = nx.Graph()\n",
    "G_extended.add_nodes_from(tweets)\n",
    "print('TWEETS GRAPH')\n",
    "print('Number of nodes: ', G_extended.number_of_nodes(), ' Number of edges: ', G_extended.number_of_edges())\n",
    "G_extended.add_edges_from([tuple(x) for x in author_relationship])\n",
    "print('AUTHOR GRAPH')\n",
    "print('Number of nodes: ', G_extended.number_of_nodes(), ' Number of edges: ', G_extended.number_of_edges())\n",
    "G_extended.add_edges_from(G_followers.edges())\n",
    "print('EXTENDED GRAPH')\n",
    "print('Number of nodes: ', G_extended.number_of_nodes(), ' Number of edges: ', G_extended.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph embedding\n",
    "Create a graph embedding for our graphs\n",
    "\n",
    "For now the best options seem _node2vec_ and _Structural Deep Network Embedding (Also used in Mishra 2018) <br>\n",
    "I found everything here: https://towardsdatascience.com/graph-embeddings-the-summary-cc6075aba007 <br>\n",
    "<br>\n",
    "REF: https://github.com/eliorc/node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create node2vec embedding object\n",
    "node2vec = Node2Vec(G_community, dimensions=128, walk_length=10, num_walks=100, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the embedding (this might take a long time)\n",
    "model = node2vec.fit(window=20, min_count=3, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save both the embedding model and the embedded nodes, .load() to load \n",
    "EMBEDDING_FILENAME = 'pickle_files/embedded_vectors'\n",
    "EMBEDDING_MODEL_FILENAME = 'pickle_files/embedding_model'\n",
    "model.wv.save_word2vec_format(EMBEDDING_FILENAME)\n",
    "model.save(EMBEDDING_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dictionary where embeddings are stored for each node\n",
    "embedded_G_community = model.wv.vectors\n",
    "users_dict = {}\n",
    "for index, usr in enumerate(np.array(G_community.nodes)):\n",
    "    users_dict[usr] = embedded_G_community[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the user_dict to pickle\n",
    "with open('pickle_files/users_dict.p', 'wb') as handle:\n",
    "    pickle.dump(users_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create adjacency matrix\n",
    "\n",
    "Here we create the adjacency matrices which can be used by our graph/social model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18880, 18880)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_matrix = nx.adjacency_matrix(G_extended)\n",
    "adjacency_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just check that every row has at list on nonzero element, should not print anything\n",
    "nonzero = adjacency_matrix.nonzero()[0]\n",
    "for i in range(adjacency_matrix.shape[0]):\n",
    "    if i not in nonzero:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the extended graph adjacency \n",
    "with open('pickle_files/adjacency_matrix_extended_graph.p', 'wb') as handle:\n",
    "    pickle.dump(adjacency_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2031, 2031)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creates reduced adjacency matrix to save memory\n",
    "# (not necessary in general, but was necessary for me due to memory contraints)\n",
    "no_of_tweets = 16849\n",
    "\n",
    "#there are 16849 tweets, the rest is users, we save the submatrix\n",
    "users_adjacency_matrix = adjacency_matrix[no_of_tweets:, no_of_tweets:]\n",
    "display(users_adjacency_matrix.shape)\n",
    "np.save(\"pickle_files/users_data/users_adjacency_matrix.npy\", users_adjacency_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each tweet, creates an index to read up author information and connections\n",
    "# (important if we use the reduced ajacency matrix from before)\n",
    "authors_idx = np.zeros((no_of_tweets, ), dtype=int)\n",
    "for tweet_idx in range(no_of_tweets):\n",
    "    author_idx = np.nonzero(adjacency_matrix[:,:][tweet_idx])[1]\n",
    "    authors_idx[tweet_idx] = author_idx - no_of_tweets  \n",
    "    \n",
    "# we save the array\n",
    "np.save(\"pickle_files/users_data/authorship.npy\", authors_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
